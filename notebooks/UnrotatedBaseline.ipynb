{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import PyTorch modules"
      ],
      "metadata": {
        "id": "RfBeheCNos9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaRjQJEDokeO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "import os\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "EjYFfoM3sHvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SHARED_PATH = Path(\"drive/MyDrive/Colab Notebooks/Shared\")\n",
        "HRSC_PATH = SHARED_PATH / \"HRSC2016_Final_Splits\"\n",
        "DOTA_PATH = SHARED_PATH / \"DOTA_Final_Splits\"\n",
        "NWPU_PATH = SHARED_PATH / \"NWPU_VHR-10_Final_Splits\"\n",
        "\n",
        "print(\"HRSC subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in HRSC_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "print(\"DOTA subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in DOTA_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "print(\"NWPU subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in NWPU_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "\n",
        "HRSC_TRAIN_IMAGES = HRSC_PATH/ \"train/images\"\n",
        "HRSC_TRAIN_ANNOTATIONS = HRSC_PATH / \"train/annotations\"\n",
        "HRSC_VAL_IMAGES = HRSC_PATH / \"val/images\"\n",
        "HRSC_VAL_ANNOTATIONS = HRSC_PATH / \"val/annotations\"\n",
        "HRSC_TEST_IMAGES = HRSC_PATH / \"test/images\"\n",
        "HRSC_TEST_ANNOTATIONS = HRSC_PATH / \"test/annotations\"\n",
        "\n",
        "DOTA_TRAIN_IMAGES = DOTA_PATH / \"train/images\"\n",
        "DOTA_TRAIN_ANNOTATIONS = DOTA_PATH / \"train/hbb\"\n",
        "DOTA_VAL_IMAGES = DOTA_PATH / \"val/images\"\n",
        "DOTA_VAL_ANNOTATIONS = DOTA_PATH / \"val/hbb\"\n",
        "DOTA_TEST_IMAGES = DOTA_PATH / \"test/images\"\n",
        "DOTA_TEST_ANNOTATIONS = DOTA_PATH / \"test/hbb\"\n",
        "\n",
        "NWPU_TRAIN_IMAGES = NWPU_PATH / \"train/images\"\n",
        "NWPU_TRAIN_ANNOTATIONS = NWPU_PATH / \"train/annotations\"\n",
        "NWPU_VAL_IMAGES = NWPU_PATH / \"val/images\"\n",
        "NWPU_VAL_ANNOTATIONS = NWPU_PATH / \"val/annotations\"\n",
        "NWPU_TEST_IMAGES = NWPU_PATH / \"test/images\"\n",
        "NWPU_TEST_ANNOTATIONS = NWPU_PATH / \"test/annotations\""
      ],
      "metadata": {
        "id": "PT9GIwS8sX4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore datasets"
      ],
      "metadata": {
        "id": "yp-lr_Prs6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore, Style\n",
        "\n",
        "def explore_header(dataset: str, subfolder: str):\n",
        "  print(\"Exploring\", Fore.GREEN + dataset + Style.RESET_ALL, Fore.MAGENTA + subfolder + Style.RESET_ALL, \"folder...\")\n",
        "\n",
        "def explore(images_folder: Path, ext: str):\n",
        "  files = list(images_folder.glob(f\"*.{ext}\"))\n",
        "  print(f\"Number of {ext.upper()} files:\", len(files))\n",
        "  print(f\"{ext.upper()} sample files:\", [path.name for path in files[:3]])\n",
        "\n",
        "explore_header(\"HRSC\", \"train\")\n",
        "explore(HRSC_TRAIN_IMAGES, \"bmp\")\n",
        "explore(HRSC_TRAIN_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"HRSC\", \"val\")\n",
        "explore(HRSC_VAL_IMAGES, \"bmp\")\n",
        "explore(HRSC_VAL_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"HRSC\", \"test\")\n",
        "explore(HRSC_TEST_IMAGES, \"bmp\")\n",
        "explore(HRSC_TEST_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"train\")\n",
        "explore(DOTA_TRAIN_IMAGES, \"png\")\n",
        "explore(DOTA_TRAIN_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"val\")\n",
        "explore(DOTA_VAL_IMAGES, \"png\")\n",
        "explore(DOTA_VAL_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"test\")\n",
        "explore(DOTA_TEST_IMAGES, \"png\")\n",
        "explore(DOTA_TEST_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"NWPU\", \"train\")\n",
        "explore(NWPU_TRAIN_IMAGES, \"jpg\")\n",
        "explore(NWPU_TRAIN_ANNOTATIONS, \"json\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"NWPU\", \"val\")\n",
        "explore(NWPU_VAL_IMAGES, \"jpg\")\n",
        "explore(NWPU_VAL_ANNOTATIONS, \"json\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"NWPU\", \"test\")\n",
        "explore(NWPU_TEST_IMAGES, \"jpg\")\n",
        "explore(NWPU_TEST_ANNOTATIONS, \"json\")"
      ],
      "metadata": {
        "id": "jf-wMzlNs8Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define label parsers"
      ],
      "metadata": {
        "id": "ELZ-eU81tvbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "HRSC_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_hrsc_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "  tree = ET.parse(label_path.as_posix())\n",
        "  root = tree.getroot()\n",
        "\n",
        "  objects = root.findall(\".//HRSC_Object\")\n",
        "  for obj in objects:\n",
        "    try:\n",
        "      # Bounds\n",
        "      xmin = float(obj.find('box_xmin').text)\n",
        "      ymin = float(obj.find('box_ymin').text)\n",
        "      xmax = float(obj.find('box_xmax').text)\n",
        "      ymax = float(obj.find('box_ymax').text)\n",
        "\n",
        "      # Category\n",
        "      class_id = int(obj.find('Class_ID').text)\n",
        "      if class_id not in HRSC_CLASSES:\n",
        "        HRSC_CLASSES[class_id] = len(HRSC_CLASSES)\n",
        "      class_id_index = HRSC_CLASSES[class_id]\n",
        "\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "\n",
        "def hrsc_image_rescale(label_path: Path, image_width, image_height):\n",
        "  tree = ET.parse(label_path.as_posix())\n",
        "  root = tree.getroot()\n",
        "  width = int(root.find('.//Img_SizeWidth').text)\n",
        "  height = int(root.find('.//Img_SizeHeight').text)\n",
        "  return image_width / width, image_height / height\n",
        "\n",
        "\n",
        "DOTA_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_dota_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "\n",
        "  # Find all objects\n",
        "  for line in itertools.islice(label_path.read_text().splitlines(), 2, None):  # Start from line index 2\n",
        "    try:\n",
        "      obj = line.strip().split()\n",
        "      x1, y1, x2, y2, x3, y3, x4, y4, category, difficulty = (*map(float, obj[:8]), *obj[8:])\n",
        "\n",
        "      # Bounds\n",
        "      xmin = min(x1, x2, x3, x4)\n",
        "      ymin = min(y1, y2, y3, y4)\n",
        "      xmax = max(x1, x2, x3, x4)\n",
        "      ymax = max(y1, y2, y3, y4)\n",
        "\n",
        "      # Category\n",
        "      if category not in DOTA_CLASSES:\n",
        "        DOTA_CLASSES[category] = len(DOTA_CLASSES)\n",
        "      class_id_index = DOTA_CLASSES[category]\n",
        "\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "\n",
        "def dota_image_rescale(label_path: Path, image_width, image_height):\n",
        "  return 1.0, 1.0\n",
        "\n",
        "\n",
        "NWPU_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_nwpu_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "\n",
        "  data = json.loads(label_path.read_text())\n",
        "  for category in data['categories']:\n",
        "    NWPU_CLASSES[category['name']] = category['id'] - 1  # categories are 1-indexed -> convert to 0-indexed\n",
        "\n",
        "  for annotation in data['annotations']:\n",
        "    try:\n",
        "      # Bounds\n",
        "      bbox = annotation['bbox']\n",
        "      xmin, ymin, width, height = map(float, bbox)\n",
        "      xmax = xmin + width\n",
        "      ymax = ymin + height\n",
        "\n",
        "      # Category\n",
        "      class_id_index = annotation['category_id'] - 1\n",
        "\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "def nwpu_image_rescale(label_path: Path, image_width, image_height):\n",
        "  data = json.loads(label_path.read_text())\n",
        "  image = data['images'][0]\n",
        "  width = image['width']\n",
        "  height = image['height']\n",
        "  return image_width / width, image_height / height"
      ],
      "metadata": {
        "id": "c1iomq0mtvAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch dataset structure"
      ],
      "metadata": {
        "id": "MVdX-5_GpPse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "BASIC_TRANSFORM = T.Compose([T.ToTensor()])\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_folder: Path, annotations_folder: Path, label_parser, image_rescale, transforms=BASIC_TRANSFORM, max_files=10) -> None:\n",
        "    super().__init__()\n",
        "    self.label_parser = label_parser\n",
        "    self.image_rescale = image_rescale\n",
        "    self.transforms = transforms\n",
        "\n",
        "    images = (\n",
        "      sorted(\n",
        "        [f for f in images_folder.iterdir() if f.suffix.lower() in (\".jpg\", \".png\", \".bmp\")],\n",
        "        key=lambda p: p.stem.lower()\n",
        "      )\n",
        "      if images_folder.exists() else []\n",
        "    )\n",
        "\n",
        "    annotations = (\n",
        "      sorted(\n",
        "        [f for f in annotations_folder.iterdir() if f.suffix.lower() in (\".xml\", \".txt\", \".json\")],\n",
        "        key=lambda p: p.stem.lower()\n",
        "      )\n",
        "      if annotations_folder.exists() else []\n",
        "    )\n",
        "\n",
        "    if max_files > 0:\n",
        "      images = images[:max_files]\n",
        "      annotations = annotations[:max_files]\n",
        "\n",
        "    # Keep only images/annotations one-to-one correspondences\n",
        "    image_set = set(f.stem for f in images)\n",
        "    annotation_set = set(f.stem for f in annotations)\n",
        "    self.ids = image_set.intersection(annotation_set)\n",
        "    self.images = {f.stem: f for f in images if f.stem in self.ids}\n",
        "    self.annotations = {f.stem: f for f in annotations if f.stem in self.ids}\n",
        "    self.ids = list(self.ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    id = self.ids[index]\n",
        "    image_path = self.images[id]\n",
        "    label_path = self.annotations[id]\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    boxes, labels = self.label_parser(label_path)\n",
        "    target = {\n",
        "        \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
        "        \"labels\": torch.as_tensor(labels, dtype=torch.int64) + 1,  # Offset by 1 for background label\n",
        "    }\n",
        "    image = self.transforms(image)\n",
        "    image.filepath = image_path;\n",
        "    return image, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n",
        "\n",
        "  def get_image_rescale(self, index):\n",
        "    id = self.ids[index]\n",
        "    image_path = self.images[id]\n",
        "    label_path = self.annotations[id]\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return self.image_rescale(label_path, image.width, image.height)\n",
        "\n",
        "  def compute_total_number_of_objects(self, max_workers=8):\n",
        "    def count_objects(id):\n",
        "      boxes, _ = self.label_parser(self.annotations[id])\n",
        "      return len(boxes)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "      results = executor.map(count_objects, self.ids)\n",
        "    return sum(results)"
      ],
      "metadata": {
        "id": "4usg0yx6pTIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare datasets for PyTorch"
      ],
      "metadata": {
        "id": "-kt6q3Jqwu-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing HRSC training dataset...\")\n",
        "HRSC_TRAIN_DATASET = TorchDataset(HRSC_TRAIN_IMAGES, HRSC_TRAIN_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC training dataset sample:\", HRSC_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing HRSC validation dataset...\")\n",
        "HRSC_VAL_DATASET = TorchDataset(HRSC_VAL_IMAGES, HRSC_VAL_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC validation dataset sample:\", HRSC_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing HRSC testing dataset...\")\n",
        "HRSC_TEST_DATASET = TorchDataset(HRSC_TEST_IMAGES, HRSC_TEST_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC testing dataset sample:\", HRSC_TEST_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA training dataset...\")\n",
        "DOTA_TRAIN_DATASET = TorchDataset(DOTA_TRAIN_IMAGES, DOTA_TRAIN_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA training dataset sample:\", DOTA_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA validation dataset...\")\n",
        "DOTA_VAL_DATASET = TorchDataset(DOTA_VAL_IMAGES, DOTA_VAL_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA validation dataset sample:\", DOTA_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA testing dataset...\")\n",
        "DOTA_TEST_DATASET = TorchDataset(DOTA_TEST_IMAGES, DOTA_TEST_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA testing dataset sample:\", DOTA_TEST_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing NWPU training dataset...\")\n",
        "NWPU_TRAIN_DATASET = TorchDataset(NWPU_TRAIN_IMAGES, NWPU_TRAIN_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"NWPU training dataset sample:\", NWPU_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing NWPU validation dataset...\")\n",
        "NWPU_VAL_DATASET = TorchDataset(NWPU_VAL_IMAGES, NWPU_VAL_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"NWPU validation dataset sample:\", NWPU_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing NWPU testing dataset...\")\n",
        "NWPU_TEST_DATASET = TorchDataset(NWPU_TEST_IMAGES, NWPU_TEST_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"NWPU testing dataset sample:\", NWPU_TEST_DATASET.ids[:5])"
      ],
      "metadata": {
        "id": "c3s53360w44K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define PyTorch Faster R-CNN model"
      ],
      "metadata": {
        "id": "c2O8GWlr2sif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms as T\n",
        "\n",
        "class FasterRCNNModel:\n",
        "  def __init__(self, model_path: Path, train_dataset: TorchDataset, val_dataset: TorchDataset, test_dataset: TorchDataset, class_names: list, batch_size=4, shuffle_datasets=False) -> None:\n",
        "    self.model_path = model_path\n",
        "    self.train_dataset = train_dataset\n",
        "    self.val_dataset = val_dataset\n",
        "    self.test_dataset = test_dataset\n",
        "\n",
        "    self.class_names = [\"__background__\"] + list(class_names)\n",
        "    self.num_classes = len(self.class_names)\n",
        "\n",
        "    collate_fn = lambda x: tuple(zip(*x))\n",
        "    self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    self.val_loader = torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    if len(self.test_dataset) > 0:\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    else:\n",
        "        self.test_loader = None\n",
        "\n",
        "    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    self.model = fasterrcnn_resnet50_fpn(weights=weights)\n",
        "    self.in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "    self.model.roi_heads.box_predictor = FastRCNNPredictor(self.in_features, self.num_classes)\n",
        "    self.model.to(DEVICE)\n",
        "\n",
        "  def train(self, num_epochs=50, learning_rate=0.0005):\n",
        "    # Example optimizer\n",
        "    optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
        "\n",
        "      # Training loop\n",
        "      print(\"\\tStarting training loop...\")\n",
        "      train_loss = 0.0\n",
        "      self.model.train()\n",
        "      for images, targets in self.train_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = self.model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += losses.item()\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      print(\"\\t...Training loop complete.\")\n",
        "\n",
        "      # Validation loop\n",
        "      print(\"\\tStarting validation loop...\")\n",
        "      val_loss = 0.0\n",
        "      with torch.no_grad():\n",
        "        self.model.train()\n",
        "        for images, targets in self.val_loader:\n",
        "          images = [img.to(DEVICE) for img in images]\n",
        "          targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "          loss_dict = self.model(images, targets)\n",
        "          losses = sum(loss for loss in loss_dict.values())\n",
        "          val_loss += losses.item()\n",
        "      print(\"\\t...Validation loop complete.\")\n",
        "\n",
        "      print(f\"...Finished epoch {epoch+1}/{num_epochs}, Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    self.save_weights()\n",
        "\n",
        "  def test_results(self):\n",
        "    if self.test_loader is None:\n",
        "        print(\"Test dataset is empty. Skipping evaluation.\")\n",
        "        return [], [], []\n",
        "\n",
        "    self.load_weights()\n",
        "    all_boxes, all_labels, all_scores = [], [], []\n",
        "    with torch.no_grad():\n",
        "      self.model.eval()\n",
        "      for images, _ in self.test_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        predictions = self.model(images)\n",
        "        for prediction in predictions:\n",
        "          boxes, labels, scores = prediction['boxes'], prediction['labels'], prediction['scores']\n",
        "          all_boxes.append(boxes.detach().cpu().numpy())\n",
        "          all_labels.append(labels.detach().cpu().numpy())\n",
        "          all_scores.append(scores.detach().cpu().numpy())\n",
        "    return all_boxes, all_labels, all_scores\n",
        "\n",
        "  def save_weights(self):\n",
        "    torch.save({\n",
        "        \"model_state_dict\": self.model.state_dict(),\n",
        "        \"class_names\": self.class_names\n",
        "    }, self.model_path)\n",
        "\n",
        "  def load_weights(self):\n",
        "    checkpoint = torch.load(self.model_path, map_location=DEVICE)\n",
        "    self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    self.class_names = checkpoint[\"class_names\"]"
      ],
      "metadata": {
        "id": "Y1hJruRt2yVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset models"
      ],
      "metadata": {
        "id": "20AyC4tw8HuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_MODEL = FasterRCNNModel(Path(\"hrsc_unrotated_faster_rcnn_model.pth\"), HRSC_TRAIN_DATASET, HRSC_VAL_DATASET, HRSC_TEST_DATASET, [id for id, _ in sorted(HRSC_CLASSES.items(), key=lambda item: item[1])])\n",
        "DOTA_MODEL = FasterRCNNModel(Path(\"dota_unrotated_faster_rcnn_model.pth\"), DOTA_TRAIN_DATASET, DOTA_VAL_DATASET, DOTA_TEST_DATASET, [id for id, _ in sorted(DOTA_CLASSES.items(), key=lambda item: item[1])])\n",
        "NWPU_MODEL = FasterRCNNModel(Path(\"nwpu_unrotated_faster_rcnn_model.pth\"), NWPU_TRAIN_DATASET, NWPU_VAL_DATASET, NWPU_TEST_DATASET, [id for id, _ in sorted(NWPU_CLASSES.items(), key=lambda item: item[1])])"
      ],
      "metadata": {
        "id": "tFPaQy7t8MaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WJ1EUsg8sZQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train dataset models"
      ],
      "metadata": {
        "id": "eLEecH3487cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if HRSC_MODEL.model_path.exists():\n",
        "  print(\"Loading HRSC weights...\")\n",
        "  HRSC_MODEL.load_weights()\n",
        "  print(\"...HRSC weights loaded.\")\n",
        "else:\n",
        "  print(\"Training HRSC model...\")\n",
        "  HRSC_MODEL.train(3)\n",
        "  print(\"...HRSC model trained.\")\n",
        "\n",
        "if DOTA_MODEL.model_path.exists():\n",
        "  print(\"Loading DOTA weights...\")\n",
        "  DOTA_MODEL.load_weights()\n",
        "  print(\"...DOTA weights loaded.\")\n",
        "else:\n",
        "  print(\"Training DOTA model...\")\n",
        "  DOTA_MODEL.train(3)\n",
        "  print(\"...DOTA model trained.\")\n",
        "\n",
        "if NWPU_MODEL.model_path.exists():\n",
        "  print(\"Loading NWPU weights...\")\n",
        "  NWPU_MODEL.load_weights()\n",
        "  print(\"...NWPU weights loaded.\")\n",
        "else:\n",
        "  print(\"Training NWPU model...\")\n",
        "  NWPU_MODEL.train(3)\n",
        "  print(\"...NWPU model trained.\")"
      ],
      "metadata": {
        "id": "-_VvrW4p9NVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate dataset models"
      ],
      "metadata": {
        "id": "fo9xloKX89bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_MODEL.model.roi_heads.score_thresh = 1.0e-6\n",
        "HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES = HRSC_MODEL.test_results()\n",
        "\n",
        "DOTA_MODEL.model.roi_heads.score_thresh = 0.012\n",
        "DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES = DOTA_MODEL.test_results()\n",
        "\n",
        "NWPU_MODEL.model.roi_heads.score_thresh = 0.15\n",
        "NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES = NWPU_MODEL.test_results()"
      ],
      "metadata": {
        "id": "sthCTKuq9Pmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "FJFkIFmdPr4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class Visualizer:\n",
        "  def __init__(self, test_dataset: TorchDataset, class_names: list, boxes: list, labels: list, scores: list, results_folder: Path):\n",
        "    self.test_dataset = test_dataset\n",
        "    self.class_names = class_names\n",
        "    self.boxes = boxes\n",
        "    self.labels = labels\n",
        "    self.scores = scores\n",
        "    self.results_folder = results_folder\n",
        "    self.results_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  def overlay_rotated_box(self, output, box, wmult, hmult, color, label, score):\n",
        "    xmin, ymin, xmax, ymax = box\n",
        "    theta = 0\n",
        "\n",
        "    category = self.class_names[label]\n",
        "    if score is None:\n",
        "      text = f\"{category}\"\n",
        "    else:\n",
        "      text = f\"{category} - score={score:.3g}\"\n",
        "\n",
        "    center_x = float((xmin + xmax) / 2) * wmult\n",
        "    center_y = float((ymin + ymax) / 2) * hmult\n",
        "    width = float(xmax - xmin) * wmult\n",
        "    height = float(ymax - ymin) * hmult\n",
        "    box_points = cv2.boxPoints(((center_x, center_y), (width, height), theta)).astype(np.int32)\n",
        "\n",
        "    cv2.drawContours(output, [box_points], 0, color, 1)\n",
        "    text_pos = tuple(box_points[1])\n",
        "    cv2.putText(output, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "  def visualize(self, index):\n",
        "    image, target = self.test_dataset[index]\n",
        "    wmult, hmult = self.test_dataset.get_image_rescale(index)\n",
        "\n",
        "    # Convert tensor image (C,H,W) â†’ numpy (H,W,C)\n",
        "    if isinstance(image, np.ndarray):\n",
        "      output = image.copy()\n",
        "    else:\n",
        "      output = image.permute(1, 2, 0).cpu().numpy()\n",
        "      output = (output * 255).astype(np.uint8).copy()\n",
        "\n",
        "    # Ground truth\n",
        "    true_boxes = target[\"boxes\"]\n",
        "    true_labels = target[\"labels\"]\n",
        "\n",
        "    # Predictions\n",
        "    predicted_boxes = self.boxes[index]\n",
        "    predicted_labels = self.labels[index]\n",
        "    predicted_scores = self.scores[index]\n",
        "\n",
        "    # Draw boxes\n",
        "    for box, label in zip(true_boxes, true_labels):\n",
        "      self.overlay_rotated_box(output, box, wmult, hmult, (0, 255, 0), label, None)\n",
        "\n",
        "    for box, label, score in zip(predicted_boxes, predicted_labels, predicted_scores):\n",
        "      self.overlay_rotated_box(output, box, 1.0, 1.0, (255, 0, 0), label, score)\n",
        "\n",
        "    # Save output\n",
        "    output_path = self.results_folder / f\"{self.test_dataset.ids[index]}.png\"\n",
        "    cv2.imwrite(output_path.as_posix(), cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n",
        "    print(f\"Saved visualization to {output_path.as_posix()}\")\n",
        "\n",
        "  def visualize_multiple(self, count = 100, start_index = 0, max_workers=4):\n",
        "    end_index = min(start_index + count, len(self.test_dataset)) if count > 0 else len(self.test_dataset)\n",
        "    indices = list(range(start_index, end_index))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "      futures = {executor.submit(self.visualize, i): i for i in indices}\n",
        "      for future in as_completed(futures):\n",
        "        idx = futures[future]\n",
        "        try:\n",
        "          future.result()\n",
        "        except Exception as e:\n",
        "          print(Fore.RED + f\"Visualization failed for index {idx}:\" + Style.RESET_ALL, e)"
      ],
      "metadata": {
        "id": "kVbUuJJfPrhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize results"
      ],
      "metadata": {
        "id": "vG0e5VAvc7lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESULTS_PARENT_FOLDER = Path(\"drive/MyDrive/Colab Notebooks/Results\")\n",
        "HRSC_VISUALIZER = Visualizer(HRSC_TEST_DATASET, HRSC_MODEL.class_names, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES, RESULTS_PARENT_FOLDER / \"unrotated\" / \"HRSC\")\n",
        "DOTA_VISUALIZER = Visualizer(DOTA_TEST_DATASET, DOTA_MODEL.class_names, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES, RESULTS_PARENT_FOLDER / \"unrotated\" / \"DOTA\")\n",
        "NWPU_VISUALIZER = Visualizer(NWPU_TEST_DATASET, NWPU_MODEL.class_names, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES, RESULTS_PARENT_FOLDER / \"unrotated\" / \"NWPU\")\n",
        "\n",
        "HRSC_VISUALIZER.visualize_multiple()\n",
        "DOTA_VISUALIZER.visualize_multiple()\n",
        "NWPU_VISUALIZER.visualize_multiple()"
      ],
      "metadata": {
        "id": "KzLhU-RJc_5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Computation"
      ],
      "metadata": {
        "id": "MrZTUNbzfhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "class Statistician:\n",
        "  def __init__(self, test_dataset: TorchDataset, predicted_boxes: list, predicted_labels: list, predicted_scores: list):\n",
        "    self.metric = MeanAveragePrecision()\n",
        "    self.targets = [target for image, target in test_dataset]\n",
        "    self.predictions = [\n",
        "        {\n",
        "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
        "            \"scores\": torch.as_tensor(scores, dtype=torch.float32)\n",
        "        }\n",
        "        for boxes, labels, scores in zip(predicted_boxes, predicted_labels, predicted_scores)\n",
        "    ]\n",
        "    self.metric.update(self.predictions, self.targets)\n",
        "    self.result = self.metric.compute()\n",
        "\n",
        "  def get_map(self):\n",
        "    return self.result[\"map\"]\n",
        "\n",
        "  def get_map_percent(self):\n",
        "    return self.get_map().detach().cpu().numpy() * 100\n",
        "\n",
        "  def get_map_50(self):\n",
        "    return self.result[\"map_50\"]\n",
        "\n",
        "  def get_map_75(self):\n",
        "    return self.result[\"map_75\"]\n",
        "\n",
        "  def get_map_small(self):\n",
        "    return self.result[\"map_small\"]\n",
        "\n",
        "  def get_map_medium(self):\n",
        "    return self.result[\"map_medium\"]\n",
        "\n",
        "  def get_map_large(self):\n",
        "    return self.result[\"map_large\"]\n",
        "\n",
        "  def get_mar_1(self):\n",
        "    return self.result[\"mar_1\"]\n",
        "\n",
        "  def get_mar_10(self):\n",
        "    return self.result[\"mar_10\"]\n",
        "\n",
        "  def get_mar_100(self):\n",
        "    return self.result[\"mar_100\"]\n",
        "\n",
        "  def get_mar_small(self):\n",
        "    return self.result[\"mar_small\"]\n",
        "\n",
        "  def get_mar_medium(self):\n",
        "    return self.result[\"mar_medium\"]\n",
        "\n",
        "  def get_mar_large(self):\n",
        "    return self.result[\"mar_large\"]\n",
        "\n",
        "  def get_map_per_class(self):\n",
        "    return self.result[\"map_per_class\"]\n",
        "\n",
        "  def get_mar_100_per_class(self):\n",
        "    return self.result[\"mar_100_per_class\"]\n",
        "\n",
        "  def get_classes(self):\n",
        "    return self.result[\"classes\"]"
      ],
      "metadata": {
        "id": "o1_haXv1fmcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print Statistics"
      ],
      "metadata": {
        "id": "i-TRuIeBhtvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_PRED_STATS = Statistician(HRSC_TEST_DATASET, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES)\n",
        "DOTA_PRED_STATS = Statistician(DOTA_TEST_DATASET, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES)\n",
        "NWPU_PRED_STATS = Statistician(NWPU_TEST_DATASET, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES)\n",
        "\n",
        "print(\"HRSC prediction statistics:\")\n",
        "print(f\"{HRSC_PRED_STATS.get_map_percent()}%\")\n",
        "print(\"DOTA prediction statistics:\")\n",
        "print(f\"{DOTA_PRED_STATS.get_map_percent()}%\")\n",
        "print(\"NWPU prediction statistics:\")\n",
        "print(f\"{NWPU_PRED_STATS.get_map_percent()}%\")"
      ],
      "metadata": {
        "id": "nkicIfHJsf2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}