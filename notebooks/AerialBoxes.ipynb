{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q opencv-python\n",
        "!pip install -q colorama\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q shapely\n",
        "!pip install -q tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.models.detection.roi_heads import RoIHeads\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as F_transforms\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from detectron2.layers import roi_align_rotated\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import itertools\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import pprint\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from shapely.geometry import Polygon\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from colorama import Fore, Style\n",
        "from tqdm import tqdm\n",
        "import gc"
      ],
      "metadata": {
        "id": "OkbHdJGpBF-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iImd1rYBBJku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHARED_PATH = Path(\"drive/MyDrive/Colab Notebooks/Shared\")\n",
        "HRSC_PATH = SHARED_PATH / \"HRSC2016_Final_Splits\"\n",
        "DOTA_PATH = SHARED_PATH / \"DOTA_Final_Splits\"\n",
        "NWPU_PATH = SHARED_PATH / \"NWPU_VHR-10_Final_Splits\"\n",
        "\n",
        "HRSC_TRAIN_IMAGES = HRSC_PATH / \"train/images\"\n",
        "HRSC_TRAIN_ANNOTATIONS = HRSC_PATH / \"train/annotations\"\n",
        "HRSC_VAL_IMAGES = HRSC_PATH / \"val/images\"\n",
        "HRSC_VAL_ANNOTATIONS = HRSC_PATH / \"val/annotations\"\n",
        "HRSC_TEST_IMAGES = HRSC_PATH / \"test/images\"\n",
        "HRSC_TEST_ANNOTATIONS = HRSC_PATH / \"test/annotations\"\n",
        "\n",
        "DOTA_TRAIN_IMAGES = DOTA_PATH / \"train/images\"\n",
        "DOTA_TRAIN_ANNOTATIONS = DOTA_PATH / \"train/hbb\"\n",
        "DOTA_VAL_IMAGES = DOTA_PATH / \"val/images\"\n",
        "DOTA_VAL_ANNOTATIONS = DOTA_PATH / \"val/hbb\"\n",
        "DOTA_TEST_IMAGES = DOTA_PATH / \"test/images\"\n",
        "DOTA_TEST_ANNOTATIONS = DOTA_PATH / \"test/hbb\"\n",
        "\n",
        "NWPU_TRAIN_IMAGES = NWPU_PATH / \"train/images\"\n",
        "NWPU_TRAIN_ANNOTATIONS = NWPU_PATH / \"train/annotations\"\n",
        "NWPU_VAL_IMAGES = NWPU_PATH / \"val/images\"\n",
        "NWPU_VAL_ANNOTATIONS = NWPU_PATH / \"val/annotations\"\n",
        "NWPU_TEST_IMAGES = NWPU_PATH / \"test/images\"\n",
        "NWPU_TEST_ANNOTATIONS = NWPU_PATH / \"test/annotations\""
      ],
      "metadata": {
        "id": "HhT6C7jBBVYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_CLASSES = {}\n",
        "DOTA_CLASSES = {}\n",
        "NWPU_CLASSES = {}"
      ],
      "metadata": {
        "id": "Uy67NT4sBYiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_hrsc_labels(label_path: Path):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    tree = ET.parse(label_path.as_posix())\n",
        "    root = tree.getroot()\n",
        "    objects = root.findall(\".//HRSC_Object\")\n",
        "    for obj in objects:\n",
        "        try:\n",
        "            center_x = float(obj.find('mbox_cx').text)\n",
        "            center_y = float(obj.find('mbox_cy').text)\n",
        "            width = float(obj.find('mbox_w').text)\n",
        "            height = float(obj.find('mbox_h').text)\n",
        "            angle = float(obj.find('mbox_ang').text)\n",
        "            class_id = int(obj.find('Class_ID').text)\n",
        "            if class_id not in HRSC_CLASSES:\n",
        "                HRSC_CLASSES[class_id] = len(HRSC_CLASSES)\n",
        "            class_id_index = HRSC_CLASSES[class_id]\n",
        "            boxes.append([center_x, center_y, width, height, angle])\n",
        "            labels.append(class_id_index)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    return boxes, labels\n",
        "\n",
        "def hrsc_image_rescale(label_path: Path, image_width, image_height):\n",
        "    tree = ET.parse(label_path.as_posix())\n",
        "    root = tree.getroot()\n",
        "    width = int(root.find('.//Img_SizeWidth').text)\n",
        "    height = int(root.find('.//Img_SizeHeight').text)\n",
        "    return image_width / width, image_height / height\n",
        "\n",
        "def parse_dota_labels(label_path: Path):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for line in itertools.islice(label_path.read_text().splitlines(), 2, None):\n",
        "        try:\n",
        "            obj = line.strip().split()\n",
        "            x1, y1, x2, y2, x3, y3, x4, y4, category, difficulty = (*map(float, obj[:8]), *obj[8:])\n",
        "            points = [(x1, y1), (x2, y2), (x3, y3), (x4, y4)]\n",
        "            pts_np = np.array(points, dtype=np.float32).reshape(-1, 1, 2)\n",
        "            (cx, cy), (w, h), angle_deg = cv2.minAreaRect(pts_np)\n",
        "            angle_rad = math.radians(angle_deg)\n",
        "            if category not in DOTA_CLASSES:\n",
        "                DOTA_CLASSES[category] = len(DOTA_CLASSES)\n",
        "            class_id_index = DOTA_CLASSES[category]\n",
        "            boxes.append([cx, cy, w, h, angle_rad])\n",
        "            labels.append(class_id_index)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    return boxes, labels\n",
        "\n",
        "def dota_image_rescale(label_path: Path, image_width, image_height):\n",
        "    return 1.0, 1.0\n",
        "\n",
        "def parse_nwpu_labels(label_path: Path):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    data = json.loads(label_path.read_text())\n",
        "    for category in data['categories']:\n",
        "        NWPU_CLASSES[category['name']] = category['id'] - 1  # categories are 1-indexed -> convert to 0-indexed\n",
        "\n",
        "    for annotation in data['annotations']:\n",
        "        try:\n",
        "            # Bounds\n",
        "            segmentation = annotation['segmentation'][0]\n",
        "            pts_np = np.array(segmentation, dtype=np.float32).reshape(-1, 2)\n",
        "            (cx, cy), (w, h), angle_deg = cv2.minAreaRect(pts_np)\n",
        "            angle_rad = math.radians(angle_deg)\n",
        "\n",
        "            # Category\n",
        "            class_id_index = annotation['category_id'] - 1\n",
        "\n",
        "            boxes.append([cx, cy, w, h, angle_rad])\n",
        "            labels.append(class_id_index)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return boxes, labels\n",
        "\n",
        "def nwpu_image_rescale(label_path: Path, image_width, image_height):\n",
        "    data = json.loads(label_path.read_text())\n",
        "    image = data['images'][0]\n",
        "    width = image['width']\n",
        "    height = image['height']\n",
        "    return image_width / width, image_height / height\n",
        "\n",
        "def dbbox2delta(proposals, gt, means=[0, 0, 0, 0, 0], stds=[1, 1, 1, 1, 1]):\n",
        "    proposals = proposals.float()\n",
        "    gt = gt.float()\n",
        "    gt_widths = gt[..., 2]\n",
        "    gt_heights = gt[..., 3]\n",
        "    gt_angle = gt[..., 4]\n",
        "    proposals_widths = proposals[..., 2]\n",
        "    proposals_heights = proposals[..., 3]\n",
        "    proposals_angle = proposals[..., 4]\n",
        "    coord = gt[..., 0:2] - proposals[..., 0:2]\n",
        "    dx = (torch.cos(proposals[..., 4]) * coord[..., 0] + torch.sin(proposals[..., 4]) * coord[..., 1]) / proposals_widths\n",
        "    dy = (-torch.sin(proposals[..., 4]) * coord[..., 0] + torch.cos(proposals[..., 4]) * coord[..., 1]) / proposals_heights\n",
        "    dw = torch.log(gt_widths / proposals_widths)\n",
        "    dh = torch.log(gt_heights / proposals_heights)\n",
        "    dangle = (gt_angle - proposals_angle) % (2 * math.pi) / (2 * math.pi)\n",
        "    deltas = torch.stack((dx, dy, dw, dh, dangle), -1)\n",
        "    means = deltas.new_tensor(means).unsqueeze(0)\n",
        "    stds = deltas.new_tensor(stds).unsqueeze(0)\n",
        "    deltas = deltas.sub_(means).div_(stds)\n",
        "    return deltas\n",
        "\n",
        "def delta2dbbox(Rrois, deltas, means=[0, 0, 0, 0, 0], stds=[1, 1, 1, 1, 1], max_shape=None, wh_ratio_clip=16/1000):\n",
        "    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 5)\n",
        "    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 5)\n",
        "    denorm_deltas = deltas * stds + means\n",
        "    dx = denorm_deltas[:, 0::5]\n",
        "    dy = denorm_deltas[:, 1::5]\n",
        "    dw = denorm_deltas[:, 2::5]\n",
        "    dh = denorm_deltas[:, 3::5]\n",
        "    dangle = denorm_deltas[:, 4::5]\n",
        "    max_ratio = np.abs(np.log(wh_ratio_clip))\n",
        "    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n",
        "    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n",
        "    Rroi_x = (Rrois[:, 0]).unsqueeze(1).expand_as(dx)\n",
        "    Rroi_y = (Rrois[:, 1]).unsqueeze(1).expand_as(dy)\n",
        "    Rroi_w = (Rrois[:, 2]).unsqueeze(1).expand_as(dw)\n",
        "    Rroi_h = (Rrois[:, 3]).unsqueeze(1).expand_as(dh)\n",
        "    Rroi_angle = (Rrois[:, 4]).unsqueeze(1).expand_as(dangle)\n",
        "    gx = dx * Rroi_w * torch.cos(Rroi_angle) - dy * Rroi_h * torch.sin(Rroi_angle) + Rroi_x\n",
        "    gy = dx * Rroi_w * torch.sin(Rroi_angle) + dy * Rroi_h * torch.cos(Rroi_angle) + Rroi_y\n",
        "    gw = Rroi_w * dw.exp()\n",
        "    gh = Rroi_h * dh.exp()\n",
        "    gangle = (2 * np.pi) * dangle + Rroi_angle\n",
        "    gangle = gangle % (2 * np.pi)\n",
        "    bboxes = torch.stack([gx, gy, gw, gh, gangle], dim=-1).view_as(deltas)\n",
        "    return bboxes\n",
        "\n",
        "def hbb2obb_v2(boxes):\n",
        "    num_boxes = boxes.size(0)\n",
        "    ex_heights = boxes[..., 2] - boxes[..., 0] + 1.0\n",
        "    ex_widths = boxes[..., 3] - boxes[..., 1] + 1.0\n",
        "    ex_ctr_x = boxes[..., 0] + 0.5 * (ex_heights - 1.0)\n",
        "    ex_ctr_y = boxes[..., 1] + 0.5 * (ex_widths - 1.0)\n",
        "    c_bboxes = torch.cat((ex_ctr_x.unsqueeze(1), ex_ctr_y.unsqueeze(1), ex_widths.unsqueeze(1), ex_heights.unsqueeze(1)), 1)\n",
        "    initial_angles = -c_bboxes.new_ones((num_boxes, 1)) * np.pi / 2\n",
        "    dbboxes = torch.cat((c_bboxes, initial_angles), 1)\n",
        "    return dbboxes\n",
        "\n",
        "def rotated_box_to_polygon(box):\n",
        "    if isinstance(box, torch.Tensor):\n",
        "        box = box.cpu().numpy()\n",
        "    if len(box) == 4:\n",
        "        x1, y1, x2, y2 = box\n",
        "        cx = (x1 + x2) / 2\n",
        "        cy = (y1 + y2) / 2\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "        angle = 0.0\n",
        "    elif len(box) == 5:\n",
        "        cx, cy, w, h, angle = box\n",
        "    else:\n",
        "        raise ValueError(f\"Box must have 4 or 5 elements, got {len(box)}\")\n",
        "    hw = w / 2\n",
        "    hh = h / 2\n",
        "    corners = np.array([[-hw, -hh], [hw, -hh], [hw, hh], [-hw, hh]], dtype=np.float32)\n",
        "    cosA = np.cos(angle)\n",
        "    sinA = np.sin(angle)\n",
        "    R = np.array([[cosA, -sinA], [sinA, cosA]])\n",
        "    rotated = corners @ R.T\n",
        "    rotated[:, 0] += cx\n",
        "    rotated[:, 1] += cy\n",
        "    return Polygon(rotated.tolist())\n",
        "\n",
        "def box_iou_rotated(boxes1, boxes2):\n",
        "    N = boxes1.shape[0]\n",
        "    M = boxes2.shape[0]\n",
        "    ious = torch.zeros((N, M), dtype=torch.float32, device=boxes1.device)\n",
        "    for i in range(N):\n",
        "        try:\n",
        "            p1 = rotated_box_to_polygon(boxes1[i])\n",
        "            area1 = p1.area\n",
        "            if area1 <= 0:\n",
        "                continue\n",
        "            for j in range(M):\n",
        "                try:\n",
        "                    p2 = rotated_box_to_polygon(boxes2[j])\n",
        "                    inter = p1.intersection(p2).area\n",
        "                    union = area1 + p2.area - inter\n",
        "                    if union > 0:\n",
        "                        ious[i, j] = inter / union\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            continue\n",
        "    return ious\n",
        "\n",
        "BASIC_TRANSFORM = T.Compose([T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_folder: Path, annotations_folder: Path, label_parser, image_rescale, transforms=BASIC_TRANSFORM, max_files=10, max_objects=100, target_size=800):\n",
        "        super().__init__()\n",
        "        self.label_parser = label_parser\n",
        "        self.image_rescale = image_rescale\n",
        "        self.transforms = transforms\n",
        "        self.max_objects = max_objects\n",
        "        self.target_size = target_size\n",
        "\n",
        "        def sorting_key(p: Path):\n",
        "            m = re.search(r'(\\d+)', p.stem)\n",
        "            if m:\n",
        "                return int(m.group())\n",
        "            else:\n",
        "                return float('inf')\n",
        "\n",
        "        if images_folder.exists():\n",
        "            images = sorted([f for f in images_folder.iterdir() if f.suffix.lower() in (\".jpg\", \".png\", \".bmp\")], key=sorting_key)\n",
        "        else:\n",
        "            images = []\n",
        "\n",
        "        if annotations_folder.exists():\n",
        "            annotations = sorted([f for f in annotations_folder.iterdir() if f.suffix.lower() in (\".xml\", \".txt\", \".json\")], key=sorting_key)\n",
        "        else:\n",
        "            annotations = []\n",
        "\n",
        "        if max_files > 0:\n",
        "            images = images[:max_files]\n",
        "            annotations = annotations[:max_files]\n",
        "\n",
        "        image_set = set(f.stem for f in images)\n",
        "        annotation_set = set(f.stem for f in annotations)\n",
        "        self.ids = image_set.intersection(annotation_set)\n",
        "        self.images = {f.stem: f for f in images if f.stem in self.ids}\n",
        "        self.annotations = {f.stem: f for f in annotations if f.stem in self.ids}\n",
        "        self.ids = list(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id = self.ids[index]\n",
        "        image_path = self.images[id]\n",
        "        label_path = self.annotations[id]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        boxes, labels = self.label_parser(label_path)\n",
        "        if self.max_objects > 0:\n",
        "            boxes = boxes[:self.max_objects]\n",
        "            labels = labels[:self.max_objects]\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 5), dtype=torch.float32)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            if boxes.dim() == 1:\n",
        "                boxes = boxes.unsqueeze(0)\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        labels = labels + 1\n",
        "        image, boxes = self.preprocess(image, boxes)\n",
        "        image = F_transforms.to_tensor(image)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "        image = self.transforms(image)\n",
        "        image.filepath = image_path\n",
        "        return image, target\n",
        "\n",
        "    def preprocess(self, img, boxes):\n",
        "        old_w, old_h = img.width, img.height\n",
        "        scale = self.target_size / max(old_h, old_w)\n",
        "        new_w = int(old_w * scale)\n",
        "        new_h = int(old_h * scale)\n",
        "        img = F_transforms.resize(img, (new_h, new_w))\n",
        "        boxes = boxes.clone()\n",
        "        if boxes.numel() > 0 and boxes.dim() == 2:\n",
        "            boxes[:, 0] *= scale\n",
        "            boxes[:, 1] *= scale\n",
        "            boxes[:, 2] *= scale\n",
        "            boxes[:, 3] *= scale\n",
        "        pad_w = self.target_size - new_w\n",
        "        pad_h = self.target_size - new_h\n",
        "        img = F_transforms.pad(img, (0, 0, pad_w, pad_h))\n",
        "        return img, boxes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def get_image_rescale(self, index):\n",
        "        id = self.ids[index]\n",
        "        image_path = self.images[id]\n",
        "        label_path = self.annotations[id]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return self.image_rescale(label_path, image.width, image.height)\n",
        "\n",
        "    def compute_total_number_of_objects(self, max_workers=8):\n",
        "        def count_objects(id):\n",
        "            boxes, _ = self.label_parser(self.annotations[id])\n",
        "            return len(boxes)\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            results = executor.map(count_objects, self.ids)\n",
        "        return sum(results)\n",
        "\n",
        "print(\"Preparing HRSC training dataset...\")\n",
        "HRSC_TRAIN_DATASET = TorchDataset(HRSC_TRAIN_IMAGES, HRSC_TRAIN_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing HRSC validation dataset...\")\n",
        "HRSC_VAL_DATASET = TorchDataset(HRSC_VAL_IMAGES, HRSC_VAL_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing HRSC testing dataset...\")\n",
        "HRSC_TEST_DATASET = TorchDataset(HRSC_TEST_IMAGES, HRSC_TEST_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing DOTA training dataset...\")\n",
        "DOTA_TRAIN_DATASET = TorchDataset(DOTA_TRAIN_IMAGES, DOTA_TRAIN_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing DOTA validation dataset...\")\n",
        "DOTA_VAL_DATASET = TorchDataset(DOTA_VAL_IMAGES, DOTA_VAL_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing DOTA testing dataset...\")\n",
        "DOTA_TEST_DATASET = TorchDataset(DOTA_TEST_IMAGES, DOTA_TEST_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing NWPU training dataset...\")\n",
        "NWPU_TRAIN_DATASET = TorchDataset(NWPU_TRAIN_IMAGES, NWPU_TRAIN_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing NWPU validation dataset...\")\n",
        "NWPU_VAL_DATASET = TorchDataset(NWPU_VAL_IMAGES, NWPU_VAL_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing NWPU testing dataset...\")\n",
        "NWPU_TEST_DATASET = TorchDataset(NWPU_TEST_IMAGES, NWPU_TEST_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "class BenchmarkTracker:\n",
        "    def __init__(self):\n",
        "        self.data = defaultdict(list)\n",
        "\n",
        "    def start(self, key):\n",
        "        self.data[key].append({\"start\": time.time(), \"end\": None})\n",
        "\n",
        "    def stop(self, key):\n",
        "        self.data[key][-1][\"end\"] = time.time()\n",
        "\n",
        "    def summary(self):\n",
        "        report = {}\n",
        "        for key, records in self.data.items():\n",
        "            durations = [(r[\"end\"] - r[\"start\"]) for r in records if r[\"end\"]]\n",
        "            report[key] = {\n",
        "                \"total\": sum(durations),\n",
        "                \"avg\": np.mean(durations),\n",
        "                \"std\": np.std(durations),\n",
        "                \"count\": len(durations)\n",
        "            }\n",
        "        return report\n",
        "\n",
        "class RotatedBoxPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "        self.bbox_pred = nn.Linear(in_channels, num_classes * 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 4:\n",
        "            x = x.flatten(start_dim=1)\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "        return scores, bbox_deltas\n",
        "\n",
        "class RotatedRoIHeads(RoIHeads):\n",
        "    def __init__(self, box_roi_pool, box_head, box_predictor, fg_iou_thresh=0.5, bg_iou_thresh=0.5, batch_size_per_image=512, positive_fraction=0.25, bbox_reg_weights=None, score_thresh=0.05, nms_thresh=0.5, detections_per_img=100):\n",
        "        super().__init__(box_roi_pool, box_head, box_predictor, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction, bbox_reg_weights, score_thresh, nms_thresh, detections_per_img)\n",
        "        self.box_predictor = box_predictor\n",
        "        self.fg_iou_thresh = fg_iou_thresh\n",
        "        self.bg_iou_thresh = bg_iou_thresh\n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.detections_per_img = detections_per_img\n",
        "\n",
        "    def forward(self, features, proposals, image_shapes, targets=None):\n",
        "        if self.training and targets is not None:\n",
        "            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n",
        "        else:\n",
        "            labels = None\n",
        "            regression_targets = None\n",
        "            matched_idxs = None\n",
        "        proposals = [p.to(DEVICE) for p in proposals]\n",
        "        box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
        "        box_features = self.box_head(box_features)\n",
        "        rotated_proposals_list = []\n",
        "        for i, props in enumerate(proposals):\n",
        "            props = props.to(DEVICE)\n",
        "            x1, y1, x2, y2 = props.unbind(1)\n",
        "            cx = (x1 + x2) / 2\n",
        "            cy = (y1 + y2) / 2\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            angle = torch.zeros_like(cx)\n",
        "            batch_idx = torch.full_like(cx, i, dtype=torch.float32, device=DEVICE)\n",
        "            rotated = torch.stack([batch_idx, cx, cy, w, h, angle], dim=1)\n",
        "            rotated_proposals_list.append(rotated)\n",
        "        rotated_boxes = torch.cat(rotated_proposals_list, dim=0)\n",
        "        if isinstance(features, dict):\n",
        "            feature_map = list(features.values())[0]\n",
        "            spatial_scale = 1.0 / 4.0\n",
        "        else:\n",
        "            feature_map = features\n",
        "            spatial_scale = 1.0 / 4.0\n",
        "        rotated_features = roi_align_rotated(feature_map, rotated_boxes, (7, 7), spatial_scale, 2)\n",
        "        rotated_features = self.box_head(rotated_features)\n",
        "        class_logits, box_regression = self.box_predictor(rotated_features)\n",
        "        if self.training:\n",
        "            return {'class_logits': class_logits, 'box_regression': box_regression, 'proposals': proposals, 'matched_idxs': matched_idxs, 'labels': labels, 'regression_targets': regression_targets}\n",
        "        else:\n",
        "            return self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
        "\n",
        "    def postprocess_detections(self, class_logits, box_regression, proposals, image_shapes):\n",
        "        device = DEVICE\n",
        "        num_classes = class_logits.shape[-1]\n",
        "        boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in proposals]\n",
        "        pred_boxes = self.decode_boxes(box_regression, proposals)\n",
        "        pred_scores = F.softmax(class_logits, -1)\n",
        "        pred_boxes_list = pred_boxes.split(boxes_per_image, 0)\n",
        "        pred_scores_list = pred_scores.split(boxes_per_image, 0)\n",
        "        all_boxes = []\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "        for boxes, scores, image_shape in zip(pred_boxes_list, pred_scores_list, image_shapes):\n",
        "            boxes = boxes.to(device)\n",
        "            scores = scores.to(device)\n",
        "            boxes = self.clip_boxes_to_image(boxes, image_shape)\n",
        "            labels = torch.arange(num_classes, device=device)\n",
        "            labels = labels.view(1, -1).expand_as(scores)\n",
        "            boxes = boxes.reshape(-1, 5)\n",
        "            scores = scores.reshape(-1)\n",
        "            labels = labels.reshape(-1)\n",
        "            inds = labels > 0\n",
        "            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
        "            inds = scores > self.score_thresh\n",
        "            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
        "            keep = self.rotated_nms(boxes, scores, self.nms_thresh)\n",
        "            keep = keep[:self.detections_per_img]\n",
        "            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "            all_boxes.append(boxes)\n",
        "            all_scores.append(scores)\n",
        "            all_labels.append(labels)\n",
        "        return all_boxes, all_scores, all_labels\n",
        "\n",
        "    def decode_boxes(self, box_regression, proposals):\n",
        "        rotated_proposals = []\n",
        "        for props in proposals:\n",
        "            props = props.to(DEVICE)\n",
        "            rotated_props = hbb2obb_v2(props)\n",
        "            rotated_proposals.append(rotated_props)\n",
        "        rotated_proposals = torch.cat(rotated_proposals, dim=0)\n",
        "        num_classes = box_regression.shape[1] // 5\n",
        "        box_regression = box_regression.view(-1, num_classes, 5)\n",
        "        decoded_boxes = []\n",
        "        for cls_idx in range(num_classes):\n",
        "            deltas = box_regression[:, cls_idx, :]\n",
        "            decoded = delta2dbbox(rotated_proposals, deltas)\n",
        "            decoded_boxes.append(decoded)\n",
        "        decoded_boxes = torch.stack(decoded_boxes, dim=1)\n",
        "        return decoded_boxes\n",
        "\n",
        "    def clip_boxes_to_image(self, boxes, image_shape):\n",
        "        h, w = image_shape\n",
        "        boxes = boxes.clone().to(DEVICE)\n",
        "        w_tensor = torch.tensor(w, dtype=boxes.dtype, device=DEVICE)\n",
        "        h_tensor = torch.tensor(h, dtype=boxes.dtype, device=DEVICE)\n",
        "        boxes[:, 0] = boxes[:, 0].clamp(0, w_tensor)\n",
        "        boxes[:, 1] = boxes[:, 1].clamp(0, h_tensor)\n",
        "        boxes[:, 2] = boxes[:, 2].clamp(min=1)\n",
        "        boxes[:, 3] = boxes[:, 3].clamp(min=1)\n",
        "        return boxes\n",
        "\n",
        "    def rotated_nms(self, boxes, scores, iou_threshold):\n",
        "        if len(boxes) == 0:\n",
        "            return torch.empty((0,), dtype=torch.long, device=boxes.device)\n",
        "        keep = []\n",
        "        order = scores.argsort(descending=True)\n",
        "        while len(order) > 0:\n",
        "            i = order[0]\n",
        "            keep.append(i)\n",
        "            if len(order) == 1:\n",
        "                break\n",
        "            ious = box_iou_rotated(boxes[i:i+1], boxes[order[1:]])\n",
        "            ious = ious.squeeze(0)\n",
        "            inds = (ious <= iou_threshold).nonzero(as_tuple=True)[0]\n",
        "            order = order[inds + 1]\n",
        "        return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "    def select_training_samples(self, proposals, targets):\n",
        "        labels = []\n",
        "        matched_idxs = []\n",
        "        regression_targets = []\n",
        "        for props, target in zip(proposals, targets):\n",
        "            props = props.to(DEVICE)\n",
        "            rotated_props = hbb2obb_v2(props)\n",
        "            target_boxes = target['boxes'].to(DEVICE)\n",
        "            target_labels = target['labels'].to(DEVICE)\n",
        "            ious = box_iou_rotated(rotated_props, target_boxes)\n",
        "            max_ious, matched_idx = ious.max(dim=1)\n",
        "            label = torch.zeros(len(props), dtype=torch.long, device=DEVICE)\n",
        "            pos_mask = max_ious > self.fg_iou_thresh\n",
        "            if pos_mask.any():\n",
        "                label[pos_mask] = target_labels[matched_idx[pos_mask]]\n",
        "            labels.append(label)\n",
        "            matched_idxs.append(matched_idx)\n",
        "            if pos_mask.any():\n",
        "                matched_gt = target_boxes[matched_idx[pos_mask]]\n",
        "                deltas = dbbox2delta(rotated_props[pos_mask], matched_gt)\n",
        "                reg_target = torch.zeros(len(props), 5, device=DEVICE)\n",
        "                reg_target[pos_mask] = deltas\n",
        "            else:\n",
        "                reg_target = torch.zeros(len(props), 5, device=DEVICE)\n",
        "            regression_targets.append(reg_target)\n",
        "        return proposals, matched_idxs, labels, regression_targets\n",
        "\n",
        "class RotatedFasterRCNNModel(FasterRCNN):\n",
        "    def __init__(self, model_path: Path, train_dataset: TorchDataset, val_dataset: TorchDataset, test_dataset: TorchDataset, class_names: list, batch_size=4, shuffle_datasets=False):\n",
        "        self.model_path = model_path\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.class_names = [\"__background__\"] + list(class_names)\n",
        "        self.num_classes = len(self.class_names)\n",
        "        collate_fn = lambda x: tuple(zip(*x))\n",
        "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "        if len(self.test_dataset) > 0:\n",
        "            self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "        else:\n",
        "            self.test_loader = None\n",
        "        super().__init__(backbone=resnet_fpn_backbone(backbone_name='resnet50', weights=ResNet50_Weights.DEFAULT), num_classes=self.num_classes, rpn_anchor_generator=None)\n",
        "        self.in_features = self.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.box_detector = RotatedBoxPredictor(self.in_features, self.num_classes)\n",
        "        self.roi_heads = RotatedRoIHeads(self.roi_heads.box_roi_pool, self.roi_heads.box_head, self.box_detector)\n",
        "        self.to(DEVICE)\n",
        "        self.bench = BenchmarkTracker()\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def train_model(self, num_epochs=50, learning_rate=0.0005):\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
        "            print(\"\\tStarting training loop...\")\n",
        "            self.bench.start(\"train_epoch\")\n",
        "            train_loss = 0.0\n",
        "            self.train()\n",
        "            for images, targets in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                self.bench.start(\"train_batch\")\n",
        "                loss_dict = self(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                optimizer.zero_grad()\n",
        "                losses.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += losses.item()\n",
        "                self.bench.stop(\"train_batch\")\n",
        "            lr_scheduler.step()\n",
        "            self.bench.stop(\"train_epoch\")\n",
        "            print(\"\\t...Training loop complete.\")\n",
        "            print(\"\\tStarting validation loop...\")\n",
        "            self.bench.start(\"val_epoch\")\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                self.train()\n",
        "                for images, targets in self.val_loader:\n",
        "                    self.bench.start(\"val_batch\")\n",
        "                    loss_dict = self(images, targets)\n",
        "                    losses = sum(loss for loss in loss_dict.values())\n",
        "                    val_loss += losses.item()\n",
        "                    self.bench.stop(\"val_batch\")\n",
        "            self.bench.stop(\"val_epoch\")\n",
        "            print(\"\\t...Validation loop complete.\")\n",
        "            print(f\"...Finished epoch {epoch+1}/{num_epochs}, Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                self.save_weights()\n",
        "        self.save_weights()\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        self.training = targets is not None and self.training\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        image_sizes = [img.shape[-2:] for img in images]\n",
        "        images = ImageList(torch.stack(images), image_sizes)\n",
        "        features = self.backbone(images.tensors)\n",
        "        rpn_targets = []\n",
        "        if targets is not None:\n",
        "            for t in targets:\n",
        "                boxes = t['boxes'].to(DEVICE)\n",
        "                labels = t['labels'].to(DEVICE)\n",
        "                if boxes.numel() > 0 and boxes.dim() == 2 and boxes.shape[1] == 5:\n",
        "                    cx, cy, w, h, theta = boxes.unbind(1)\n",
        "                    x1 = cx - w/2\n",
        "                    y1 = cy - h/2\n",
        "                    x2 = cx + w/2\n",
        "                    y2 = cy + h/2\n",
        "                    rpn_targets.append({'boxes': torch.stack([x1, y1, x2, y2], dim=1), 'labels': labels})\n",
        "                else:\n",
        "                    rpn_targets.append({'boxes': torch.zeros((0, 4), dtype=torch.float32, device=DEVICE), 'labels': labels})\n",
        "        proposals, rpn_losses = self.rpn(images, features, rpn_targets if targets else None)\n",
        "        if targets is not None:\n",
        "            targets = [{'boxes': t['boxes'].to(DEVICE), 'labels': t['labels'].to(DEVICE)} for t in targets]\n",
        "        roi_outputs = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
        "        loss_dict = {}\n",
        "        if self.training and targets is not None:\n",
        "            loss_dict.update(rpn_losses)\n",
        "            class_logits = roi_outputs['class_logits']\n",
        "            labels = torch.cat(roi_outputs['labels'], dim=0)\n",
        "            loss_dict['loss_classifier'] = F.cross_entropy(class_logits, labels)\n",
        "            box_regression = roi_outputs['box_regression']\n",
        "            regression_targets = torch.cat(roi_outputs['regression_targets'], dim=0)\n",
        "            pos_mask = labels > 0\n",
        "            if pos_mask.any():\n",
        "                num_classes = box_regression.shape[1] // 5\n",
        "                box_regression = box_regression.view(-1, num_classes, 5)\n",
        "                reg_for_labels = box_regression[pos_mask, labels[pos_mask] - 1, :]\n",
        "                loss_dict['loss_box_reg'] = F.smooth_l1_loss(reg_for_labels, regression_targets[pos_mask], reduction='mean')\n",
        "            else:\n",
        "                loss_dict['loss_box_reg'] = torch.tensor(0.0, device=class_logits.device)\n",
        "            return loss_dict\n",
        "        else:\n",
        "            all_boxes, all_scores, all_labels = roi_outputs\n",
        "            return [{'boxes': boxes, 'scores': scores, 'labels': labels} for boxes, scores, labels in zip(all_boxes, all_scores, all_labels)]\n",
        "\n",
        "    def save_weights(self):\n",
        "        torch.save({\"model_state_dict\": self.state_dict(), \"class_names\": self.class_names}, self.model_path)\n",
        "\n",
        "    def load_weights(self):\n",
        "        checkpoint = torch.load(self.model_path, map_location=DEVICE)\n",
        "        self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        self.class_names = checkpoint[\"class_names\"]\n",
        "\n",
        "    def test_results(self):\n",
        "        if self.test_loader is None:\n",
        "            print(\"Test dataset is empty. Skipping evaluation.\")\n",
        "            return [], [], []\n",
        "        self.load_weights()\n",
        "        self.bench.start(\"test_total\")\n",
        "        all_boxes, all_labels, all_scores = [], [], []\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            for images, _ in self.test_loader:\n",
        "                self.bench.start(\"test_batch\")\n",
        "                images = [img.to(DEVICE) for img in images]\n",
        "                predictions = self(images)\n",
        "                for prediction in predictions:\n",
        "                    boxes = prediction['boxes'].detach().cpu().numpy()\n",
        "                    labels = prediction['labels'].detach().cpu().numpy()\n",
        "                    scores = prediction['scores'].detach().cpu().numpy()\n",
        "                    if len(boxes) == 0:\n",
        "                        boxes = np.zeros((0, 5), dtype=np.float32)\n",
        "                    if len(labels) == 0:\n",
        "                        labels = np.zeros((0,), dtype=np.int64)\n",
        "                    if len(scores) == 0:\n",
        "                        scores = np.zeros((0,), dtype=np.float32)\n",
        "                    all_boxes.append(boxes)\n",
        "                    all_labels.append(labels)\n",
        "                    all_scores.append(scores)\n",
        "                self.bench.stop(\"test_batch\")\n",
        "        self.bench.stop(\"test_total\")\n",
        "        return all_boxes, all_labels, all_scores\n",
        "\n",
        "MODEL_SAVE_PATH = Path(\"drive/MyDrive/Colab Notebooks/Models\")\n",
        "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "HRSC_MODEL = RotatedFasterRCNNModel(MODEL_SAVE_PATH / \"hrsc_faster_rcnn_model.pth\", HRSC_TRAIN_DATASET, HRSC_VAL_DATASET, HRSC_TEST_DATASET, [id for id, _ in sorted(HRSC_CLASSES.items(), key=lambda item: item[1])], batch_size=2)\n",
        "DOTA_MODEL = RotatedFasterRCNNModel(MODEL_SAVE_PATH / \"dota_faster_rcnn_model.pth\", DOTA_TRAIN_DATASET, DOTA_VAL_DATASET, DOTA_TEST_DATASET, [id for id, _ in sorted(DOTA_CLASSES.items(), key=lambda item: item[1])], batch_size=2)\n",
        "NWPU_MODEL = RotatedFasterRCNNModel(MODEL_SAVE_PATH / \"nwpu_faster_rcnn_model.pth\", NWPU_TRAIN_DATASET, NWPU_VAL_DATASET, NWPU_TEST_DATASET, [id for id, _ in sorted(NWPU_CLASSES.items(), key=lambda item: item[1])], batch_size=2)\n",
        "\n",
        "if HRSC_MODEL.model_path.exists():\n",
        "    print(\"Loading HRSC weights...\")\n",
        "    HRSC_MODEL.load_weights()\n",
        "    print(\"...HRSC weights loaded.\")\n",
        "else:\n",
        "    print(\"Training HRSC model...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    HRSC_MODEL.train_model(3)\n",
        "    print(\"...HRSC model trained.\")\n",
        "\n",
        "if DOTA_MODEL.model_path.exists():\n",
        "    print(\"Loading DOTA weights...\")\n",
        "    DOTA_MODEL.load_weights()\n",
        "    print(\"...DOTA weights loaded.\")\n",
        "else:\n",
        "    print(\"Training DOTA model...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    DOTA_MODEL.train_model(3)\n",
        "    print(\"...DOTA model trained.\")\n",
        "\n",
        "if NWPU_MODEL.model_path.exists():\n",
        "    print(\"Loading NWPU weights...\")\n",
        "    NWPU_MODEL.load_weights()\n",
        "    print(\"...NWPU weights loaded.\")\n",
        "else:\n",
        "    print(\"Training NWPU model...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    NWPU_MODEL.train_model(3)\n",
        "    print(\"...NWPU model trained.\")"
      ],
      "metadata": {
        "id": "aJATfSSyHKRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_MODEL.roi_heads.score_thresh = 1.0e-6\n",
        "HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES = HRSC_MODEL.test_results()\n",
        "\n",
        "DOTA_MODEL.roi_heads.score_thresh = 0.012\n",
        "DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES = DOTA_MODEL.test_results()\n",
        "\n",
        "NWPU_MODEL.roi_heads.score_thresh = 0.001\n",
        "NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES = NWPU_MODEL.test_results()"
      ],
      "metadata": {
        "id": "z2kU2QYzHn4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Visualizer:\n",
        "    def __init__(self, test_dataset: TorchDataset, class_names: list,\n",
        "                 boxes: list, labels: list, scores: list, results_folder: Path,\n",
        "                 normalize_mean=(0.485, 0.456, 0.406),\n",
        "                 normalize_std=(0.229, 0.224, 0.225)):\n",
        "\n",
        "        self.test_dataset = test_dataset\n",
        "        self.class_names = class_names\n",
        "        self.boxes = boxes\n",
        "        self.labels = labels\n",
        "        self.scores = scores\n",
        "        self.results_folder = results_folder\n",
        "        self.results_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # normalization undo params\n",
        "        self.mean = np.array(normalize_mean)\n",
        "        self.std = np.array(normalize_std)\n",
        "\n",
        "    # HELPERS\n",
        "\n",
        "    def unnormalize(self, img):\n",
        "        \"\"\"\n",
        "        img: float32 numpy array in [C,H,W] or [H,W,C] with ImageNet normalization.\n",
        "        Returns image in [0,1].\n",
        "        \"\"\"\n",
        "        return img * self.std + self.mean\n",
        "\n",
        "    def tensor_to_uint8(self, tensor):\n",
        "        \"\"\"Converts CHW torch tensor to uint8 HWC image (PIL-like RGB).\"\"\"\n",
        "        img = tensor.permute(1, 2, 0).cpu().numpy()  # HWC float32\n",
        "        # Undo normalization (ImageNet)\n",
        "        img = self.unnormalize(img)  # still HWC float32 in [0,1]\n",
        "        # Clip and convert\n",
        "        img = np.clip(img * 255.0, 0, 255).astype(np.uint8)\n",
        "        # Ensure contiguous memory layout for OpenCV\n",
        "        img = np.ascontiguousarray(img)\n",
        "        return img\n",
        "\n",
        "    # BOX DRAWING\n",
        "\n",
        "    def overlay_rotated_box(self, output, box, wmult, hmult, color, label, score):\n",
        "        center_x, center_y, width, height, theta = box\n",
        "        angle_deg = math.degrees(theta)\n",
        "\n",
        "        category = self.class_names[label]\n",
        "        text = f\"{category}\" if score is None else f\"{category} - score={score:.3g}\"\n",
        "\n",
        "        # image rescaling\n",
        "        center_x *= wmult\n",
        "        center_y *= hmult\n",
        "        width *= wmult\n",
        "        height *= hmult\n",
        "\n",
        "        # draw box + label\n",
        "        box_points = cv2.boxPoints(((center_x, center_y), (width, height), angle_deg)).astype(np.int32)\n",
        "        cv2.drawContours(output, [box_points], 0, color, 2)\n",
        "        text_pos = tuple(box_points[1])\n",
        "        cv2.putText(output, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "    # VISUALIZATION\n",
        "\n",
        "    def visualize(self, index):\n",
        "        image, target = self.test_dataset[index]\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "            output = image.copy()\n",
        "        else:\n",
        "            output = self.tensor_to_uint8(image)\n",
        "\n",
        "        wmult, hmult = self.test_dataset.get_image_rescale(index)\n",
        "\n",
        "        # ground truth\n",
        "        true_boxes = target[\"boxes\"]\n",
        "        true_labels = target[\"labels\"]\n",
        "\n",
        "        # predictions\n",
        "        predicted_boxes = self.boxes[index] if index < len(self.boxes) else np.zeros((0, 5))\n",
        "        predicted_labels = self.labels[index] if index < len(self.labels) else np.zeros((0,), dtype=np.int64)\n",
        "        predicted_scores = self.scores[index] if index < len(self.scores) else np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "        # convert tensors to numpy\n",
        "        if isinstance(true_boxes, torch.Tensor):\n",
        "            true_boxes = true_boxes.cpu().numpy()\n",
        "        if isinstance(true_labels, torch.Tensor):\n",
        "            true_labels = true_labels.cpu().numpy()\n",
        "\n",
        "        # draw ground truths\n",
        "        if len(true_boxes) > 0 and true_boxes.shape[1] == 5:\n",
        "            for box, label in zip(true_boxes, true_labels):\n",
        "                self.overlay_rotated_box(output, box, wmult, hmult,\n",
        "                                         (0, 255, 0), int(label), None)\n",
        "\n",
        "        # draw predictions\n",
        "        if len(predicted_boxes) > 0 and predicted_boxes.shape[1] == 5:\n",
        "            for box, label, score in zip(predicted_boxes, predicted_labels, predicted_scores):\n",
        "                self.overlay_rotated_box(output, box, 1.0, 1.0,\n",
        "                                         (255, 0, 0), int(label), float(score))\n",
        "\n",
        "        # save files\n",
        "        output_path = self.results_folder / f\"{self.test_dataset.ids[index]}.png\"\n",
        "        cv2.imwrite(str(output_path), cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        print(f\"Saved visualization to {output_path}\")\n",
        "\n",
        "    # BATCH VISUALIZATION\n",
        "\n",
        "    def visualize_multiple(self, count=100, start_index=0, max_workers=4):\n",
        "        end_index = min(start_index + count, len(self.test_dataset)) if count > 0 else len(self.test_dataset)\n",
        "        indices = list(range(start_index, end_index))\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = {executor.submit(self.visualize, i): i for i in indices}\n",
        "            for future in as_completed(futures):\n",
        "                idx = futures[future]\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as e:\n",
        "                    print(f\"Visualization failed for index {idx}: {e}\")\n"
      ],
      "metadata": {
        "id": "OkCjVM0PHwrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESULTS_PARENT_FOLDER = Path(\"drive/MyDrive/Colab Notebooks/Results\")\n",
        "HRSC_VISUALIZER = Visualizer(HRSC_TEST_DATASET, HRSC_MODEL.class_names, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES, RESULTS_PARENT_FOLDER / \"HRSC\")\n",
        "DOTA_VISUALIZER = Visualizer(DOTA_TEST_DATASET, DOTA_MODEL.class_names, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES, RESULTS_PARENT_FOLDER / \"DOTA\")\n",
        "NWPU_VISUALIZER = Visualizer(NWPU_TEST_DATASET, NWPU_MODEL.class_names, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES, RESULTS_PARENT_FOLDER / \"NWPU\")\n",
        "\n",
        "HRSC_VISUALIZER.visualize_multiple()\n",
        "DOTA_VISUALIZER.visualize_multiple()\n",
        "NWPU_VISUALIZER.visualize_multiple()"
      ],
      "metadata": {
        "id": "D2DcyM6sH3Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Summaries"
      ],
      "metadata": {
        "id": "wBzStM8EMIEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== HRSC Benchmark Summary ===\\n\")\n",
        "print(f\"Batch Size: {HRSC_MODEL.batch_size}\")\n",
        "pprint.pprint(HRSC_MODEL.bench.summary())\n",
        "\n",
        "print(\"\\n=== DOTA Benchmark Summary ===\\n\")\n",
        "print(f\"Batch Size: {DOTA_MODEL.batch_size}\")\n",
        "pprint.pprint(DOTA_MODEL.bench.summary())\n",
        "\n",
        "print(\"\\n=== NWPU Benchmark Summary ===\\n\")\n",
        "print(f\"Batch Size: {NWPU_MODEL.batch_size}\")\n",
        "pprint.pprint(NWPU_MODEL.bench.summary())"
      ],
      "metadata": {
        "id": "PRGiMgcoMK8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}