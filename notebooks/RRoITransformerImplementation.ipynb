{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Setup"
      ],
      "metadata": {
        "id": "HzULUdkkNN30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages and import modules"
      ],
      "metadata": {
        "id": "yoXEGwWsMsdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q colorama\n",
        "!pip install -q opencv-python\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q shapely\n",
        "!pip install -q tqdm\n",
        "\n",
        "# --- Standard Library ---\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "import itertools\n",
        "import json\n",
        "import time\n",
        "import pprint\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# --- Third-Party Libraries ---\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from shapely.geometry import Polygon\n",
        "from colorama import Fore, Style\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- PyTorch / TorchVision / TorchMetrics / Detectron2 ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as F_transforms\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.models import ResNet101_Weights\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "from torchvision.models.detection.roi_heads import RoIHeads\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from detectron2.layers import roi_align_rotated"
      ],
      "metadata": {
        "id": "OkbHdJGpBF-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Torch device and connect to Google Drive"
      ],
      "metadata": {
        "id": "3bS-OGRpM27r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure GPU/CPU device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Mount Google Drive for dataset access\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iImd1rYBBJku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define dataset filepaths\n",
        "\n",
        "---\n",
        "\n",
        "Training, validation, and testing splits for HRSC, DOTA, and NWPU datasets."
      ],
      "metadata": {
        "id": "mcyW4tynNIO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SHARED_PATH = Path(\"drive/MyDrive/Colab Notebooks/Shared\")\n",
        "HRSC_PATH = SHARED_PATH / \"HRSC2016_Final_Splits\"\n",
        "DOTA_PATH = SHARED_PATH / \"DOTA_Final_Splits\"\n",
        "NWPU_PATH = SHARED_PATH / \"NWPU_VHR-10_Final_Splits\"\n",
        "\n",
        "# ========== HRSC ==========\n",
        "\n",
        "HRSC_TRAIN_IMAGES = HRSC_PATH / \"train/images\"\n",
        "HRSC_TRAIN_ANNOTATIONS = HRSC_PATH / \"train/annotations\"\n",
        "HRSC_VAL_IMAGES = HRSC_PATH / \"val/images\"\n",
        "HRSC_VAL_ANNOTATIONS = HRSC_PATH / \"val/annotations\"\n",
        "HRSC_TEST_IMAGES = HRSC_PATH / \"test/images\"\n",
        "HRSC_TEST_ANNOTATIONS = HRSC_PATH / \"test/annotations\"\n",
        "\n",
        "# ========== DOTA ==========\n",
        "\n",
        "DOTA_TRAIN_IMAGES = DOTA_PATH / \"train/images\"\n",
        "DOTA_TRAIN_ANNOTATIONS = DOTA_PATH / \"train/annotations\"\n",
        "DOTA_VAL_IMAGES = DOTA_PATH / \"val/images\"\n",
        "DOTA_VAL_ANNOTATIONS = DOTA_PATH / \"val/annotations\"\n",
        "DOTA_TEST_IMAGES = DOTA_PATH / \"test/images\"\n",
        "DOTA_TEST_ANNOTATIONS = DOTA_PATH / \"test/annotations\"\n",
        "\n",
        "# ========== NWPU ==========\n",
        "\n",
        "NWPU_TRAIN_IMAGES = NWPU_PATH / \"train/images\"\n",
        "NWPU_TRAIN_ANNOTATIONS = NWPU_PATH / \"train/annotations\"\n",
        "NWPU_VAL_IMAGES = NWPU_PATH / \"val/images\"\n",
        "NWPU_VAL_ANNOTATIONS = NWPU_PATH / \"val/annotations\"\n",
        "NWPU_TEST_IMAGES = NWPU_PATH / \"test/images\"\n",
        "NWPU_TEST_ANNOTATIONS = NWPU_PATH / \"test/annotations\""
      ],
      "metadata": {
        "id": "HhT6C7jBBVYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define label parsers\n",
        "\n",
        "---\n",
        "\n",
        "Parses annotation files from different datasets and extracts rotated bounding box coordinates."
      ],
      "metadata": {
        "id": "Unurw_HlNU4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== HRSC ==========\n",
        "\n",
        "HRSC_CLASSES = {}\n",
        "\n",
        "def parse_hrsc_labels(label_path: Path):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    tree = ET.parse(label_path.as_posix())\n",
        "    root = tree.getroot()\n",
        "    objects = root.findall(\".//HRSC_Object\")\n",
        "    for obj in objects:\n",
        "        try:\n",
        "            # Bounds\n",
        "            center_x = float(obj.find('mbox_cx').text)\n",
        "            center_y = float(obj.find('mbox_cy').text)\n",
        "            width = float(obj.find('mbox_w').text)\n",
        "            height = float(obj.find('mbox_h').text)\n",
        "            angle = float(obj.find('mbox_ang').text)\n",
        "\n",
        "            # Category\n",
        "            class_id = int(obj.find('Class_ID').text)\n",
        "            if class_id not in HRSC_CLASSES:\n",
        "                HRSC_CLASSES[class_id] = len(HRSC_CLASSES)\n",
        "            class_id_index = HRSC_CLASSES[class_id]\n",
        "\n",
        "            # Append\n",
        "            boxes.append([center_x, center_y, width, height, angle])\n",
        "            labels.append(class_id_index)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    return boxes, labels\n",
        "\n",
        "\n",
        "# ========== DOTA ==========\n",
        "\n",
        "DOTA_CLASSES = {}\n",
        "\n",
        "def parse_dota_labels(label_path: Path):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for line in itertools.islice(label_path.read_text().splitlines(), 2, None):\n",
        "        try:\n",
        "            obj = line.strip().split()\n",
        "\n",
        "            # Bounds\n",
        "            x1, y1, x2, y2, x3, y3, x4, y4, category, difficulty = (*map(float, obj[:8]), *obj[8:])\n",
        "            points = [(x1, y1), (x2, y2), (x3, y3), (x4, y4)]\n",
        "            pts_np = np.array(points, dtype=np.float32).reshape(-1, 1, 2)\n",
        "            (cx, cy), (w, h), angle_deg = cv2.minAreaRect(pts_np)\n",
        "            angle_rad = math.radians(angle_deg)\n",
        "\n",
        "            # Category\n",
        "            if category not in DOTA_CLASSES:\n",
        "                DOTA_CLASSES[category] = len(DOTA_CLASSES)\n",
        "            class_id_index = DOTA_CLASSES[category]\n",
        "\n",
        "            # Append\n",
        "            boxes.append([cx, cy, w, h, angle_rad])\n",
        "            labels.append(class_id_index)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    return boxes, labels\n",
        "\n",
        "\n",
        "# ========== NWPU ==========\n",
        "\n",
        "NWPU_CLASSES = {}\n",
        "\n",
        "def parse_nwpu_labels(label_path: Path):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    data = json.loads(label_path.read_text())\n",
        "    for category in data['categories']:\n",
        "        NWPU_CLASSES[category['name']] = category['id'] - 1  # categories are 1-indexed -> convert to 0-indexed\n",
        "\n",
        "    for annotation in data['annotations']:\n",
        "        try:\n",
        "            # Bounds\n",
        "            segmentation = annotation['segmentation'][0]\n",
        "            pts_np = np.array(segmentation, dtype=np.float32).reshape(-1, 2)\n",
        "            (cx, cy), (w, h), angle_deg = cv2.minAreaRect(pts_np)\n",
        "            angle_rad = math.radians(angle_deg)\n",
        "\n",
        "            # Category\n",
        "            class_id_index = annotation['category_id'] - 1  # categories are 1-indexed -> convert to 0-indexed\n",
        "\n",
        "            # Append\n",
        "            boxes.append([cx, cy, w, h, angle_rad])\n",
        "            labels.append(class_id_index)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return boxes, labels"
      ],
      "metadata": {
        "id": "Uy67NT4sBYiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define image rescalers\n",
        "\n",
        "---\n",
        "\n",
        "Computes image scaling factors to handle visualization of different image sizes across each dataset."
      ],
      "metadata": {
        "id": "CBt6ewQ9NqE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== HRSC ==========\n",
        "\n",
        "def hrsc_image_rescale(label_path: Path, image_width, image_height):\n",
        "    tree = ET.parse(label_path.as_posix())\n",
        "    root = tree.getroot()\n",
        "    width = int(root.find('.//Img_SizeWidth').text)\n",
        "    height = int(root.find('.//Img_SizeHeight').text)\n",
        "    return image_width / width, image_height / height\n",
        "\n",
        "# ========== DOTA ==========\n",
        "\n",
        "def dota_image_rescale(label_path: Path, image_width, image_height):\n",
        "    return 1.0, 1.0\n",
        "\n",
        "# ========== NWPU ==========\n",
        "\n",
        "def nwpu_image_rescale(label_path: Path, image_width, image_height):\n",
        "    data = json.loads(label_path.read_text())\n",
        "    image = data['images'][0]\n",
        "    width = image['width']\n",
        "    height = image['height']\n",
        "    return image_width / width, image_height / height"
      ],
      "metadata": {
        "id": "bGVLwYjaNuP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define helper mathematical functions\n",
        "\n",
        "---\n",
        "\n",
        "Implements core mathematical operations from the paper, including regression targets, coordinate transformations and IoU calculations."
      ],
      "metadata": {
        "id": "C9C9fvkJN39m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dbbox2delta(proposals, gt, means=[0, 0, 0, 0, 0], stds=[1, 1, 1, 1, 1]):\n",
        "    \"\"\"\n",
        "    Convert ground truth rotated bounding boxes to delta regression targets.\n",
        "\n",
        "    Implements Equation (1) from the RoI Transformer paper:\n",
        "    Transforms GT boxes into offset values relative to proposals for regression.\n",
        "\n",
        "    Args:\n",
        "        proposals (Tensor): Proposed rotated boxes [cx, cy, w, h, angle]\n",
        "        gt (Tensor): Ground truth rotated boxes [cx, cy, w, h, angle]\n",
        "        means (list): Normalization means for delta values\n",
        "        stds (list): Normalization stds for delta values\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Normalized delta values [dx, dy, dw, dh, dangle]\n",
        "    \"\"\"\n",
        "    # Add epsilon for numerical stability\n",
        "    eps = 1e-6\n",
        "\n",
        "    proposals = proposals.float()\n",
        "    gt = gt.float()\n",
        "    gt_widths = gt[..., 2].clamp(min=eps)\n",
        "    gt_heights = gt[..., 3].clamp(min=eps)\n",
        "    gt_angle = gt[..., 4]\n",
        "    proposals_widths = proposals[..., 2].clamp(min=eps)\n",
        "    proposals_heights = proposals[..., 3].clamp(min=eps)\n",
        "    proposals_angle = proposals[..., 4]\n",
        "\n",
        "    # Calculate coordinate offsets in proposal's rotated coordinate system (equation #1)\n",
        "    coord = gt[..., 0:2] - proposals[..., 0:2]\n",
        "    dx = (torch.cos(proposals[..., 4]) * coord[..., 0] + torch.sin(proposals[..., 4]) * coord[..., 1]) / proposals_widths\n",
        "    dy = (-torch.sin(proposals[..., 4]) * coord[..., 0] + torch.cos(proposals[..., 4]) * coord[..., 1]) / proposals_heights\n",
        "    dw = torch.log(gt_widths / proposals_widths.clamp(min=eps))\n",
        "    dh = torch.log(gt_heights / proposals_heights.clamp(min=eps))\n",
        "    dangle = (gt_angle - proposals_angle) % (2 * math.pi) / (2 * math.pi)  # Normalized angle difference (0-1 range for 0-2pi)\n",
        "\n",
        "    # Normalize deltas\n",
        "    deltas = torch.stack((dx, dy, dw, dh, dangle), -1)\n",
        "    means = deltas.new_tensor(means).unsqueeze(0)\n",
        "    stds = deltas.new_tensor(stds).unsqueeze(0)\n",
        "    deltas = deltas.sub_(means).div_(stds)\n",
        "    return deltas\n",
        "\n",
        "\n",
        "def delta2dbbox(Rrois, deltas, means=[0, 0, 0, 0, 0], stds=[1, 1, 1, 1, 1], max_shape=None, wh_ratio_clip=16/1000):\n",
        "    \"\"\"\n",
        "    Convert delta regression targets back to rotated bounding boxes.\n",
        "\n",
        "    Implements the inverse of Equation (1) from the paper:\n",
        "    Transforms regression deltas back to absolute rotated box coordinates.\n",
        "\n",
        "    Args:\n",
        "        Rrois (Tensor): Base rotated boxes [cx, cy, w, h, angle]\n",
        "        deltas (Tensor): Regression deltas [dx, dy, dw, dh, dangle]\n",
        "        means (list): Denormalization means\n",
        "        stds (list): Denormalization stds\n",
        "        max_shape: Maximum allowed shape for clipping\n",
        "        wh_ratio_clip: Width-height ratio clipping threshold\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Reconstructed rotated boxes [cx, cy, w, h, angle]\n",
        "    \"\"\"\n",
        "    # Add epsilon for numerical stability\n",
        "    eps = 1e-6\n",
        "\n",
        "    # Un-normalize deltas\n",
        "    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 5)\n",
        "    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 5)\n",
        "    denorm_deltas = deltas * stds + means\n",
        "\n",
        "    dx = denorm_deltas[:, 0::5]\n",
        "    dy = denorm_deltas[:, 1::5]\n",
        "    dw = denorm_deltas[:, 2::5]\n",
        "    dh = denorm_deltas[:, 3::5]\n",
        "    dangle = denorm_deltas[:, 4::5]\n",
        "\n",
        "    # Clip width-height ratios to prevent extreme values\n",
        "    max_ratio = np.abs(np.log(wh_ratio_clip))\n",
        "    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n",
        "    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n",
        "\n",
        "    # Expand Rroi dimensions for broadcasting\n",
        "    Rroi_x = (Rrois[:, 0]).unsqueeze(1).expand_as(dx)\n",
        "    Rroi_y = (Rrois[:, 1]).unsqueeze(1).expand_as(dy)\n",
        "    Rroi_w = (Rrois[:, 2]).unsqueeze(1).expand_as(dw).clamp(min=eps)\n",
        "    Rroi_h = (Rrois[:, 3]).unsqueeze(1).expand_as(dh).clamp(min=eps)\n",
        "    Rroi_angle = (Rrois[:, 4]).unsqueeze(1).expand_as(dangle)\n",
        "\n",
        "    # Reconstruct coordinates from original image space (inverse of equation #1)\n",
        "    gx = dx * Rroi_w * torch.cos(Rroi_angle) - dy * Rroi_h * torch.sin(Rroi_angle) + Rroi_x\n",
        "    gy = dx * Rroi_w * torch.sin(Rroi_angle) + dy * Rroi_h * torch.cos(Rroi_angle) + Rroi_y\n",
        "    gw = Rroi_w * torch.exp(dw.clamp(min=-10, max=10))  # Clamp exponentials\n",
        "    gh = Rroi_h * torch.exp(dh.clamp(min=-10, max=10))\n",
        "    gangle = (2 * np.pi) * dangle + Rroi_angle\n",
        "    gangle = gangle % (2 * np.pi)  # Normalize angle to [0, 2pi]\n",
        "\n",
        "    bboxes = torch.stack([gx, gy, gw, gh, gangle], dim=-1).view_as(deltas)\n",
        "    return bboxes\n",
        "\n",
        "\n",
        "def hbb2obb_v2(boxes):\n",
        "    \"\"\"\n",
        "    Convert horizontal bounding boxes to oriented bounding boxes.\n",
        "\n",
        "    Transforms standard [x1, y1, x2, y2] boxes to rotated [cx, cy, w, h, angle] format.\n",
        "    Initializes all boxes with -90° angle (vertical orientation).\n",
        "\n",
        "    Args:\n",
        "        boxes (Tensor): Horizontal boxes [x1, y1, x2, y2]\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Rotated boxes [cx, cy, w, h, angle]\n",
        "    \"\"\"\n",
        "    num_boxes = boxes.size(0)\n",
        "    ex_heights = boxes[..., 2] - boxes[..., 0] + 1.0\n",
        "    ex_widths = boxes[..., 3] - boxes[..., 1] + 1.0\n",
        "    ex_ctr_x = boxes[..., 0] + 0.5 * (ex_heights - 1.0)\n",
        "    ex_ctr_y = boxes[..., 1] + 0.5 * (ex_widths - 1.0)\n",
        "\n",
        "    # Convert to center-based representation\n",
        "    c_bboxes = torch.cat((ex_ctr_x.unsqueeze(1), ex_ctr_y.unsqueeze(1), ex_widths.unsqueeze(1), ex_heights.unsqueeze(1)), 1)\n",
        "\n",
        "    # Initialize all angles to -90° (vertical)\n",
        "    initial_angles = -c_bboxes.new_ones((num_boxes, 1)) * np.pi / 2\n",
        "    dbboxes = torch.cat((c_bboxes, initial_angles), 1)\n",
        "\n",
        "    return dbboxes\n",
        "\n",
        "\n",
        "def rotated_box_to_polygon(box):\n",
        "    \"\"\"\n",
        "    Convert rotated bounding box to polygon coordinates.\n",
        "\n",
        "    Implements Equation (4) from the paper:\n",
        "    Transforms [cx, cy, w, h, angle] representation to 4-corner polygon.\n",
        "\n",
        "    Args:\n",
        "        box: Rotated box as [cx, cy, w, h, angle] or horizontal box as [x1, y1, x2, y2]\n",
        "\n",
        "    Returns:\n",
        "        Polygon: Shapely Polygon object representing the rotated rectangle\n",
        "    \"\"\"\n",
        "    if isinstance(box, torch.Tensor):\n",
        "        box = box.cpu().numpy()\n",
        "\n",
        "    if len(box) == 4:\n",
        "        # Convert horizontal bbox to rotated format\n",
        "        x1, y1, x2, y2 = box\n",
        "        cx = (x1 + x2) / 2\n",
        "        cy = (y1 + y2) / 2\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "        angle = 0.0\n",
        "    elif len(box) == 5:\n",
        "        cx, cy, w, h, angle = box\n",
        "    else:\n",
        "        raise ValueError(f\"Box must have 4 or 5 elements, got {len(box)}\")\n",
        "\n",
        "    # Calculate corner points in local coordinate system\n",
        "    hw = w / 2\n",
        "    hh = h / 2\n",
        "    corners = np.array([[-hw, -hh], [hw, -hh], [hw, hh], [-hw, hh]], dtype=np.float32)\n",
        "\n",
        "    # Create rotation matrix\n",
        "    cosA = np.cos(angle)\n",
        "    sinA = np.sin(angle)\n",
        "    R = np.array([[cosA, -sinA], [sinA, cosA]])\n",
        "\n",
        "    # Rotate and translate corners to global coordinates (equation #4)\n",
        "    rotated = corners @ R.T\n",
        "    rotated[:, 0] += cx\n",
        "    rotated[:, 1] += cy\n",
        "\n",
        "    return Polygon(rotated.tolist())\n",
        "\n",
        "\n",
        "def box_iou_rotated(boxes1, boxes2): #This implements the polygon-based IoU calculation from Equation (5)\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) for rotated bounding boxes.\n",
        "\n",
        "    Implements polygon-based IoU calculation from Equation (5) in the paper.\n",
        "    Uses Shapely for polygon intersection operations.\n",
        "\n",
        "    Args:\n",
        "        boxes1 (Tensor): First set of rotated boxes\n",
        "        boxes2 (Tensor): Second set of rotated boxes\n",
        "\n",
        "    Returns:\n",
        "        Tensor: IoU matrix where [i, j] contains IoU between boxes1[i] and boxes2[j]\n",
        "    \"\"\"\n",
        "    N = boxes1.shape[0]\n",
        "    M = boxes2.shape[0]\n",
        "    ious = torch.zeros((N, M), dtype=torch.float32, device=boxes1.device)\n",
        "\n",
        "    for i in range(N):\n",
        "        try:\n",
        "            # Convert first box to polygon\n",
        "            p1 = rotated_box_to_polygon(boxes1[i])\n",
        "            area1 = p1.area\n",
        "            if area1 <= 0:\n",
        "                continue\n",
        "\n",
        "            for j in range(M):\n",
        "                try:\n",
        "                    # Convert second box to polygon\n",
        "                    p2 = rotated_box_to_polygon(boxes2[j])\n",
        "\n",
        "                    # Calculate intersection and union\n",
        "                    inter = p1.intersection(p2).area\n",
        "                    union = area1 + p2.area - inter\n",
        "\n",
        "                    if union > 0:\n",
        "                        ious[i, j] = inter / union  # (equation #5)\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return ious"
      ],
      "metadata": {
        "id": "Rl5hZlyaN794"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model creation"
      ],
      "metadata": {
        "id": "mxK6kz7KN_8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define dataset wrapper structure\n",
        "\n",
        "---\n",
        "\n",
        "Wraps aerial image datasets with preprocessing, resizing, and image/annotation loading capabilities."
      ],
      "metadata": {
        "id": "0h60C86xODQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASIC_TRANSFORM = T.Compose([T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for aerial imagery with rotated bounding box annotations.\n",
        "\n",
        "    Handles multiple dataset formats (HRSC, DOTA, NWPU) with unified interface.\n",
        "    Supports image resizing, normalization, and data augmentation transformations.\n",
        "\n",
        "    Args:\n",
        "        images_folder (Path): Directory containing image files\n",
        "        annotations_folder (Path): Directory containing annotation files\n",
        "        label_parser (callable): Function to parse annotation files into boxes/labels\n",
        "        image_rescale (callable): Function to compute image scaling factors for visualization\n",
        "        transforms (callable): Image transformations and normalization\n",
        "        max_files (int): Maximum number of files to load (for debugging)\n",
        "        target_size (int): Target size for image resizing (maintains aspect ratio)\n",
        "    \"\"\"\n",
        "    def __init__(self, images_folder: Path, annotations_folder: Path, label_parser, image_rescale, transforms=BASIC_TRANSFORM, max_files=200, target_size=800):\n",
        "        super().__init__()\n",
        "        self.label_parser = label_parser\n",
        "        self.image_rescale = image_rescale\n",
        "        self.transforms = transforms\n",
        "        self.target_size = target_size\n",
        "\n",
        "        # Sort images and annotations in increasing file number\n",
        "        def sorting_key(p: Path):\n",
        "            m = re.search(r'(\\d+)', p.stem)\n",
        "            if m:\n",
        "                return int(m.group())\n",
        "            else:\n",
        "                return float('inf')\n",
        "\n",
        "        if images_folder.exists():\n",
        "            images = sorted([f for f in images_folder.iterdir() if f.suffix.lower() in (\".jpg\", \".png\", \".bmp\")], key=sorting_key)\n",
        "        else:\n",
        "            images = []\n",
        "\n",
        "        if annotations_folder.exists():\n",
        "            annotations = sorted([f for f in annotations_folder.iterdir() if f.suffix.lower() in (\".xml\", \".txt\", \".json\")], key=sorting_key)\n",
        "        else:\n",
        "            annotations = []\n",
        "\n",
        "        if max_files > 0:\n",
        "            images = images[:max_files]\n",
        "            annotations = annotations[:max_files]\n",
        "\n",
        "        # Keep only images/annotations one-to-one correspondences\n",
        "        image_set = set(f.stem for f in images)\n",
        "        annotation_set = set(f.stem for f in annotations)\n",
        "        self.ids = image_set.intersection(annotation_set)\n",
        "        self.images = {f.stem: f for f in images if f.stem in self.ids}\n",
        "        self.annotations = {f.stem: f for f in annotations if f.stem in self.ids}\n",
        "        self.ids = list(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Get a single image and its annotations.\n",
        "\n",
        "        Args:\n",
        "            index (int): Dataset index\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - image (Tensor): Preprocessed image tensor\n",
        "                - target (dict): Annotation dictionary with:\n",
        "                    - 'boxes' (Tensor): Rotated bounding boxes [N, 5] as [cx, cy, w, h, angle]\n",
        "                    - 'labels' (Tensor): Class labels [N] (1-indexed, 0=background)\n",
        "        \"\"\"\n",
        "        id = self.ids[index]\n",
        "        image_path = self.images[id]\n",
        "        label_path = self.annotations[id]\n",
        "\n",
        "        # Load image in RGB format\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Parse bounding boxes and labels from annotation file\n",
        "        boxes, labels = self.label_parser(label_path)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 5), dtype=torch.float32)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            if boxes.dim() == 1:\n",
        "                boxes = boxes.unsqueeze(0)\n",
        "\n",
        "        if len(labels) == 0:\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Offset by 1 for background label\n",
        "        labels = labels + 1\n",
        "\n",
        "        image, boxes = self.preprocess(image, boxes)\n",
        "\n",
        "        # Convert to tensor (normalized 0-1)\n",
        "        image = F_transforms.to_tensor(image)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "        image = self.transforms(image)\n",
        "        image.filepath = image_path\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def preprocess(self, img, boxes):\n",
        "        \"\"\"\n",
        "        Preprocess image and annotations for model input.\n",
        "\n",
        "        Steps:\n",
        "        1. Resize image while maintaining aspect ratio\n",
        "        2. Scale bounding box coordinates accordingly\n",
        "        3. Pad image to target_size x target_size\n",
        "\n",
        "        Args:\n",
        "            img (PIL.Image): Input image\n",
        "            boxes (Tensor): Rotated bounding boxes\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - img (PIL.Image): Preprocessed image\n",
        "                - boxes (Tensor): Scaled bounding boxes\n",
        "        \"\"\"\n",
        "        old_w, old_h = img.width, img.height\n",
        "        scale = self.target_size / max(old_h, old_w)  # uniform scale\n",
        "        new_w = int(old_w * scale)\n",
        "        new_h = int(old_h * scale)\n",
        "\n",
        "        # Resize image\n",
        "        img = F_transforms.resize(img, (new_h, new_w))\n",
        "\n",
        "        # Scale boxes\n",
        "        boxes = boxes.clone()\n",
        "        if boxes.numel() > 0 and boxes.dim() == 2:\n",
        "            boxes[:, 0] *= scale  # cx\n",
        "            boxes[:, 1] *= scale  # cy\n",
        "            boxes[:, 2] *= scale  # w\n",
        "            boxes[:, 3] *= scale  # h\n",
        "            # Note: Angle stays unchanged\n",
        "\n",
        "        # Pad to target_size x target_size (right and bottom)\n",
        "        pad_w = self.target_size - new_w\n",
        "        pad_h = self.target_size - new_h\n",
        "        img = F_transforms.pad(img, (0, 0, pad_w, pad_h))\n",
        "        return img, boxes\n",
        "\n",
        "    def get_image_rescale(self, index):\n",
        "        \"\"\"\n",
        "        Compute scaling factors for converting between original and processed image coordinates.\n",
        "\n",
        "        Useful for visualization to map predictions back to original image space.\n",
        "\n",
        "        Args:\n",
        "            index (int): Dataset index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (width_scale_factor, height_scale_factor)\n",
        "        \"\"\"\n",
        "        id = self.ids[index]\n",
        "        image_path = self.images[id]\n",
        "        label_path = self.annotations[id]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return self.image_rescale(label_path, image.width, image.height)\n",
        "\n",
        "    def compute_total_number_of_objects(self, max_workers=8):\n",
        "        \"\"\"\n",
        "        Compute total number of annotated objects in the dataset.\n",
        "\n",
        "        Uses parallel processing for efficient counting across large datasets.\n",
        "\n",
        "        Args:\n",
        "            max_workers (int): Number of parallel workers for counting\n",
        "\n",
        "        Returns:\n",
        "            int: Total number of objects in the dataset\n",
        "        \"\"\"\n",
        "        def count_objects(id):\n",
        "            boxes, _ = self.label_parser(self.annotations[id])\n",
        "            return len(boxes)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            results = executor.map(count_objects, self.ids)\n",
        "        return sum(results)"
      ],
      "metadata": {
        "id": "66O14a-DOHtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load datasets"
      ],
      "metadata": {
        "id": "Xg7m3ZL5OKVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== HRSC ==========\n",
        "\n",
        "print(\"Preparing HRSC training dataset...\")\n",
        "HRSC_TRAIN_DATASET = TorchDataset(HRSC_TRAIN_IMAGES, HRSC_TRAIN_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing HRSC validation dataset...\")\n",
        "HRSC_VAL_DATASET = TorchDataset(HRSC_VAL_IMAGES, HRSC_VAL_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing HRSC testing dataset...\")\n",
        "HRSC_TEST_DATASET = TorchDataset(HRSC_TEST_IMAGES, HRSC_TEST_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "# ========== DOTA ==========\n",
        "\n",
        "print(\"Preparing DOTA training dataset...\")\n",
        "DOTA_TRAIN_DATASET = TorchDataset(DOTA_TRAIN_IMAGES, DOTA_TRAIN_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing DOTA validation dataset...\")\n",
        "DOTA_VAL_DATASET = TorchDataset(DOTA_VAL_IMAGES, DOTA_VAL_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing DOTA testing dataset...\")\n",
        "DOTA_TEST_DATASET = TorchDataset(DOTA_TEST_IMAGES, DOTA_TEST_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "# ========== NWPU ==========\n",
        "\n",
        "print(\"Preparing NWPU training dataset...\")\n",
        "NWPU_TRAIN_DATASET = TorchDataset(NWPU_TRAIN_IMAGES, NWPU_TRAIN_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing NWPU validation dataset...\")\n",
        "NWPU_VAL_DATASET = TorchDataset(NWPU_VAL_IMAGES, NWPU_VAL_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "\n",
        "print(\"Preparing NWPU testing dataset...\")\n",
        "NWPU_TEST_DATASET = TorchDataset(NWPU_TEST_IMAGES, NWPU_TEST_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TEST_DATASET.compute_total_number_of_objects()} total objects\")"
      ],
      "metadata": {
        "id": "QNin3WwoONUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define benchmark tracking class\n",
        "\n",
        "---\n",
        "\n",
        "Tracks training and inference timing for performance analysis."
      ],
      "metadata": {
        "id": "HD4uwcQ1OXCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BenchmarkTracker:\n",
        "    def __init__(self):\n",
        "        self.data = defaultdict(list)\n",
        "\n",
        "    def start(self, key):\n",
        "        self.data[key].append({\"start\": time.time(), \"end\": None})\n",
        "\n",
        "    def stop(self, key):\n",
        "        self.data[key][-1][\"end\"] = time.time()\n",
        "\n",
        "    def summary(self):\n",
        "        report = {}\n",
        "        for key, records in self.data.items():\n",
        "            durations = [(r[\"end\"] - r[\"start\"]) for r in records if r[\"end\"]]\n",
        "            report[key] = {\n",
        "                \"total\": sum(durations),\n",
        "                \"avg\": np.mean(durations),\n",
        "                \"std\": np.std(durations),\n",
        "                \"count\": len(durations)\n",
        "            }\n",
        "        return report"
      ],
      "metadata": {
        "id": "Vr6ePLB4Oaho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement RRoI modules\n",
        "\n",
        "---\n",
        "\n",
        "Implements the RoI Transformer architecture with rotated region proposal handling."
      ],
      "metadata": {
        "id": "EMvOB_0rSttG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotatedBoxPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Prediction head for rotated bounding box regression and classification.\n",
        "\n",
        "    Takes ROI-pooled features and produces:\n",
        "    - Classification scores for each class\n",
        "    - Regression deltas for rotated box refinement (dx, dy, dw, dh, dtheta)\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input feature channels\n",
        "        num_classes (int): Number of object classes (including background)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "        self.bbox_pred = nn.Linear(in_channels, num_classes * 5)  # 5 parameters (dx, dy, dw, dh, dtheta) per class\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for rotated box prediction.\n",
        "\n",
        "        Processes RoI-pooled features to generate:\n",
        "        - Classification logits for each class\n",
        "        - Bounding box regression deltas for each class\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input features from RoI pooling, shape [N, C, H, W] or [N, C*H*W]\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - scores (Tensor): Classification logits, shape [N, num_classes]\n",
        "                - bbox_deltas (Tensor): Bounding box regression deltas, shape [N, num_classes * 5]\n",
        "        \"\"\"\n",
        "        # Flatten spatial dimensions if needed\n",
        "        if x.dim() == 4:\n",
        "            x = x.flatten(start_dim=1)\n",
        "\n",
        "        # Generate classification scores and bounding box deltas\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "\n",
        "        return scores, bbox_deltas\n",
        "\n",
        "\n",
        "class RotatedRoIHeads(RoIHeads):\n",
        "    \"\"\"\n",
        "    RoI head module specialized for rotated bounding box detection.\n",
        "\n",
        "    Extends standard RoIHeads with:\n",
        "    - Rotated ROI alignment using detectron2's roi_align_rotated\n",
        "    - Rotated box regression and classification\n",
        "    - Rotated NMS for post-processing\n",
        "\n",
        "    Args:\n",
        "        box_roi_pool: ROI pooling module\n",
        "        box_head: Feature extraction head\n",
        "        box_predictor: Rotated box prediction head\n",
        "        fg_iou_thresh: Foreground IoU threshold\n",
        "        bg_iou_thresh: Background IoU threshold\n",
        "        batch_size_per_image: RoIs per image for training\n",
        "        positive_fraction: Positive RoI fraction\n",
        "        bbox_reg_weights: BBox regression weights\n",
        "        score_thresh: Detection score threshold\n",
        "        nms_thresh: NMS IoU threshold\n",
        "        detections_per_img: Max detections per image\n",
        "    \"\"\"\n",
        "    def __init__(self, box_roi_pool, box_head, box_predictor, fg_iou_thresh=0.5, bg_iou_thresh=0.5, batch_size_per_image=512, positive_fraction=0.25, bbox_reg_weights=None, score_thresh=0.05, nms_thresh=0.5, detections_per_img=100):\n",
        "        super().__init__(box_roi_pool, box_head, box_predictor, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction, bbox_reg_weights, score_thresh, nms_thresh, detections_per_img)\n",
        "        self.box_predictor = box_predictor\n",
        "        self.fg_iou_thresh = fg_iou_thresh\n",
        "        self.bg_iou_thresh = bg_iou_thresh\n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.detections_per_img = detections_per_img\n",
        "\n",
        "    def forward(self, features, proposals, image_shapes, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Rotated RoI Heads.\n",
        "\n",
        "        Handles both training and inference modes:\n",
        "        - Training: Computes losses and returns training targets\n",
        "        - Inference: Performs post-processing and returns final detections\n",
        "\n",
        "        Args:\n",
        "            features (dict/Tensor): Feature maps from backbone\n",
        "            proposals (list[Tensor]): Proposed regions from RPN\n",
        "            image_shapes (list[tuple]): Original image shapes (H, W)\n",
        "            targets (list[dict], optional): Ground truth targets for training\n",
        "\n",
        "        Returns:\n",
        "            Training: dict with losses and training targets\n",
        "            Inference: tuple of (boxes, scores, labels) for each image\n",
        "        \"\"\"\n",
        "        # Training sample selection\n",
        "        if self.training and targets is not None:\n",
        "            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n",
        "        else:\n",
        "            labels = None\n",
        "            regression_targets = None\n",
        "            matched_idxs = None\n",
        "\n",
        "        # Convert proposals to rotated format for RoI alignment\n",
        "        proposals = [p.to(DEVICE) for p in proposals]\n",
        "        rotated_proposals_list = []\n",
        "\n",
        "        for i, props in enumerate(proposals):\n",
        "            props = props.to(DEVICE)\n",
        "            x1, y1, x2, y2 = props.unbind(1)\n",
        "            cx = (x1 + x2) / 2\n",
        "            cy = (y1 + y2) / 2\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            angle = torch.zeros_like(cx)  # Initialize with 0 angle\n",
        "            batch_idx = torch.full_like(cx, i, dtype=torch.float32, device=DEVICE)\n",
        "            rotated = torch.stack([batch_idx, cx, cy, w, h, angle], dim=1)\n",
        "            rotated_proposals_list.append(rotated)\n",
        "\n",
        "        rotated_boxes = torch.cat(rotated_proposals_list, dim=0)\n",
        "\n",
        "        # Extract feature map and perform rotated RoI alignment\n",
        "        if isinstance(features, dict):\n",
        "            feature_map = list(features.values())[0]\n",
        "            spatial_scale = 1.0 / 4.0 # Scale for FPN features\n",
        "        else:\n",
        "            feature_map = features\n",
        "            spatial_scale = 1.0 / 4.0\n",
        "\n",
        "        # Rotated RoI alignment - key differentiator from standard detection\n",
        "        rotated_features = roi_align_rotated(feature_map, rotated_boxes, (7, 7), spatial_scale, 2)  # (7, 7) output size, sampling_ratio=2\n",
        "        rotated_features = self.box_head(rotated_features)\n",
        "        class_logits, box_regression = self.box_predictor(rotated_features)\n",
        "\n",
        "        if self.training:\n",
        "            return {'class_logits': class_logits, 'box_regression': box_regression, 'proposals': proposals, 'matched_idxs': matched_idxs, 'labels': labels, 'regression_targets': regression_targets}\n",
        "        else:\n",
        "            return self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
        "\n",
        "    def postprocess_detections(self, class_logits, box_regression, proposals, image_shapes):\n",
        "        \"\"\"\n",
        "        Post-process raw predictions into final detections.\n",
        "\n",
        "        Performs:\n",
        "        - Softmax scoring for classification\n",
        "        - Decoding of regression deltas to absolute box coordinates\n",
        "        - Score-based filtering\n",
        "        - Rotated Non-Maximum Suppression (NMS)\n",
        "        - Results trimming to maximum detections per image\n",
        "\n",
        "        Args:\n",
        "            class_logits (Tensor): Raw classification logits [N, num_classes]\n",
        "            box_regression (Tensor): Raw regression deltas [N, num_classes * 5]\n",
        "            proposals (list[Tensor]): Original region proposals\n",
        "            image_shapes (list[tuple]): Original image shapes (H, W)\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - all_boxes (list[Tensor]): Final detected boxes for each image\n",
        "                - all_scores (list[Tensor]): Confidence scores for each detection\n",
        "                - all_labels (list[Tensor]): Class labels for each detection\n",
        "        \"\"\"\n",
        "        device = DEVICE\n",
        "        num_classes = class_logits.shape[-1]\n",
        "        boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in proposals]\n",
        "\n",
        "        # Decode regression deltas to absolute rotated box coordinates\n",
        "        pred_boxes = self.decode_boxes(box_regression, proposals)\n",
        "\n",
        "        # Apply softmax to get classification probabilities\n",
        "        pred_scores = F.softmax(class_logits, -1)\n",
        "\n",
        "        # Split batch predictions into per-image lists\n",
        "        pred_boxes_list = pred_boxes.split(boxes_per_image, 0)\n",
        "        pred_scores_list = pred_scores.split(boxes_per_image, 0)\n",
        "\n",
        "        all_boxes = []\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Process each image independently\n",
        "        for boxes, scores, image_shape in zip(pred_boxes_list, pred_scores_list, image_shapes):\n",
        "            boxes = boxes.to(device)\n",
        "            scores = scores.to(device)\n",
        "\n",
        "            # Clip boxes to image boundaries\n",
        "            boxes = self.clip_boxes_to_image(boxes, image_shape)\n",
        "\n",
        "            # Create label tensor matching score dimensions\n",
        "            labels = torch.arange(num_classes, device=device)\n",
        "            labels = labels.view(1, -1).expand_as(scores)\n",
        "\n",
        "            # Flatten all predictions for filtering\n",
        "            boxes = boxes.reshape(-1, 5)\n",
        "            scores = scores.reshape(-1)\n",
        "            labels = labels.reshape(-1)\n",
        "\n",
        "            # Remove background predictions (class 0)\n",
        "            inds = labels > 0\n",
        "            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
        "\n",
        "            # Apply score threshold\n",
        "            inds = scores > self.score_thresh\n",
        "            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
        "\n",
        "            # Apply rotated NMS to remove duplicates\n",
        "            keep = self.rotated_nms(boxes, scores, self.nms_thresh)\n",
        "            keep = keep[:self.detections_per_img] # Keep top-k detections\n",
        "            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "\n",
        "            all_boxes.append(boxes)\n",
        "            all_scores.append(scores)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "        return all_boxes, all_scores, all_labels\n",
        "\n",
        "    def decode_boxes(self, box_regression, proposals):\n",
        "        \"\"\"\n",
        "        Decode regression deltas back to absolute rotated box coordinates.\n",
        "\n",
        "        Applies the inverse transformation of dbbox2delta to convert\n",
        "        network outputs [dx, dy, dw, dh, dtheta] to [cx, cy, w, h, theta].\n",
        "\n",
        "        Args:\n",
        "            box_regression (Tensor): Regression deltas [N, num_classes * 5]\n",
        "            proposals (list[Tensor]): Original region proposals in horizontal format\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Decoded rotated boxes [N, num_classes, 5]\n",
        "        \"\"\"\n",
        "        # Convert horizontal proposals to rotated format\n",
        "        rotated_proposals = []\n",
        "        for props in proposals:\n",
        "            props = props.to(DEVICE)\n",
        "            rotated_props = hbb2obb_v2(props)  # Convert to [cx, cy, w, h, angle]\n",
        "            rotated_proposals.append(rotated_props)\n",
        "\n",
        "        rotated_proposals = torch.cat(rotated_proposals, dim=0)\n",
        "\n",
        "        # Reshape regression deltas to [N, num_classes, 5]\n",
        "        num_classes = box_regression.shape[1] // 5\n",
        "        box_regression = box_regression.view(-1, num_classes, 5)\n",
        "\n",
        "        # Decode boxes for each class independently\n",
        "        decoded_boxes = []\n",
        "        for cls_idx in range(num_classes):\n",
        "            deltas = box_regression[:, cls_idx, :]\n",
        "            # Apply inverse transformation to get absolute coordinates\n",
        "            decoded = delta2dbbox(rotated_proposals, deltas)\n",
        "            decoded_boxes.append(decoded)\n",
        "\n",
        "        # Stack along class dimension: [N, num_classes, 5]\n",
        "        decoded_boxes = torch.stack(decoded_boxes, dim=1)\n",
        "        return decoded_boxes\n",
        "\n",
        "    def clip_boxes_to_image(self, boxes, image_shape):\n",
        "        \"\"\"\n",
        "        Clip rotated bounding boxes to image boundaries.\n",
        "\n",
        "        Ensures boxes don't extend beyond image edges while preserving\n",
        "        their rotational characteristics.\n",
        "\n",
        "        Args:\n",
        "            boxes (Tensor): Rotated boxes [N, 5] as [cx, cy, w, h, angle]\n",
        "            image_shape (tuple): Image dimensions (height, width)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Clipped boxes [N, 5]\n",
        "        \"\"\"\n",
        "        h, w = image_shape\n",
        "        boxes = boxes.clone().to(DEVICE)\n",
        "        w_tensor = torch.tensor(w, dtype=boxes.dtype, device=DEVICE)\n",
        "        h_tensor = torch.tensor(h, dtype=boxes.dtype, device=DEVICE)\n",
        "\n",
        "        # Clip center coordinates to image boundaries\n",
        "        boxes[:, 0] = boxes[:, 0].clamp(0, w_tensor)  # cx\n",
        "        boxes[:, 1] = boxes[:, 1].clamp(0, h_tensor)  # cy\n",
        "\n",
        "        # Ensure box dimensions are positive\n",
        "        boxes[:, 2] = boxes[:, 2].clamp(min=1)  # width\n",
        "        boxes[:, 3] = boxes[:, 3].clamp(min=1)  # height\n",
        "\n",
        "        # Note: Angles are not clipped as they're periodic\n",
        "        return boxes\n",
        "\n",
        "    def rotated_nms(self, boxes, scores, iou_threshold):\n",
        "        \"\"\"\n",
        "        Rotated Non-Maximum Suppression for oriented bounding boxes.\n",
        "\n",
        "        Greedily selects high-score detections and suppresses overlapping\n",
        "        detections using rotated IoU computation. Implements the standard\n",
        "        NMS algorithm but with rotated IoU instead of axis-aligned IoU.\n",
        "\n",
        "        Args:\n",
        "            boxes (Tensor): Rotated boxes [N, 5] as [cx, cy, w, h, angle]\n",
        "            scores (Tensor): Detection confidence scores [N]\n",
        "            iou_threshold (float): IoU threshold for suppression\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Indices of kept detections\n",
        "        \"\"\"\n",
        "        if len(boxes) == 0:\n",
        "            return torch.empty((0,), dtype=torch.long, device=boxes.device)\n",
        "\n",
        "        keep = []\n",
        "        # Sort by descending score\n",
        "        order = scores.argsort(descending=True)\n",
        "\n",
        "        while len(order) > 0:\n",
        "            # Select highest scoring box\n",
        "            i = order[0]\n",
        "            keep.append(i)\n",
        "\n",
        "            if len(order) == 1:\n",
        "                break\n",
        "\n",
        "            # Compute rotated IoU with remaining boxes\n",
        "            ious = box_iou_rotated(boxes[i:i+1], boxes[order[1:]])\n",
        "            ious = ious.squeeze(0)\n",
        "\n",
        "            # Keep boxes with IoU below threshold\n",
        "            inds = (ious <= iou_threshold).nonzero(as_tuple=True)[0]\n",
        "            order = order[inds + 1]\n",
        "\n",
        "        return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "    def select_training_samples(self, proposals, targets):\n",
        "        \"\"\"\n",
        "        Select training samples and compute regression targets for rotated boxes.\n",
        "        \"\"\"\n",
        "        labels = []\n",
        "        matched_idxs = []\n",
        "        regression_targets = []\n",
        "\n",
        "        for props, target in zip(proposals, targets):\n",
        "            props = props.to(DEVICE)\n",
        "            target_boxes = target['boxes'].to(DEVICE)\n",
        "            target_labels = target['labels'].to(DEVICE)\n",
        "\n",
        "            # Handle empty proposals case\n",
        "            if props.shape[0] == 0:\n",
        "                print(\"  WARNING: No proposals for this image\")\n",
        "                labels.append(torch.zeros(0, dtype=torch.long, device=DEVICE))\n",
        "                matched_idxs.append(torch.zeros(0, dtype=torch.long, device=DEVICE))\n",
        "                regression_targets.append(torch.zeros(0, 5, device=DEVICE))\n",
        "                continue\n",
        "\n",
        "            rotated_props = hbb2obb_v2(props)  # Convert to rotated format\n",
        "\n",
        "            # Calculate IoU between proposals and targets\n",
        "            ious = box_iou_rotated(rotated_props, target_boxes)\n",
        "            max_ious, matched_idx = ious.max(dim=1)\n",
        "\n",
        "            # Assign labels based on IoU thresholds\n",
        "            label = torch.zeros(len(props), dtype=torch.long, device=DEVICE)\n",
        "            pos_mask = max_ious > self.fg_iou_thresh\n",
        "\n",
        "            if pos_mask.any():\n",
        "                label[pos_mask] = target_labels[matched_idx[pos_mask]]\n",
        "\n",
        "            labels.append(label)\n",
        "            matched_idxs.append(matched_idx)\n",
        "\n",
        "            # Compute regression targets for positive samples\n",
        "            if pos_mask.any():\n",
        "                matched_gt = target_boxes[matched_idx[pos_mask]]\n",
        "                deltas = dbbox2delta(rotated_props[pos_mask], matched_gt)\n",
        "                reg_target = torch.zeros(len(props), 5, device=DEVICE)\n",
        "                reg_target[pos_mask] = deltas\n",
        "            else:\n",
        "                reg_target = torch.zeros(len(props), 5, device=DEVICE)\n",
        "\n",
        "            regression_targets.append(reg_target)\n",
        "\n",
        "        return proposals, matched_idxs, labels, regression_targets"
      ],
      "metadata": {
        "id": "R2O_jO1bOgD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model class\n",
        "\n",
        "---\n",
        "\n",
        "Integrates all components into a complete oriented object detection system."
      ],
      "metadata": {
        "id": "bt4fDGC1OjCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotatedFasterRCNNModel(FasterRCNN):\n",
        "    \"\"\"\n",
        "    Complete Rotated Faster R-CNN model for oriented object detection.\n",
        "\n",
        "    Integrates:\n",
        "    - ResNet-50 FPN backbone for feature extraction\n",
        "    - RPN for horizontal proposal generation\n",
        "    - Rotated RoI heads for oriented object detection\n",
        "    - End-to-end training and inference pipeline\n",
        "\n",
        "    Args:\n",
        "        model_path (Path): Path to save/load model weights\n",
        "        train_dataset: Training dataset\n",
        "        val_dataset: Validation dataset\n",
        "        test_dataset: Test dataset\n",
        "        class_names: List of class names\n",
        "        batch_size: Training batch size\n",
        "        shuffle_datasets: Whether to shuffle datasets\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path: Path, train_dataset: TorchDataset, val_dataset: TorchDataset, test_dataset: TorchDataset, class_names: list, batch_size=2, shuffle_datasets=False):\n",
        "        self.model_path = model_path\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.class_names = [\"__background__\"] + list(class_names)\n",
        "        self.num_classes = len(self.class_names)\n",
        "\n",
        "        # Create data loaders\n",
        "        collate_fn = lambda x: tuple(zip(*x))\n",
        "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "\n",
        "        if len(self.test_dataset) > 0:\n",
        "            self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "        else:\n",
        "            self.test_loader = None\n",
        "\n",
        "        # Initialize Faster R-CNN with ResNet-101 FPN backbone (from paper)\n",
        "        backbone = resnet_fpn_backbone(backbone_name='resnet101', weights=ResNet101_Weights.DEFAULT, trainable_layers=5)\n",
        "        super().__init__(backbone=backbone, num_classes=self.num_classes, min_size=800, max_size=1333)\n",
        "\n",
        "        # Replace standard box predictor with rotated version\n",
        "        self.in_features = self.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.box_detector = RotatedBoxPredictor(self.in_features, self.num_classes)\n",
        "\n",
        "        # Configure FPN parameters for RPN and RoI heads\n",
        "        self._configure_fpn_parameters()\n",
        "\n",
        "        self.roi_heads = RotatedRoIHeads(self.roi_heads.box_roi_pool, self.roi_heads.box_head, self.box_detector)\n",
        "\n",
        "        self.to(DEVICE)\n",
        "        self.bench = BenchmarkTracker()  # Performance benchmarking\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def _configure_fpn_parameters(self):\n",
        "        \"\"\"Configure RPN with FPN-specific parameters from paper (Section 4.2).\"\"\"\n",
        "        self.rpn.pre_nms_top_n = lambda: 6000;  # RoIs from RPN before NMS\n",
        "        self.rpn.post_nms_top_n = lambda: 600;  # RoIs after NMS\n",
        "        self.rpn.batch_size_per_image = 512     # RPN batch size\n",
        "\n",
        "\n",
        "    def train_model(self, num_epochs=7):\n",
        "        \"\"\"\n",
        "        Execute complete training pipeline with validation.\n",
        "\n",
        "        Training loop features:\n",
        "        - SGD optimizer with momentum and weight decay\n",
        "        - Step learning rate scheduling\n",
        "        - Per-epoch training and validation loss tracking\n",
        "        - Automatic checkpointing every 5 epochs\n",
        "        - GPU memory management and benchmarking\n",
        "\n",
        "        Args:\n",
        "            num_epochs (int): Number of training epochs\n",
        "            learning_rate (float): Initial learning rate\n",
        "\n",
        "        Side effects:\n",
        "            - Updates model parameters in-place\n",
        "            - Saves model checkpoints periodically\n",
        "            - Prints progress and loss information\n",
        "            - Updates benchmark tracker with timing data\n",
        "        \"\"\"\n",
        "        # Initialize optimizer with SGD (commonly used for detection tasks)\n",
        "        # Use learning rate and scheduler parameters from section 4.2 of paper\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
        "\n",
        "            # --- TRAINING PHASE ---\n",
        "            print(\"\\tStarting training loop...\")\n",
        "            self.bench.start(\"train_epoch\")\n",
        "            train_loss = 0.0\n",
        "            self.train()  # set model to training mode\n",
        "\n",
        "            for images, targets in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                self.bench.start(\"train_batch\")\n",
        "\n",
        "                # Forward pass - compute losses\n",
        "                loss_dict = self(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                # Backward pass - compute gradients\n",
        "                optimizer.zero_grad()\n",
        "                losses.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Accumulate loss for epoch statistics\n",
        "                train_loss += losses.item()\n",
        "                self.bench.stop(\"train_batch\")\n",
        "\n",
        "            # Update learning rate for next epoch\n",
        "            lr_scheduler.step()\n",
        "            self.bench.stop(\"train_epoch\")\n",
        "            print(\"\\t...Training loop complete.\")\n",
        "\n",
        "            # --- VALIDATION PHASE ---\n",
        "            print(\"\\tStarting validation loop...\")\n",
        "            self.bench.start(\"val_epoch\")\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "                self.train()  # Keep in train mode for consistency with batch norm\n",
        "                for images, targets in self.val_loader:\n",
        "                    self.bench.start(\"val_batch\")\n",
        "\n",
        "                    # Forward pass - compute losses\n",
        "                    loss_dict = self(images, targets)\n",
        "                    losses = sum(loss for loss in loss_dict.values())\n",
        "                    val_loss += losses.item()\n",
        "\n",
        "                    self.bench.stop(\"val_batch\")\n",
        "\n",
        "            self.bench.stop(\"val_epoch\")\n",
        "            print(\"\\t...Validation loop complete.\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs for recovery and model selection\n",
        "            print(f\"...Finished epoch {epoch+1}/{num_epochs}, Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                self.save_weights()\n",
        "\n",
        "        # Save final model weights after all epoches complete\n",
        "        self.save_weights()\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training and inference.\n",
        "\n",
        "        During training: Returns loss dictionary\n",
        "        During inference: Returns detection predictions\n",
        "        \"\"\"\n",
        "        self.training = targets is not None and self.training\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        image_sizes = [img.shape[-2:] for img in images]\n",
        "        images = ImageList(torch.stack(images), image_sizes)\n",
        "\n",
        "        # Backbone feature extraction\n",
        "        features = self.backbone(images.tensors)\n",
        "\n",
        "        # Region proposal networks (RPN) for horizontal proposal generation\n",
        "        rpn_targets = []\n",
        "        if targets is not None:\n",
        "            # Convert rotated targets to horizontal for RPN\n",
        "            for t in targets:\n",
        "                boxes = t['boxes'].to(DEVICE)\n",
        "                labels = t['labels'].to(DEVICE)\n",
        "                if boxes.numel() > 0 and boxes.dim() == 2 and boxes.shape[1] == 5:\n",
        "                    cx, cy, w, h, theta = boxes.unbind(1)\n",
        "                    x1 = cx - w/2\n",
        "                    y1 = cy - h/2\n",
        "                    x2 = cx + w/2\n",
        "                    y2 = cy + h/2\n",
        "                    rpn_targets.append({'boxes': torch.stack([x1, y1, x2, y2], dim=1), 'labels': labels})\n",
        "                else:\n",
        "                    rpn_targets.append({'boxes': torch.zeros((0, 4), dtype=torch.float32, device=DEVICE), 'labels': labels})\n",
        "\n",
        "        proposals, rpn_losses = self.rpn(images, features, rpn_targets if targets else None)\n",
        "\n",
        "        # Rotated RoI heads for oriented detection\n",
        "        if targets is not None:\n",
        "            targets = [{'boxes': t['boxes'].to(DEVICE), 'labels': t['labels'].to(DEVICE)} for t in targets]\n",
        "        roi_outputs = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
        "\n",
        "        # Loss computation (training only)\n",
        "        if self.training and targets is not None:\n",
        "            loss_dict = {}\n",
        "            loss_dict.update(rpn_losses)\n",
        "\n",
        "            # Classification loss\n",
        "            class_logits = roi_outputs['class_logits']\n",
        "            labels = torch.cat(roi_outputs['labels'], dim=0)\n",
        "\n",
        "            # Stabilize logits to prevent NaN in cross-entropy\n",
        "            if class_logits.numel() > 0:\n",
        "                # Clip extreme logits that cause overflow in softmax\n",
        "                class_logits = torch.clamp(class_logits, -50, 50)\n",
        "\n",
        "            loss_dict['loss_classifier'] = F.cross_entropy(class_logits, labels)\n",
        "\n",
        "            # Regression loss (only for positive samples)\n",
        "            box_regression = roi_outputs['box_regression']\n",
        "            regression_targets = torch.cat(roi_outputs['regression_targets'], dim=0)\n",
        "            pos_mask = labels > 0\n",
        "\n",
        "            if pos_mask.any():\n",
        "                num_classes = box_regression.shape[1] // 5\n",
        "                box_regression = box_regression.view(-1, num_classes, 5)\n",
        "                reg_for_labels = box_regression[pos_mask, labels[pos_mask] - 1, :]\n",
        "                loss_dict['loss_box_reg'] = F.smooth_l1_loss(reg_for_labels, regression_targets[pos_mask], reduction='mean')\n",
        "            else:\n",
        "                loss_dict['loss_box_reg'] = torch.tensor(0.0, device=class_logits.device)\n",
        "\n",
        "            return loss_dict\n",
        "        else:\n",
        "            # Return detections during inference\n",
        "            all_boxes, all_scores, all_labels = roi_outputs\n",
        "            return [{'boxes': boxes, 'scores': scores, 'labels': labels} for boxes, scores, labels in zip(all_boxes, all_scores, all_labels)]\n",
        "\n",
        "    def save_weights(self):\n",
        "        \"\"\"\n",
        "        Save model weights and metadata to disk.\n",
        "\n",
        "        Persists:\n",
        "        - Model state dictionary (learned parameters)\n",
        "        - Class names mapping for inference\n",
        "        - Model configuration metadata\n",
        "\n",
        "        File format: PyTorch checkpoint (.pth)\n",
        "        Usage: Model persistence, transfer learning, deployment\n",
        "\n",
        "        Side effects:\n",
        "        - Creates/overwrites model file at self.model_path\n",
        "        - Maintains training progress for resumption\n",
        "        \"\"\"\n",
        "        torch.save({\"model_state_dict\": self.state_dict(), \"class_names\": self.class_names}, self.model_path)\n",
        "\n",
        "    def load_weights(self):\n",
        "        \"\"\"\n",
        "        Load model weights and metadata from disk.\n",
        "\n",
        "        Restores:\n",
        "        - All model parameters (backbone, RPN, RoI heads)\n",
        "        - Class names mapping for consistent inference\n",
        "        - Model state for continued training or evaluation\n",
        "\n",
        "        Requirements:\n",
        "        - Model architecture must match saved checkpoint\n",
        "        - File must exist at self.model_path\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If model_path doesn't exist\n",
        "            RuntimeError: If architecture mismatch occurs\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(self.model_path, map_location=DEVICE)\n",
        "        self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        self.class_names = checkpoint[\"class_names\"]\n",
        "\n",
        "    def test_results(self):\n",
        "        \"\"\"\n",
        "        Run model inference on test dataset and collect predictions.\n",
        "\n",
        "        Performs:\n",
        "        - Batch-wise inference with gradient computation disabled\n",
        "        - Score-based filtering using roi_heads.score_thresh\n",
        "        - Device-agnostic tensor handling (CPU/GPU)\n",
        "        - Performance benchmarking for timing analysis\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - all_boxes (list[np.array]): Predicted boxes for each image [M, 5]\n",
        "                - all_labels (list[np.array]): Predicted class labels for each image [M]\n",
        "                - all_scores (list[np.array]): Prediction confidence scores [M]\n",
        "\n",
        "        Note:\n",
        "            Empty predictions are returned as zero-length arrays to maintain\n",
        "            consistent indexing with the dataset\n",
        "        \"\"\"\n",
        "        if self.test_loader is None:\n",
        "            print(\"Test dataset is empty. Skipping evaluation.\")\n",
        "            return [], [], []\n",
        "\n",
        "        self.load_weights()  # Ensure latest weights are loaded\n",
        "        self.bench.start(\"test_total\")\n",
        "\n",
        "        all_boxes, all_labels, all_scores = [], [], []\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "            self.eval()  # Set model to evaluation mode\n",
        "\n",
        "            for images, _ in self.test_loader:\n",
        "                self.bench.start(\"test_batch\")\n",
        "                images = [img.to(DEVICE) for img in images]\n",
        "\n",
        "                # Run forward pass (inference mode)\n",
        "                predictions = self(images)\n",
        "\n",
        "                # Process each image's prediction\n",
        "                for prediction in predictions:\n",
        "                    boxes = prediction['boxes'].detach().cpu().numpy()\n",
        "                    labels = prediction['labels'].detach().cpu().numpy()\n",
        "                    scores = prediction['scores'].detach().cpu().numpy()\n",
        "\n",
        "                    # Handle empty predictions gracefully\n",
        "                    if len(boxes) == 0:\n",
        "                        boxes = np.zeros((0, 5), dtype=np.float32)\n",
        "                    if len(labels) == 0:\n",
        "                        labels = np.zeros((0,), dtype=np.int64)\n",
        "                    if len(scores) == 0:\n",
        "                        scores = np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "                    all_boxes.append(boxes)\n",
        "                    all_labels.append(labels)\n",
        "                    all_scores.append(scores)\n",
        "\n",
        "                self.bench.stop(\"test_batch\")\n",
        "\n",
        "        self.bench.stop(\"test_total\")\n",
        "        return all_boxes, all_labels, all_scores"
      ],
      "metadata": {
        "id": "iN7ZZGf2Olnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate models\n",
        "\n",
        "---\n",
        "\n",
        "Creates model instances for each dataset with appropriate configurations."
      ],
      "metadata": {
        "id": "ZIJaXT2YOq9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_PATH = Path(\"drive/MyDrive/Colab Notebooks/Models\")\n",
        "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "HRSC_MODEL = RotatedFasterRCNNModel(MODEL_SAVE_PATH / \"hrsc_faster_rcnn_model.pth\", HRSC_TRAIN_DATASET, HRSC_VAL_DATASET, HRSC_TEST_DATASET, [id for id, _ in sorted(HRSC_CLASSES.items(), key=lambda item: item[1])])\n",
        "DOTA_MODEL = RotatedFasterRCNNModel(MODEL_SAVE_PATH / \"dota_faster_rcnn_model.pth\", DOTA_TRAIN_DATASET, DOTA_VAL_DATASET, DOTA_TEST_DATASET, [id for id, _ in sorted(DOTA_CLASSES.items(), key=lambda item: item[1])])\n",
        "NWPU_MODEL = RotatedFasterRCNNModel(MODEL_SAVE_PATH / \"nwpu_faster_rcnn_model.pth\", NWPU_TRAIN_DATASET, NWPU_VAL_DATASET, NWPU_TEST_DATASET, [id for id, _ in sorted(NWPU_CLASSES.items(), key=lambda item: item[1])])"
      ],
      "metadata": {
        "id": "mpcmKFvCOs2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "URCVwiSqOuf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run training loops\n",
        "\n",
        "---\n",
        "\n",
        "Executes training procedures for each dataset model."
      ],
      "metadata": {
        "id": "TJLgDJahOwmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== HRSC ==========\n",
        "\n",
        "if HRSC_MODEL.model_path.exists():\n",
        "    print(\"Loading HRSC weights...\")\n",
        "    HRSC_MODEL.load_weights()\n",
        "    print(\"...HRSC weights loaded.\")\n",
        "else:\n",
        "    print(\"Training HRSC model...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    HRSC_MODEL.train_model()\n",
        "    print(\"...HRSC model trained.\")"
      ],
      "metadata": {
        "id": "K3Z3otM8OyJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== DOTA ==========\n",
        "\n",
        "if DOTA_MODEL.model_path.exists():\n",
        "    print(\"Loading DOTA weights...\")\n",
        "    DOTA_MODEL.load_weights()\n",
        "    print(\"...DOTA weights loaded.\")\n",
        "else:\n",
        "    print(\"Training DOTA model...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    DOTA_MODEL.train_model()\n",
        "    print(\"...DOTA model trained.\")"
      ],
      "metadata": {
        "id": "cZFEAu_dPDUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== NWPU ==========\n",
        "\n",
        "if NWPU_MODEL.model_path.exists():\n",
        "    print(\"Loading NWPU weights...\")\n",
        "    NWPU_MODEL.load_weights()\n",
        "    print(\"...NWPU weights loaded.\")\n",
        "else:\n",
        "    print(\"Training NWPU model...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    NWPU_MODEL.train_model()\n",
        "    print(\"...NWPU model trained.\")"
      ],
      "metadata": {
        "id": "vJsj-ZbAPDXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run testing loops\n",
        "\n",
        "---\n",
        "\n",
        "Runs inference and collects detection results for evaluation."
      ],
      "metadata": {
        "id": "Nc3DcRw9O-7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== HRSC ==========\n",
        "\n",
        "HRSC_MODEL.roi_heads.score_thresh = 1.0e-6\n",
        "HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES = HRSC_MODEL.test_results()"
      ],
      "metadata": {
        "id": "rhhcBIA1PImR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== DOTA ==========\n",
        "\n",
        "DOTA_MODEL.roi_heads.score_thresh = 0.012\n",
        "DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES = DOTA_MODEL.test_results()"
      ],
      "metadata": {
        "id": "0M0idiL3PIoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== NWPU ==========\n",
        "\n",
        "NWPU_MODEL.roi_heads.score_thresh = 0.001\n",
        "NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES = NWPU_MODEL.test_results()"
      ],
      "metadata": {
        "id": "z2kU2QYzHn4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-evaluation analysis"
      ],
      "metadata": {
        "id": "b401UhRxPQR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define visualization class\n",
        "\n",
        "---\n",
        "\n",
        "Generates visualizations showing ground truth vs predicted rotated bounding boxes."
      ],
      "metadata": {
        "id": "fA4xpKZtPUTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Visualizer:\n",
        "    def __init__(self, test_dataset: TorchDataset, class_names: list,\n",
        "                 boxes: list, labels: list, scores: list, results_folder: Path,\n",
        "                 normalize_mean=(0.485, 0.456, 0.406),\n",
        "                 normalize_std=(0.229, 0.224, 0.225)):\n",
        "\n",
        "        self.test_dataset = test_dataset\n",
        "        self.class_names = class_names\n",
        "        self.boxes = boxes\n",
        "        self.labels = labels\n",
        "        self.scores = scores\n",
        "        self.results_folder = results_folder\n",
        "        self.results_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # normalization undo params\n",
        "        self.mean = np.array(normalize_mean)\n",
        "        self.std = np.array(normalize_std)\n",
        "\n",
        "    # HELPERS\n",
        "\n",
        "    def unnormalize(self, img):\n",
        "        \"\"\"\n",
        "        img: float32 numpy array in [C,H,W] or [H,W,C] with ImageNet normalization.\n",
        "        Returns image in [0,1].\n",
        "        \"\"\"\n",
        "        return img * self.std + self.mean\n",
        "\n",
        "    def tensor_to_uint8(self, tensor):\n",
        "        \"\"\"Converts CHW torch tensor to uint8 HWC image (PIL-like RGB).\"\"\"\n",
        "        img = tensor.permute(1, 2, 0).cpu().numpy()  # HWC float32\n",
        "        # Undo normalization (ImageNet)\n",
        "        img = self.unnormalize(img)  # still HWC float32 in [0,1]\n",
        "        # Clip and convert\n",
        "        img = np.clip(img * 255.0, 0, 255).astype(np.uint8)\n",
        "        # Ensure contiguous memory layout for OpenCV\n",
        "        img = np.ascontiguousarray(img)\n",
        "        return img\n",
        "\n",
        "    # BOX DRAWING\n",
        "\n",
        "    def overlay_rotated_box(self, output, box, wmult, hmult, color, label, score):\n",
        "        center_x, center_y, width, height, theta = box\n",
        "        angle_deg = math.degrees(theta)\n",
        "\n",
        "        category = self.class_names[label]\n",
        "        text = f\"{category}\" if score is None else f\"{category} - score={score:.3g}\"\n",
        "\n",
        "        # image rescaling\n",
        "        center_x *= wmult\n",
        "        center_y *= hmult\n",
        "        width *= wmult\n",
        "        height *= hmult\n",
        "\n",
        "        # draw box + label\n",
        "        box_points = cv2.boxPoints(((center_x, center_y), (width, height), angle_deg)).astype(np.int32)\n",
        "        cv2.drawContours(output, [box_points], 0, color, 2)\n",
        "        text_pos = tuple(box_points[1])\n",
        "        cv2.putText(output, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "    # VISUALIZATION\n",
        "\n",
        "    def visualize(self, index):\n",
        "        image, target = self.test_dataset[index]\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "            output = image.copy()\n",
        "        else:\n",
        "            output = self.tensor_to_uint8(image)\n",
        "\n",
        "        wmult, hmult = self.test_dataset.get_image_rescale(index)\n",
        "\n",
        "        # ground truth\n",
        "        true_boxes = target[\"boxes\"]\n",
        "        true_labels = target[\"labels\"]\n",
        "\n",
        "        # predictions\n",
        "        predicted_boxes = self.boxes[index] if index < len(self.boxes) else np.zeros((0, 5))\n",
        "        predicted_labels = self.labels[index] if index < len(self.labels) else np.zeros((0,), dtype=np.int64)\n",
        "        predicted_scores = self.scores[index] if index < len(self.scores) else np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "        # convert tensors to numpy\n",
        "        if isinstance(true_boxes, torch.Tensor):\n",
        "            true_boxes = true_boxes.cpu().numpy()\n",
        "        if isinstance(true_labels, torch.Tensor):\n",
        "            true_labels = true_labels.cpu().numpy()\n",
        "\n",
        "        # draw ground truths\n",
        "        if len(true_boxes) > 0 and true_boxes.shape[1] == 5:\n",
        "            for box, label in zip(true_boxes, true_labels):\n",
        "                self.overlay_rotated_box(output, box, wmult, hmult,\n",
        "                                         (0, 255, 0), int(label), None)\n",
        "\n",
        "        # draw predictions\n",
        "        if len(predicted_boxes) > 0 and predicted_boxes.shape[1] == 5:\n",
        "            for box, label, score in zip(predicted_boxes, predicted_labels, predicted_scores):\n",
        "                self.overlay_rotated_box(output, box, 1.0, 1.0,\n",
        "                                         (255, 0, 0), int(label), float(score))\n",
        "\n",
        "        # save files\n",
        "        output_path = self.results_folder / f\"{self.test_dataset.ids[index]}.png\"\n",
        "        cv2.imwrite(str(output_path), cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        print(f\"Saved visualization to {output_path}\")\n",
        "\n",
        "    # BATCH VISUALIZATION\n",
        "\n",
        "    def visualize_multiple(self, count=100, start_index=0, max_workers=4):\n",
        "        end_index = min(start_index + count, len(self.test_dataset)) if count > 0 else len(self.test_dataset)\n",
        "        indices = list(range(start_index, end_index))\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = {executor.submit(self.visualize, i): i for i in indices}\n",
        "            for future in as_completed(futures):\n",
        "                idx = futures[future]\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as e:\n",
        "                    print(f\"Visualization failed for index {idx}: {e}\")"
      ],
      "metadata": {
        "id": "OkCjVM0PHwrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize datasets"
      ],
      "metadata": {
        "id": "7trIFJ9yPX1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESULTS_PARENT_FOLDER = Path(\"drive/MyDrive/Colab Notebooks/Results\")\n",
        "HRSC_VISUALIZER = Visualizer(HRSC_TEST_DATASET, HRSC_MODEL.class_names, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES, RESULTS_PARENT_FOLDER / \"HRSC\")\n",
        "DOTA_VISUALIZER = Visualizer(DOTA_TEST_DATASET, DOTA_MODEL.class_names, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES, RESULTS_PARENT_FOLDER / \"DOTA\")\n",
        "NWPU_VISUALIZER = Visualizer(NWPU_TEST_DATASET, NWPU_MODEL.class_names, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES, RESULTS_PARENT_FOLDER / \"NWPU\")\n",
        "\n",
        "HRSC_VISUALIZER.visualize_multiple()\n",
        "DOTA_VISUALIZER.visualize_multiple()\n",
        "NWPU_VISUALIZER.visualize_multiple()"
      ],
      "metadata": {
        "id": "D2DcyM6sH3Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print benchmark summaries"
      ],
      "metadata": {
        "id": "wBzStM8EMIEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== HRSC Benchmark Summary ===\\n\")\n",
        "print(f\"Batch Size: {HRSC_MODEL.batch_size}\")\n",
        "pprint.pprint(HRSC_MODEL.bench.summary())\n",
        "\n",
        "print(\"\\n=== DOTA Benchmark Summary ===\\n\")\n",
        "print(f\"Batch Size: {DOTA_MODEL.batch_size}\")\n",
        "pprint.pprint(DOTA_MODEL.bench.summary())\n",
        "\n",
        "print(\"\\n=== NWPU Benchmark Summary ===\\n\")\n",
        "print(f\"Batch Size: {NWPU_MODEL.batch_size}\")\n",
        "pprint.pprint(NWPU_MODEL.bench.summary())"
      ],
      "metadata": {
        "id": "PRGiMgcoMK8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics Computation\n",
        "\n",
        "---\n",
        "\n",
        "Computes evaluation metrics like mAP for model performance analysis."
      ],
      "metadata": {
        "id": "3_u7Cl3JM_b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Statistician:\n",
        "    def __init__(self, test_dataset: TorchDataset, predicted_boxes: list, predicted_labels: list, predicted_scores: list):\n",
        "        self.metric = MeanAveragePrecision()\n",
        "        self.targets = [target for image, target in test_dataset]\n",
        "        self.predictions = [\n",
        "            {\n",
        "                \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
        "                \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
        "                \"scores\": torch.as_tensor(scores, dtype=torch.float32)\n",
        "            }\n",
        "            for boxes, labels, scores in zip(predicted_boxes, predicted_labels, predicted_scores)\n",
        "        ]\n",
        "        self.metric.update(self.predictions, self.targets)\n",
        "        self.result = self.metric.compute()\n",
        "\n",
        "    def get_map(self):\n",
        "        return self.result[\"map\"]\n",
        "\n",
        "    def get_map_percent(self):\n",
        "        return self.get_map().detach().cpu().numpy() * 100\n",
        "\n",
        "    def get_map_50(self):\n",
        "        return self.result[\"map_50\"]\n",
        "\n",
        "    def get_map_75(self):\n",
        "        return self.result[\"map_75\"]\n",
        "\n",
        "    def get_map_small(self):\n",
        "        return self.result[\"map_small\"]\n",
        "\n",
        "    def get_map_medium(self):\n",
        "        return self.result[\"map_medium\"]\n",
        "\n",
        "    def get_map_large(self):\n",
        "        return self.result[\"map_large\"]\n",
        "\n",
        "    def get_mar_1(self):\n",
        "        return self.result[\"mar_1\"]\n",
        "\n",
        "    def get_mar_10(self):\n",
        "        return self.result[\"mar_10\"]\n",
        "\n",
        "    def get_mar_100(self):\n",
        "        return self.result[\"mar_100\"]\n",
        "\n",
        "    def get_mar_small(self):\n",
        "        return self.result[\"mar_small\"]\n",
        "\n",
        "    def get_mar_medium(self):\n",
        "        return self.result[\"mar_medium\"]\n",
        "\n",
        "    def get_mar_large(self):\n",
        "        return self.result[\"mar_large\"]\n",
        "\n",
        "    def get_map_per_class(self):\n",
        "        return self.result[\"map_per_class\"]\n",
        "\n",
        "    def get_mar_100_per_class(self):\n",
        "        return self.result[\"mar_100_per_class\"]\n",
        "\n",
        "    def get_classes(self):\n",
        "        return self.result[\"classes\"]\n"
      ],
      "metadata": {
        "id": "vuACCxD_NBCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Statistics"
      ],
      "metadata": {
        "id": "u6YPcMy9ND3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_PRED_STATS = Statistician(HRSC_TEST_DATASET, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES)\n",
        "DOTA_PRED_STATS = Statistician(DOTA_TEST_DATASET, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES)\n",
        "NWPU_PRED_STATS = Statistician(NWPU_TEST_DATASET, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES)\n",
        "\n",
        "print(\"HRSC prediction statistics:\")\n",
        "print(f\"{HRSC_PRED_STATS.get_map_percent()}%\")\n",
        "print(\"DOTA prediction statistics:\")\n",
        "print(f\"{DOTA_PRED_STATS.get_map_percent()}%\")\n",
        "print(\"NWPU prediction statistics:\")\n",
        "print(f\"{NWPU_PRED_STATS.get_map_percent()}%\")\n"
      ],
      "metadata": {
        "id": "st_-1tyNNGO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}