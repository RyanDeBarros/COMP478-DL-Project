{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import PyTorch modules"
      ],
      "metadata": {
        "id": "RfBeheCNos9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EaRjQJEDokeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad774b8-27d0-4784-e576-8b766de98964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "import os\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "EjYFfoM3sHvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SHARED_PATH = Path(\"drive/MyDrive/Colab Notebooks/Shared\")\n",
        "HRSC_PATH = SHARED_PATH / \"HRSC2016_Final_Splits\"\n",
        "DOTA_PATH = SHARED_PATH / \"DOTA_Final_Splits\"\n",
        "\n",
        "print(\"HRSC subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in HRSC_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "print(\"DOTA subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in DOTA_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "\n",
        "HRSC_TRAIN_IMAGES = HRSC_PATH/ \"train/images\"\n",
        "HRSC_TRAIN_ANNOTATIONS = HRSC_PATH / \"train/annotations\"\n",
        "HRSC_VAL_IMAGES = HRSC_PATH / \"val/images\"\n",
        "HRSC_VAL_ANNOTATIONS = HRSC_PATH / \"val/annotations\"\n",
        "HRSC_TEST_IMAGES = HRSC_PATH / \"test/images\"\n",
        "HRSC_TEST_ANNOTATIONS = HRSC_PATH / \"test/annotations\"\n",
        "\n",
        "DOTA_TRAIN_IMAGES = DOTA_PATH / \"train/images\"\n",
        "DOTA_TRAIN_ANNOTATIONS = DOTA_PATH / \"train/hbb\"\n",
        "DOTA_VAL_IMAGES = DOTA_PATH / \"val/images\"\n",
        "DOTA_VAL_ANNOTATIONS = DOTA_PATH / \"val/hbb\"\n",
        "DOTA_TEST_IMAGES = DOTA_PATH / \"test/images\"\n",
        "DOTA_TEST_ANNOTATIONS = DOTA_PATH / \"test/hbb\""
      ],
      "metadata": {
        "id": "PT9GIwS8sX4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707e9f92-0269-4f42-ffde-cfcb64461698"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "HRSC subfolders: ['train/images', 'train/annotations', 'val/images', 'val/annotations', 'test/images', 'test/annotations']\n",
            "DOTA subfolders: ['train/images', 'train/hbb', 'val/images', 'val/hbb', 'test/images_without_hbb', 'test/images', 'test/hbb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore datasets"
      ],
      "metadata": {
        "id": "yp-lr_Prs6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore, Style\n",
        "\n",
        "def explore_header(dataset: str, subfolder: str):\n",
        "  print(\"Exploring\", Fore.GREEN + dataset + Style.RESET_ALL, Fore.MAGENTA + subfolder + Style.RESET_ALL, \"folder...\")\n",
        "\n",
        "def explore(images_folder: Path, ext: str):\n",
        "  files = list(images_folder.glob(f\"*.{ext}\"))\n",
        "  print(f\"Number of {ext.upper()} files:\", len(files))\n",
        "  print(f\"{ext.upper()} sample files:\", [path.name for path in files[:3]])\n",
        "\n",
        "explore_header(\"HRSC\", \"train\")\n",
        "explore(HRSC_TRAIN_IMAGES, \"bmp\")\n",
        "explore(HRSC_TRAIN_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"HRSC\", \"val\")\n",
        "explore(HRSC_VAL_IMAGES, \"bmp\")\n",
        "explore(HRSC_VAL_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"HRSC\", \"test\")\n",
        "explore(HRSC_TEST_IMAGES, \"bmp\")\n",
        "explore(HRSC_TEST_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"train\")\n",
        "explore(DOTA_TRAIN_IMAGES, \"png\")\n",
        "explore(DOTA_TRAIN_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"val\")\n",
        "explore(DOTA_VAL_IMAGES, \"png\")\n",
        "explore(DOTA_VAL_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"test\")\n",
        "explore(DOTA_TEST_IMAGES, \"png\")\n",
        "explore(DOTA_TEST_ANNOTATIONS, \"txt\")"
      ],
      "metadata": {
        "id": "jf-wMzlNs8Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef1a287-00e4-432f-a5a8-65e0fb81fb0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n",
            "Exploring \u001b[32mHRSC\u001b[0m \u001b[35mtrain\u001b[0m folder...\n",
            "Number of BMP files: 436\n",
            "BMP sample files: ['100000001.bmp', '100000002.bmp', '100000004.bmp']\n",
            "Number of XML files: 436\n",
            "XML sample files: ['100000001.xml', '100000002.xml', '100000004.xml']\n",
            "\n",
            "Exploring \u001b[32mHRSC\u001b[0m \u001b[35mval\u001b[0m folder...\n",
            "Number of BMP files: 181\n",
            "BMP sample files: ['100000006.bmp', '100000010.bmp', '100000622.bmp']\n",
            "Number of XML files: 181\n",
            "XML sample files: ['100000006.xml', '100000010.xml', '100000622.xml']\n",
            "\n",
            "Exploring \u001b[32mHRSC\u001b[0m \u001b[35mtest\u001b[0m folder...\n",
            "Number of BMP files: 453\n",
            "BMP sample files: ['100000003.bmp', '100000005.bmp', '100000623.bmp']\n",
            "Number of XML files: 453\n",
            "XML sample files: ['100000003.xml', '100000005.xml', '100000623.xml']\n",
            "\n",
            "Exploring \u001b[32mDOTA\u001b[0m \u001b[35mtrain\u001b[0m folder...\n",
            "Number of PNG files: 941\n",
            "PNG sample files: ['P0000.png', 'P0001.png', 'P0020.png']\n",
            "Number of TXT files: 941\n",
            "TXT sample files: ['P0038.txt', 'P0039.txt', 'P0041.txt']\n",
            "\n",
            "Exploring \u001b[32mDOTA\u001b[0m \u001b[35mval\u001b[0m folder...\n",
            "Number of PNG files: 458\n",
            "PNG sample files: ['P0003.png', 'P0004.png', 'P0007.png']\n",
            "Number of TXT files: 458\n",
            "TXT sample files: ['P0003.txt', 'P0004.txt', 'P0007.txt']\n",
            "\n",
            "Exploring \u001b[32mDOTA\u001b[0m \u001b[35mtest\u001b[0m folder...\n",
            "Number of PNG files: 470\n",
            "PNG sample files: ['P0025.png', 'P0042.png', 'P0044.png']\n",
            "Number of TXT files: 470\n",
            "TXT sample files: ['P0042.txt', 'P0044.txt', 'P0049.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define label parsers"
      ],
      "metadata": {
        "id": "ELZ-eU81tvbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "HRSC_CLASSES = {}\n",
        "DOTA_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_hrsc_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "  tree = ET.parse(label_path.as_posix())\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # Get image dimensions\n",
        "  width = int(root.find('.//Img_SizeWidth').text)\n",
        "  height = int(root.find('.//Img_SizeHeight').text)\n",
        "\n",
        "  objects = root.findall(\".//HRSC_Object\")\n",
        "  for obj in objects:\n",
        "    try:\n",
        "      # Bounds\n",
        "      xmin = float(obj.find('box_xmin').text)\n",
        "      ymin = float(obj.find('box_ymin').text)\n",
        "      xmax = float(obj.find('box_xmax').text)\n",
        "      ymax = float(obj.find('box_ymax').text)\n",
        "\n",
        "      # Category\n",
        "      class_id = int(obj.find('Class_ID').text)\n",
        "      if class_id not in HRSC_CLASSES:\n",
        "        HRSC_CLASSES[class_id] = len(HRSC_CLASSES)\n",
        "      class_id_index = HRSC_CLASSES[class_id]\n",
        "\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "\n",
        "def parse_dota_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "\n",
        "  # Find all objects\n",
        "  for line in itertools.islice(label_path.read_text().splitlines(), 2, None):  # Start from line index 2\n",
        "    try:\n",
        "      obj = line.strip().split()\n",
        "      x1, y1, x2, y2, x3, y3, x4, y4, category, difficulty = (*map(float, obj[:8]), *obj[8:])\n",
        "\n",
        "      # Bounds\n",
        "      xmin = min(x1, x2, x3, x4)\n",
        "      ymin = min(y1, y2, y3, y4)\n",
        "      xmax = max(x1, x2, x3, x4)\n",
        "      ymax = max(y1, y2, y3, y4)\n",
        "\n",
        "      # Category\n",
        "      if category not in DOTA_CLASSES:\n",
        "        DOTA_CLASSES[category] = len(DOTA_CLASSES)\n",
        "      class_id_index = DOTA_CLASSES[category]\n",
        "\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels"
      ],
      "metadata": {
        "id": "c1iomq0mtvAA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch dataset structure"
      ],
      "metadata": {
        "id": "MVdX-5_GpPse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "BASIC_TRANSFORM = T.Compose([T.ToTensor()])\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_folder: Path, annotations_folder: Path, label_parser, transforms=BASIC_TRANSFORM) -> None:\n",
        "    super().__init__()\n",
        "    self.label_parser = label_parser\n",
        "    self.transforms = transforms\n",
        "    images = [f for f in images_folder.iterdir() if f.suffix in (\".jpg\", \".png\", \".bmp\")] if images_folder.exists() else []\n",
        "    annotations = [f for f in annotations_folder.iterdir() if f.suffix in (\".xml\", \".txt\")] if annotations_folder.exists() else []\n",
        "\n",
        "    # Keep only images/annotations one-to-one correspondences\n",
        "    image_set = set(f.stem for f in images)\n",
        "    annotation_set = set(f.stem for f in annotations)\n",
        "    self.ids = image_set.intersection(annotation_set)\n",
        "    self.images = {f.stem: f for f in images if f.stem in self.ids}\n",
        "    self.annotations = {f.stem: f for f in annotations if f.stem in self.ids}\n",
        "    self.ids = list(self.ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    id = self.ids[index]\n",
        "    image_path = self.images[id]\n",
        "    label_path = self.annotations[id]\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    boxes, labels = self.label_parser(label_path)\n",
        "    target = {\n",
        "        \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
        "        \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
        "    }\n",
        "    image = self.transforms(image)\n",
        "    return image, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n",
        "\n",
        "  def compute_total_number_of_objects(self, max_workers=8):\n",
        "    def count_objects(id):\n",
        "      boxes, _ = self.label_parser(self.annotations[id])\n",
        "      return len(boxes)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "      results = executor.map(count_objects, self.ids)\n",
        "    return sum(results)"
      ],
      "metadata": {
        "id": "4usg0yx6pTIN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare datasets for PyTorch"
      ],
      "metadata": {
        "id": "-kt6q3Jqwu-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing HRSC training dataset...\")\n",
        "HRSC_TRAIN_DATASET = TorchDataset(HRSC_TRAIN_IMAGES, HRSC_TRAIN_ANNOTATIONS, parse_hrsc_labels)\n",
        "print(f\"...Dataset prepared: {HRSC_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC training dataset sample:\", HRSC_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing HRSC validation dataset...\")\n",
        "HRSC_VAL_DATASET = TorchDataset(HRSC_VAL_IMAGES, HRSC_VAL_ANNOTATIONS, parse_hrsc_labels)\n",
        "print(f\"...Dataset prepared: {HRSC_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC validation dataset sample:\", HRSC_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing HRSC testing dataset...\")\n",
        "HRSC_TEST_DATASET = TorchDataset(HRSC_TEST_IMAGES, HRSC_TEST_ANNOTATIONS, parse_hrsc_labels)\n",
        "print(f\"...Dataset prepared: {HRSC_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC testing dataset sample:\", HRSC_TEST_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA training dataset...\")\n",
        "DOTA_TRAIN_DATASET = TorchDataset(DOTA_TRAIN_IMAGES, DOTA_TRAIN_ANNOTATIONS, parse_dota_labels)\n",
        "print(f\"...Dataset prepared: {DOTA_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA training dataset sample:\", DOTA_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA validation dataset...\")\n",
        "DOTA_VAL_DATASET = TorchDataset(DOTA_VAL_IMAGES, DOTA_VAL_ANNOTATIONS, parse_dota_labels)\n",
        "print(f\"...Dataset prepared: {DOTA_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA validation dataset sample:\", DOTA_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA testing dataset...\")\n",
        "DOTA_TEST_DATASET = TorchDataset(DOTA_TEST_IMAGES, DOTA_TEST_ANNOTATIONS, parse_dota_labels)\n",
        "print(f\"...Dataset prepared: {DOTA_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA testing dataset sample:\", DOTA_TEST_DATASET.ids[:5])"
      ],
      "metadata": {
        "id": "c3s53360w44K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fa61b9-f75f-4027-e9a2-4eb75f4e332a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing HRSC training dataset...\n",
            "...Dataset prepared: 1207 total objects\n",
            "HRSC training dataset sample: ['100001571', '100000982', '100001634', '100001320', '100000821']\n",
            "Preparing HRSC validation dataset...\n",
            "...Dataset prepared: 541 total objects\n",
            "HRSC validation dataset sample: ['100001336', '100001479', '100000006', '100000641', '100001679']\n",
            "Preparing HRSC testing dataset...\n",
            "...Dataset prepared: 1228 total objects\n",
            "HRSC testing dataset sample: ['100000716', '100001491', '100001384', '100000659', '100001517']\n",
            "Preparing DOTA training dataset...\n",
            "...Dataset prepared: 143302 total objects\n",
            "DOTA training dataset sample: ['P1070', 'P0522', 'P1842', 'P0821', 'P0580']\n",
            "Preparing DOTA validation dataset...\n",
            "...Dataset prepared: 69565 total objects\n",
            "DOTA validation dataset sample: ['P0684', 'P1005', 'P0198', 'P1398', 'P0837']\n",
            "Preparing DOTA testing dataset...\n",
            "...Dataset prepared: 67329 total objects\n",
            "DOTA testing dataset sample: ['P1084', 'P1980', 'P0677', 'P0535', 'P2783']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define PyTorch Faster R-CNN model"
      ],
      "metadata": {
        "id": "c2O8GWlr2sif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms as T\n",
        "\n",
        "class FasterRCNNModel:\n",
        "  def __init__(self, train_dataset: TorchDataset, val_dataset: TorchDataset, test_dataset: TorchDataset, class_names: list, batch_size=4, shuffle_datasets=False) -> None:\n",
        "    self.train_dataset = train_dataset\n",
        "    self.val_dataset = val_dataset\n",
        "    self.test_dataset = test_dataset\n",
        "\n",
        "    self.class_names = list(class_names) + [\"__background__\"]\n",
        "    self.num_classes = len(class_names)\n",
        "\n",
        "    collate_fn = lambda x: tuple(zip(*x))\n",
        "    self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    self.val_loader = torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    if len(self.test_dataset) > 0:\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    else:\n",
        "        self.test_loader = None\n",
        "\n",
        "    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    self.model = fasterrcnn_resnet50_fpn(weights=weights)\n",
        "    self.in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "    self.model.roi_heads.box_predictor = FastRCNNPredictor(self.in_features, self.num_classes)\n",
        "    self.model.to(DEVICE)\n",
        "\n",
        "  def train(self, num_epochs=50):\n",
        "    # TODO Add an \"RoI Learner\" after ROI pooling to predit rotated offsets (dx, dy, dw, dh, dtheta)\n",
        "    # TODO Replace roi_align with torchvision.ops.roi_align_rotated\n",
        "    # TODO Add rotation-aware losses\n",
        "\n",
        "    # Example optimizer\n",
        "    optimizer = torch.optim.SGD(self.model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
        "\n",
        "      # Training loop\n",
        "      print(\"\\tStarting training loop...\")\n",
        "      train_loss = 0.0\n",
        "      self.model.train()\n",
        "      for images, targets in self.train_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = self.model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += losses.item()\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      print(\"\\t...Training loop complete.\")\n",
        "\n",
        "      # Validation loop\n",
        "      print(\"\\tStarting validation loop...\")\n",
        "      val_loss = 0.0\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for images, targets in self.val_loader:\n",
        "          images = [img.to(DEVICE) for img in images]\n",
        "          targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "          loss_dict = self.model(images, targets)\n",
        "          losses = sum(loss for loss in loss_dict.values())\n",
        "          val_loss += losses.item()\n",
        "      print(\"\\t...Validation loop complete.\")\n",
        "\n",
        "      print(f\"...Finished epoch {epoch+1}/{num_epochs}, Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    torch.save({\n",
        "        \"model_state_dict\": self.model.state_dict(),\n",
        "        \"class_names\": self.class_names\n",
        "    }, \"faster_rcnn_model.pth\")\n",
        "\n",
        "  def test_results(self):\n",
        "    if self.test_loader is None:\n",
        "        print(\"Test dataset is empty. Skipping evaluation.\")\n",
        "        return [], [], []\n",
        "\n",
        "    self.model.eval()\n",
        "    all_boxes, all_labels, all_scores = [], [], []\n",
        "    with torch.no_grad():\n",
        "      for images, _ in self.test_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        predictions = self.model(images)\n",
        "        for prediction in predictions:\n",
        "          boxes, labels, scores = prediction['boxes'], prediction['labels'], prediction['scores']\n",
        "          all_boxes.append(boxes.cpu().numpy())\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_scores.append(scores.cpu().numpy())\n",
        "    return all_boxes, all_labels, all_scores\n",
        "\n",
        "# TODO\n",
        "# -------------------------------------------------------------------\n",
        "# FUTURE UPGRADE: RoI TRANSFORMER INTEGRATION\n",
        "# -------------------------------------------------------------------\n",
        "# To integrate the RoI Transformer:\n",
        "#   1. Add a custom RRoI Learner layer:\n",
        "#        fc = nn.Linear(roi_feature_dim, 5)\n",
        "#        -> predicts (dx, dy, dw, dh, dtheta)\n",
        "#   2. Compute rotated boxes and apply torchvision.ops.roi_align_rotated\n",
        "#   3. Replace standard SmoothL1 loss with rotated IoU or 5D regression loss\n",
        "#   4. Use torchvision.ops.box_iou_rotated() and nms_rotated() for matching and inference\n",
        "#   5. Dataset boxes should include rotation (x, y, w, h, theta)"
      ],
      "metadata": {
        "id": "Y1hJruRt2yVu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset models"
      ],
      "metadata": {
        "id": "20AyC4tw8HuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_MODEL = FasterRCNNModel(HRSC_TRAIN_DATASET, HRSC_VAL_DATASET, HRSC_TEST_DATASET, [id for id, _ in sorted(HRSC_CLASSES.items(), key=lambda item: item[1])])\n",
        "DOTA_MODEL = FasterRCNNModel(DOTA_TRAIN_DATASET, DOTA_VAL_DATASET, DOTA_TEST_DATASET, [id for id, _ in sorted(DOTA_CLASSES.items(), key=lambda item: item[1])])"
      ],
      "metadata": {
        "id": "tFPaQy7t8MaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5af0f5-f8b1-4efe-b464-22d388269616"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 263MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train dataset models"
      ],
      "metadata": {
        "id": "eLEecH3487cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training HRSC model...\")\n",
        "HRSC_MODEL.train()\n",
        "print(\"...HRSC model trained.\")\n",
        "\n",
        "print(\"Training DOTA model...\")\n",
        "DOTA_MODEL.train()\n",
        "print(\"...DOTA model trained.\")"
      ],
      "metadata": {
        "id": "-_VvrW4p9NVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596e6e50-e6ae-4f6c-8cb6-4f8605c64a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training HRSC model...\n",
            "Starting epoch 1/50...\n",
            "\tStarting training loop...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate dataset models"
      ],
      "metadata": {
        "id": "fo9xloKX89bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hrsc_boxes, hrsc_labels, hrsc_scores = HRSC_MODEL.test_results()\n",
        "dota_boxes, dota_labels, dota_scores = DOTA_MODEL.test_results()\n",
        "# TODO analyse results and visualize them"
      ],
      "metadata": {
        "id": "sthCTKuq9Pmb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}