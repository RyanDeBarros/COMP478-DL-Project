{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import PyTorch modules"
      ],
      "metadata": {
        "id": "RfBeheCNos9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaRjQJEDokeO"
      },
      "outputs": [],
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "import os\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "EjYFfoM3sHvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SHARED_PATH = Path(\"drive/MyDrive/Colab Notebooks/Shared\")\n",
        "HRSC_PATH = SHARED_PATH / \"HRSC2016_Final_Splits\"\n",
        "DOTA_PATH = SHARED_PATH / \"DOTA_Final_Splits\"\n",
        "NWPU_PATH = SHARED_PATH / \"NWPU_VHR-10_Final_Splits\"\n",
        "\n",
        "print(\"HRSC subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in HRSC_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "print(\"DOTA subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in DOTA_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "print(\"NWPU subfolders:\", [f\"{subfolder.name}/{path.name}\" for subfolder in NWPU_PATH.iterdir() for path in subfolder.iterdir()])\n",
        "\n",
        "HRSC_TRAIN_IMAGES = HRSC_PATH/ \"train/images\"\n",
        "HRSC_TRAIN_ANNOTATIONS = HRSC_PATH / \"train/annotations\"\n",
        "HRSC_VAL_IMAGES = HRSC_PATH / \"val/images\"\n",
        "HRSC_VAL_ANNOTATIONS = HRSC_PATH / \"val/annotations\"\n",
        "HRSC_TEST_IMAGES = HRSC_PATH / \"test/images\"\n",
        "HRSC_TEST_ANNOTATIONS = HRSC_PATH / \"test/annotations\"\n",
        "\n",
        "DOTA_TRAIN_IMAGES = DOTA_PATH / \"train/images\"\n",
        "DOTA_TRAIN_ANNOTATIONS = DOTA_PATH / \"train/hbb\"\n",
        "DOTA_VAL_IMAGES = DOTA_PATH / \"val/images\"\n",
        "DOTA_VAL_ANNOTATIONS = DOTA_PATH / \"val/hbb\"\n",
        "DOTA_TEST_IMAGES = DOTA_PATH / \"test/images\"\n",
        "DOTA_TEST_ANNOTATIONS = DOTA_PATH / \"test/hbb\"\n",
        "\n",
        "NWPU_TRAIN_IMAGES = NWPU_PATH / \"train/images\"\n",
        "NWPU_TRAIN_ANNOTATIONS = NWPU_PATH / \"train/annotations\"\n",
        "NWPU_VAL_IMAGES = NWPU_PATH / \"val/images\"\n",
        "NWPU_VAL_ANNOTATIONS = NWPU_PATH / \"val/annotations\"\n",
        "NWPU_TEST_IMAGES = NWPU_PATH / \"test/images\"\n",
        "NWPU_TEST_ANNOTATIONS = NWPU_PATH / \"test/annotations\""
      ],
      "metadata": {
        "id": "PT9GIwS8sX4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore datasets"
      ],
      "metadata": {
        "id": "yp-lr_Prs6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore, Style\n",
        "\n",
        "def explore_header(dataset: str, subfolder: str):\n",
        "  print(\"Exploring\", Fore.GREEN + dataset + Style.RESET_ALL, Fore.MAGENTA + subfolder + Style.RESET_ALL, \"folder...\")\n",
        "\n",
        "def explore(images_folder: Path, ext: str):\n",
        "  files = list(images_folder.glob(f\"*.{ext}\"))\n",
        "  print(f\"Number of {ext.upper()} files:\", len(files))\n",
        "  print(f\"{ext.upper()} sample files:\", [path.name for path in files[:3]])\n",
        "\n",
        "explore_header(\"HRSC\", \"train\")\n",
        "explore(HRSC_TRAIN_IMAGES, \"bmp\")\n",
        "explore(HRSC_TRAIN_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"HRSC\", \"val\")\n",
        "explore(HRSC_VAL_IMAGES, \"bmp\")\n",
        "explore(HRSC_VAL_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"HRSC\", \"test\")\n",
        "explore(HRSC_TEST_IMAGES, \"bmp\")\n",
        "explore(HRSC_TEST_ANNOTATIONS, \"xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"train\")\n",
        "explore(DOTA_TRAIN_IMAGES, \"png\")\n",
        "explore(DOTA_TRAIN_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"val\")\n",
        "explore(DOTA_VAL_IMAGES, \"png\")\n",
        "explore(DOTA_VAL_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"DOTA\", \"test\")\n",
        "explore(DOTA_TEST_IMAGES, \"png\")\n",
        "explore(DOTA_TEST_ANNOTATIONS, \"txt\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"NWPU\", \"train\")\n",
        "explore(NWPU_TRAIN_IMAGES, \"jpg\")\n",
        "explore(NWPU_TRAIN_ANNOTATIONS, \"json\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"NWPU\", \"val\")\n",
        "explore(NWPU_VAL_IMAGES, \"jpg\")\n",
        "explore(NWPU_VAL_ANNOTATIONS, \"json\")\n",
        "\n",
        "print()\n",
        "\n",
        "explore_header(\"NWPU\", \"test\")\n",
        "explore(NWPU_TEST_IMAGES, \"jpg\")\n",
        "explore(NWPU_TEST_ANNOTATIONS, \"json\")"
      ],
      "metadata": {
        "id": "jf-wMzlNs8Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define label parsers"
      ],
      "metadata": {
        "id": "ELZ-eU81tvbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "HRSC_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_hrsc_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "  tree = ET.parse(label_path.as_posix())\n",
        "  root = tree.getroot()\n",
        "\n",
        "  objects = root.findall(\".//HRSC_Object\")\n",
        "  for obj in objects:\n",
        "    try:\n",
        "      # Bounds\n",
        "      center_x = float(obj.find('mbox_cx').text)\n",
        "      center_y = float(obj.find('mbox_cy').text)\n",
        "      width = float(obj.find('mbox_w').text)\n",
        "      height = float(obj.find('mbox_h').text)\n",
        "      angle = float(obj.find('mbox_ang').text) * 180 / torch.pi\n",
        "\n",
        "      # Category\n",
        "      class_id = int(obj.find('Class_ID').text)\n",
        "      if class_id not in HRSC_CLASSES:\n",
        "        HRSC_CLASSES[class_id] = len(HRSC_CLASSES)\n",
        "      class_id_index = HRSC_CLASSES[class_id]\n",
        "\n",
        "      boxes.append([center_x, center_y, width, height, angle])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "\n",
        "def hrsc_image_rescale(label_path: Path, image_width, image_height):\n",
        "  tree = ET.parse(label_path.as_posix())\n",
        "  root = tree.getroot()\n",
        "  width = int(root.find('.//Img_SizeWidth').text)\n",
        "  height = int(root.find('.//Img_SizeHeight').text)\n",
        "  return image_width / width, image_height / height\n",
        "\n",
        "\n",
        "DOTA_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_dota_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "\n",
        "  # Find all objects\n",
        "  for line in itertools.islice(label_path.read_text().splitlines(), 2, None):  # Start from line index 2\n",
        "    try:\n",
        "      obj = line.strip().split()\n",
        "      x1, y1, x2, y2, x3, y3, x4, y4, category, difficulty = (*map(float, obj[:8]), *obj[8:])\n",
        "\n",
        "      # Bounds\n",
        "      points = [(x1, y1), (x2, y2), (x3, y3), (x4, y4)]\n",
        "      pts_np = np.array(points, dtype=np.float32).reshape(-1, 1, 2)\n",
        "      (cx, cy), (w, h), angle = cv2.minAreaRect(pts_np)\n",
        "\n",
        "      # Category\n",
        "      if category not in DOTA_CLASSES:\n",
        "        DOTA_CLASSES[category] = len(DOTA_CLASSES)\n",
        "      class_id_index = DOTA_CLASSES[category]\n",
        "\n",
        "      boxes.append([cx, cy, w, h, angle])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "\n",
        "def dota_image_rescale(label_path: Path, image_width, image_height):\n",
        "  return 1.0, 1.0\n",
        "\n",
        "\n",
        "NWPU_CLASSES = {}\n",
        "\n",
        "\n",
        "def parse_nwpu_labels(label_path: Path):\n",
        "  boxes = []\n",
        "  labels = []\n",
        "\n",
        "  data = json.loads(label_path.read_text())\n",
        "  for category in data['categories']:\n",
        "    NWPU_CLASSES[category['name']] = category['id'] - 1  # categories are 1-indexed -> convert to 0-indexed\n",
        "\n",
        "  for annotation in data['annotations']:\n",
        "    try:\n",
        "      # Bounds\n",
        "      segmentation = annotation['segmentation'][0]\n",
        "      pts_np = np.array(segmentation, dtype=np.float32).reshape(-1, 2)\n",
        "      (cx, cy), (w, h), angle = cv2.minAreaRect(pts_np)\n",
        "\n",
        "      # Category\n",
        "      class_id_index = annotation['category_id'] - 1\n",
        "\n",
        "      boxes.append([cx, cy, w, h, angle])\n",
        "      labels.append(class_id_index)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(Fore.RED + \"Warning\" + Style.RESET_ALL + f\": Could not parse object in {label_path.as_posix()}: {e}\")\n",
        "      continue\n",
        "\n",
        "  return boxes, labels\n",
        "\n",
        "def nwpu_image_rescale(label_path: Path, image_width, image_height):\n",
        "  data = json.loads(label_path.read_text())\n",
        "  image = data['images'][0]\n",
        "  width = image['width']\n",
        "  height = image['height']\n",
        "  return image_width / width, image_height / height"
      ],
      "metadata": {
        "id": "c1iomq0mtvAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch dataset structure"
      ],
      "metadata": {
        "id": "MVdX-5_GpPse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "BASIC_TRANSFORM = T.Compose([T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_folder: Path, annotations_folder: Path, label_parser, image_rescale, transforms=BASIC_TRANSFORM, max_files=10, target_size=800) -> None:\n",
        "    super().__init__()\n",
        "    self.label_parser = label_parser\n",
        "    self.image_rescale = image_rescale\n",
        "    self.transforms = transforms\n",
        "    self.target_size = target_size\n",
        "\n",
        "    images = (\n",
        "      sorted(\n",
        "        [f for f in images_folder.iterdir() if f.suffix.lower() in (\".jpg\", \".png\", \".bmp\")],\n",
        "        key=lambda p: p.stem.lower()\n",
        "      )\n",
        "      if images_folder.exists() else []\n",
        "    )\n",
        "\n",
        "    annotations = (\n",
        "      sorted(\n",
        "        [f for f in annotations_folder.iterdir() if f.suffix.lower() in (\".xml\", \".txt\", \".json\")],\n",
        "        key=lambda p: p.stem.lower()\n",
        "      )\n",
        "      if annotations_folder.exists() else []\n",
        "    )\n",
        "\n",
        "    if max_files > 0:\n",
        "      images = images[:max_files]\n",
        "      annotations = annotations[:max_files]\n",
        "\n",
        "    # Keep only images/annotations one-to-one correspondences\n",
        "    image_set = set(f.stem for f in images)\n",
        "    annotation_set = set(f.stem for f in annotations)\n",
        "    self.ids = image_set.intersection(annotation_set)\n",
        "    self.images = {f.stem: f for f in images if f.stem in self.ids}\n",
        "    self.annotations = {f.stem: f for f in annotations if f.stem in self.ids}\n",
        "    self.ids = list(self.ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    id = self.ids[index]\n",
        "    image_path = self.images[id]\n",
        "    label_path = self.annotations[id]\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    boxes, labels = self.label_parser(label_path)\n",
        "\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    labels = torch.as_tensor(labels, dtype=torch.int64) + 1  # Offset by 1 for background label\n",
        "\n",
        "    image, boxes = self.preprocess(image, boxes)\n",
        "    # Convert to tensor (normalized 0-1)\n",
        "    image = F.to_tensor(image)\n",
        "\n",
        "    target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "    image = self.transforms(image)\n",
        "    image.filepath = image_path;\n",
        "    return image, target\n",
        "\n",
        "  def preprocess(self, img, boxes):\n",
        "    old_w, old_h = img.width, img.height\n",
        "    scale = self.target_size / max(old_h, old_w)  # uniform scale\n",
        "\n",
        "    new_w = int(old_w * scale)\n",
        "    new_h = int(old_h * scale)\n",
        "\n",
        "    # Resize image\n",
        "    img = F.resize(img, (new_h, new_w))\n",
        "\n",
        "    # Scale boxes\n",
        "    boxes = boxes.clone()\n",
        "    boxes[:, 0] *= scale  # cx\n",
        "    boxes[:, 1] *= scale  # cy\n",
        "    boxes[:, 2] *= scale  # w\n",
        "    boxes[:, 3] *= scale  # h\n",
        "    # theta stays unchanged\n",
        "\n",
        "    # Pad to target_size x target_size (right and bottom)\n",
        "    pad_w = self.target_size - new_w\n",
        "    pad_h = self.target_size - new_h\n",
        "    img = F.pad(img, (0, 0, pad_w, pad_h))\n",
        "    return img, boxes\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n",
        "\n",
        "  def get_image_rescale(self, index):\n",
        "    id = self.ids[index]\n",
        "    image_path = self.images[id]\n",
        "    label_path = self.annotations[id]\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return self.image_rescale(label_path, image.width, image.height)\n",
        "\n",
        "  def compute_total_number_of_objects(self, max_workers=8):\n",
        "    def count_objects(id):\n",
        "      boxes, _ = self.label_parser(self.annotations[id])\n",
        "      return len(boxes)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "      results = executor.map(count_objects, self.ids)\n",
        "    return sum(results)"
      ],
      "metadata": {
        "id": "4usg0yx6pTIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare datasets for PyTorch"
      ],
      "metadata": {
        "id": "-kt6q3Jqwu-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing HRSC training dataset...\")\n",
        "HRSC_TRAIN_DATASET = TorchDataset(HRSC_TRAIN_IMAGES, HRSC_TRAIN_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC training dataset sample:\", HRSC_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing HRSC validation dataset...\")\n",
        "HRSC_VAL_DATASET = TorchDataset(HRSC_VAL_IMAGES, HRSC_VAL_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC validation dataset sample:\", HRSC_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing HRSC testing dataset...\")\n",
        "HRSC_TEST_DATASET = TorchDataset(HRSC_TEST_IMAGES, HRSC_TEST_ANNOTATIONS, parse_hrsc_labels, hrsc_image_rescale)\n",
        "print(f\"...Dataset prepared: {HRSC_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"HRSC testing dataset sample:\", HRSC_TEST_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA training dataset...\")\n",
        "DOTA_TRAIN_DATASET = TorchDataset(DOTA_TRAIN_IMAGES, DOTA_TRAIN_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA training dataset sample:\", DOTA_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA validation dataset...\")\n",
        "DOTA_VAL_DATASET = TorchDataset(DOTA_VAL_IMAGES, DOTA_VAL_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA validation dataset sample:\", DOTA_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing DOTA testing dataset...\")\n",
        "DOTA_TEST_DATASET = TorchDataset(DOTA_TEST_IMAGES, DOTA_TEST_ANNOTATIONS, parse_dota_labels, dota_image_rescale)\n",
        "print(f\"...Dataset prepared: {DOTA_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"DOTA testing dataset sample:\", DOTA_TEST_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing NWPU training dataset...\")\n",
        "NWPU_TRAIN_DATASET = TorchDataset(NWPU_TRAIN_IMAGES, NWPU_TRAIN_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TRAIN_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"NWPU training dataset sample:\", NWPU_TRAIN_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing NWPU validation dataset...\")\n",
        "NWPU_VAL_DATASET = TorchDataset(NWPU_VAL_IMAGES, NWPU_VAL_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_VAL_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"NWPU validation dataset sample:\", NWPU_VAL_DATASET.ids[:5])\n",
        "\n",
        "print(\"Preparing NWPU testing dataset...\")\n",
        "NWPU_TEST_DATASET = TorchDataset(NWPU_TEST_IMAGES, NWPU_TEST_ANNOTATIONS, parse_nwpu_labels, nwpu_image_rescale)\n",
        "print(f\"...Dataset prepared: {NWPU_TEST_DATASET.compute_total_number_of_objects()} total objects\")\n",
        "print(\"NWPU testing dataset sample:\", NWPU_TEST_DATASET.ids[:5])"
      ],
      "metadata": {
        "id": "c3s53360w44K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RRoI Layer"
      ],
      "metadata": {
        "id": "lx3W18UqXmH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection.roi_heads import RoIHeads\n",
        "from detectron2.layers import roi_align_rotated\n",
        "\n",
        "class RRoILearner(nn.Module):\n",
        "  \"\"\"Learns rotation offsets for each horizontal RoI.\"\"\"\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(in_features, 5)  # predicts dx, dy, dw, dh, dtheta\n",
        "\n",
        "  def forward(self, roi_features):\n",
        "    offsets = self.fc(roi_features)\n",
        "    return offsets\n",
        "\n",
        "\n",
        "class RotatedBoxPredictor(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super().__init__()\n",
        "    self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "    self.bbox_pred = nn.Linear(in_channels, num_classes * 5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if x.dim() == 4:\n",
        "      x = x.flatten(start_dim=1)\n",
        "    scores = self.cls_score(x)\n",
        "    bbox_deltas = self.bbox_pred(x)\n",
        "    return scores, bbox_deltas\n",
        "\n",
        "\n",
        "class RotatedRoIHeads(RoIHeads):\n",
        "  def __init__(self, box_roi_pool, box_head, box_predictor,\n",
        "               fg_iou_thresh=0.5,\n",
        "               bg_iou_thresh=0.5,\n",
        "               batch_size_per_image=512,\n",
        "               positive_fraction=0.25,\n",
        "               bbox_reg_weights=None,\n",
        "               score_thresh=0.05,\n",
        "               nms_thresh=0.5,\n",
        "               detections_per_img=100):\n",
        "    super().__init__(\n",
        "      box_roi_pool,\n",
        "      box_head,\n",
        "      box_predictor,\n",
        "      fg_iou_thresh,\n",
        "      bg_iou_thresh,\n",
        "      batch_size_per_image,\n",
        "      positive_fraction,\n",
        "      bbox_reg_weights,\n",
        "      score_thresh,\n",
        "      nms_thresh,\n",
        "      detections_per_img\n",
        "    )\n",
        "    in_features = self.box_head.fc7.out_features\n",
        "    self.rroi_learner = RRoILearner(in_features)\n",
        "\n",
        "  def forward(self, features, proposals, image_shapes, targets=None):\n",
        "    # 1. Standard RoI feature extraction (horizontal RoIs)\n",
        "    box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
        "    box_features = self.box_head(box_features)\n",
        "\n",
        "    # Flatten features per RoI for offset prediction\n",
        "    pooled_features = box_features\n",
        "    offsets = self.rroi_learner(pooled_features)  # (N_total_rois, 5)\n",
        "\n",
        "    # 2. Decode rotated boxes relative to horizontal proposals\n",
        "    rotated_boxes_list = []\n",
        "    start_idx = 0\n",
        "    for i, props in enumerate(proposals):\n",
        "      num_props = props.shape[0]\n",
        "      offs = offsets[start_idx:start_idx + num_props]\n",
        "\n",
        "      cx = (props[:, 0] + props[:, 2]) / 2\n",
        "      cy = (props[:, 1] + props[:, 3]) / 2\n",
        "      w = props[:, 2] - props[:, 0]\n",
        "      h = props[:, 3] - props[:, 1]\n",
        "\n",
        "      # Apply predicted offsets\n",
        "      cx_rot = cx + offs[:, 0] * w\n",
        "      cy_rot = cy + offs[:, 1] * h\n",
        "      w_rot = w * torch.exp(offs[:, 2])\n",
        "      h_rot = h * torch.exp(offs[:, 3])\n",
        "      angle_rot = offs[:, 4]\n",
        "\n",
        "      batch_idx = torch.full_like(cx, i, dtype=torch.float32)\n",
        "      rotated = torch.stack([batch_idx, cx_rot, cy_rot, w_rot, h_rot, angle_rot], dim=1)\n",
        "      rotated_boxes_list.append(rotated)\n",
        "\n",
        "      start_idx += num_props\n",
        "\n",
        "    rotated_boxes = torch.cat(rotated_boxes_list, dim=0)\n",
        "\n",
        "    # 3. Rotated RoI Align\n",
        "    # Ensure features is a dictionary (which it should be from FPN backbone) and extract the first feature map (typically P2, which is 1/4 original size).\n",
        "    if isinstance(features, dict) and len(features) > 0:\n",
        "      feature_map = next(iter(features.values())) # Robust way to get the first feature map\n",
        "      spatial_scale = 1.0 / 4.0 # For P2 feature map (1/4 downsampling)\n",
        "    else:\n",
        "      raise ValueError(\"Backbone features are not a non-empty dictionary as expected for Rotated RoI Align.\")\n",
        "\n",
        "    rotated_features = roi_align_rotated(\n",
        "      feature_map, # input\n",
        "      rotated_boxes, # boxes\n",
        "      (7, 7), # output_size\n",
        "      spatial_scale, # spatial_scale\n",
        "      2 # sampling_ratio\n",
        "    )\n",
        "\n",
        "    # 3b. Run through the box_head (this flattens 7×7 → vector)\n",
        "    rotated_features = self.box_head(rotated_features)\n",
        "\n",
        "    # 4. Classification and regression as usual\n",
        "    class_logits, box_regression = self.box_predictor(rotated_features)\n",
        "\n",
        "    return class_logits, box_regression, offsets\n",
        "\n",
        "\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "def rotated_box_to_polygon(box):\n",
        "  # box: tensor [5] = (cx, cy, w, h, angle_in_radians)\n",
        "  if box.numel() == 4:\n",
        "    # axis-aligned box\n",
        "    x1, y1, x2, y2 = box.tolist()\n",
        "    cx = (x1 + x2) / 2\n",
        "    cy = (y1 + y2) / 2\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    angle = 0.0\n",
        "  elif box.numel() == 5:\n",
        "    cx, cy, w, h, angle = box.tolist()\n",
        "  else:\n",
        "    raise ValueError(f\"Box must have 4 or 5 elements, got {box.numel()}\")\n",
        "\n",
        "  # Half sizes\n",
        "  hw = w / 2\n",
        "  hh = h / 2\n",
        "\n",
        "  # Corner points before rotation\n",
        "  corners = torch.tensor([\n",
        "    [-hw, -hh],\n",
        "    [ hw, -hh],\n",
        "    [ hw,  hh],\n",
        "    [-hw,  hh]\n",
        "  ], dtype=torch.float32)\n",
        "\n",
        "  # Rotation matrix\n",
        "  cosA = torch.cos(torch.tensor(angle))\n",
        "  sinA = torch.sin(torch.tensor(angle))\n",
        "  R = torch.tensor([\n",
        "    [cosA, -sinA],\n",
        "    [sinA,  cosA]\n",
        "  ])\n",
        "\n",
        "  # Rotate + translate corners\n",
        "  rotated = corners @ R.T\n",
        "  rotated[:, 0] += cx\n",
        "  rotated[:, 1] += cy\n",
        "\n",
        "  return Polygon(rotated.tolist())\n",
        "\n",
        "\n",
        "def box_iou_rotated(boxes1, boxes2):\n",
        "  # boxes1: [N,5]\n",
        "  # boxes2: [M,5]\n",
        "  # returns IoU matrix [N, M]\n",
        "\n",
        "  N = boxes1.shape[0]\n",
        "  M = boxes2.shape[0]\n",
        "\n",
        "  ious = torch.zeros((N, M), dtype=torch.float32)\n",
        "\n",
        "  for i in range(N):\n",
        "    p1 = rotated_box_to_polygon(boxes1[i])\n",
        "    area1 = p1.area\n",
        "    for j in range(M):\n",
        "      p2 = rotated_box_to_polygon(boxes2[j])\n",
        "\n",
        "      inter = p1.intersection(p2).area\n",
        "      union = area1 + p2.area - inter\n",
        "\n",
        "      if union > 0:\n",
        "        ious[i, j] = inter / union\n",
        "      else:\n",
        "        ious[i, j] = 0.0\n",
        "\n",
        "  return ious"
      ],
      "metadata": {
        "id": "kj_3WeAKXqgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define PyTorch Rotated Faster R-CNN Model"
      ],
      "metadata": {
        "id": "c2O8GWlr2sif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import ResNet50_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision import transforms as T\n",
        "\n",
        "class RotatedFasterRCNNModel(FasterRCNN):\n",
        "  def __init__(self, model_path: Path, train_dataset: TorchDataset, val_dataset: TorchDataset, test_dataset: TorchDataset, class_names: list, batch_size=4, shuffle_datasets=False) -> None:\n",
        "    self.model_path = model_path\n",
        "    self.train_dataset = train_dataset\n",
        "    self.val_dataset = val_dataset\n",
        "    self.test_dataset = test_dataset\n",
        "\n",
        "    self.class_names = [\"__background__\"] + list(class_names)\n",
        "    self.num_classes = len(self.class_names)\n",
        "\n",
        "    collate_fn = lambda x: tuple(zip(*x))\n",
        "    self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    self.val_loader = torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    if len(self.test_dataset) > 0:\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size, shuffle=shuffle_datasets, collate_fn=collate_fn)\n",
        "    else:\n",
        "        self.test_loader = None\n",
        "\n",
        "    super().__init__(backbone=resnet_fpn_backbone(backbone_name='resnet50', weights=ResNet50_Weights.DEFAULT), num_classes=self.num_classes, rpn_anchor_generator=None)\n",
        "    self.in_features = self.roi_heads.box_predictor.cls_score.in_features\n",
        "    self.box_detector = RotatedBoxPredictor(self.in_features, self.num_classes)\n",
        "    self.roi_heads = RotatedRoIHeads(self.roi_heads.box_roi_pool, self.roi_heads.box_head, self.box_detector)\n",
        "    self.to(DEVICE)\n",
        "\n",
        "  def train_model(self, num_epochs=50, learning_rate=0.0005):\n",
        "    # Example optimizer\n",
        "    optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
        "\n",
        "      # Training loop\n",
        "      print(\"\\tStarting training loop...\")\n",
        "      train_loss = 0.0\n",
        "      self.train()\n",
        "      for images, targets in self.train_loader:\n",
        "        loss_dict = self(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += losses.item()\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      print(\"\\t...Training loop complete.\")\n",
        "\n",
        "      # Validation loop\n",
        "      print(\"\\tStarting validation loop...\")\n",
        "      val_loss = 0.0\n",
        "      with torch.no_grad():\n",
        "        self.train()\n",
        "        for images, targets in self.val_loader:\n",
        "          loss_dict = self(images, targets)\n",
        "          losses = sum(loss for loss in loss_dict.values())\n",
        "          val_loss += losses.item()\n",
        "      print(\"\\t...Validation loop complete.\")\n",
        "\n",
        "      print(f\"...Finished epoch {epoch+1}/{num_epochs}, Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    self.save_weights()\n",
        "\n",
        "  def forward(self, images, targets=None):\n",
        "    \"\"\"\n",
        "    images: list of tensors [C,H,W]\n",
        "    targets: list of dicts {'boxes': [N,5], 'labels': [N]}\n",
        "    \"\"\"\n",
        "    self.training = targets is not None and self.training\n",
        "\n",
        "    # 1. Move images to device\n",
        "    images = [img.to(DEVICE) for img in images]\n",
        "    image_sizes = [img.shape[-2:] for img in images]  # list of (H,W)\n",
        "    images = ImageList(torch.stack(images), image_sizes)\n",
        "\n",
        "    # 2. Backbone\n",
        "    features = self.backbone(images.tensors)\n",
        "\n",
        "    # 3. RPN proposals\n",
        "    rpn_targets = []\n",
        "    for t in targets:\n",
        "      cx, cy, w, h, theta = t['boxes'].unbind(1)\n",
        "      x1 = cx - w/2\n",
        "      y1 = cy - h/2\n",
        "      x2 = cx + w/2\n",
        "      y2 = cy + h/2\n",
        "      rpn_targets.append({\n",
        "        'boxes': torch.stack([x1, y1, x2, y2], dim=1),\n",
        "        'labels': t['labels']\n",
        "      })\n",
        "\n",
        "    proposals, rpn_losses = self.rpn(images, features, rpn_targets)\n",
        "\n",
        "    # 4. Rotated RoI heads\n",
        "    class_logits, rotated_boxes, pred_offsets = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
        "\n",
        "    # 5. Compute losses\n",
        "    loss_dict = {}\n",
        "    if self.training and targets is not None:\n",
        "      # RPN losses\n",
        "      loss_dict.update(rpn_losses)\n",
        "      # Rotated box loss\n",
        "      loss_dict['loss_rbox'] = self.rotated_box_loss(pred_offsets, proposals, targets)\n",
        "      # Classification loss\n",
        "      proposal_boxes = self.xyxy_to_cxcywh_angle(torch.cat(proposals, dim=0))  # [N_rois,5]\n",
        "      roi_labels = self.assign_labels_to_rois(proposal_boxes, targets)\n",
        "      loss_dict['loss_classifier'] = nn.CrossEntropyLoss()(class_logits, roi_labels)\n",
        "\n",
        "    if self.training:\n",
        "      return loss_dict\n",
        "    else:\n",
        "      # inference\n",
        "      return class_logits, rotated_boxes\n",
        "\n",
        "  @staticmethod\n",
        "  def xyxy_to_cxcywh_angle(boxes):\n",
        "    # boxes: [N,4] = x1,y1,x2,y2 or [N,5]\n",
        "    if boxes.shape[1] == 4:\n",
        "      x1, y1, x2, y2 = boxes.unbind(dim=1)\n",
        "      cx = (x1 + x2) / 2\n",
        "      cy = (y1 + y2) / 2\n",
        "      w = x2 - x1\n",
        "      h = y2 - y1\n",
        "      angle = torch.zeros_like(cx)\n",
        "      return torch.stack([cx, cy, w, h, angle], dim=1)\n",
        "    elif boxes.shape[1] == 5:\n",
        "      return boxes\n",
        "    else:\n",
        "      raise ValueError(f\"Boxes must be 4 or 5 dims, got {boxes.shape[1]}\")\n",
        "\n",
        "  @staticmethod\n",
        "  def assign_labels_to_rois(proposals, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    proposals: [N_total_rois, 5]  (cx,cy,w,h,θ)\n",
        "    targets: list of dicts {'boxes':[M_i,5], 'labels':[M_i]}\n",
        "    returns:\n",
        "      roi_labels: [N_total_rois]\n",
        "    \"\"\"\n",
        "    device = proposals.device\n",
        "    target_boxes = torch.cat([t['boxes'] for t in targets], dim=0).to(device)\n",
        "    target_labels = torch.cat([t['labels'] for t in targets], dim=0).to(device)\n",
        "\n",
        "    ious = box_iou_rotated(proposals, target_boxes)  # [N_rois, N_gt]\n",
        "    max_iou, max_idx = ious.max(dim=1)\n",
        "\n",
        "    roi_labels = torch.zeros(proposals.shape[0], dtype=torch.long, device=device)  # background=0\n",
        "    positive_mask = max_iou > iou_threshold\n",
        "    roi_labels[positive_mask] = target_labels[max_idx[positive_mask]]\n",
        "\n",
        "    return roi_labels\n",
        "\n",
        "  @staticmethod\n",
        "  def rotated_box_loss(pred_offsets, proposals, targets):\n",
        "    # Flatten proposals into a single tensor [num_rois, 5 or 4]\n",
        "    proposal_boxes = torch.cat(proposals, dim=0).to(DEVICE)  # [N_total, 4]\n",
        "    proposal_boxes = RotatedFasterRCNNModel.xyxy_to_cxcywh_angle(proposal_boxes)    # convert to [N_total, 5]\n",
        "\n",
        "    target_boxes = torch.cat([t[\"boxes\"] for t in targets], dim=0).to(DEVICE)  # [M,5]\n",
        "\n",
        "    # Match proposals to the target boxes (IoU-based)\n",
        "    # Using rotated IoU if available\n",
        "    with torch.no_grad():\n",
        "      # Compute IoU matrix\n",
        "      ious = box_iou_rotated(proposal_boxes, target_boxes)\n",
        "\n",
        "      # For each RoI, take best match\n",
        "      matched_vals, matched_idx = ious.max(dim=1)\n",
        "      matched_gt = target_boxes[matched_idx]\n",
        "\n",
        "      # Compute rotated regression targets (5-d)\n",
        "      px, py, pw, ph, pa = proposal_boxes.unbind(dim=1)\n",
        "      gx, gy, gw, gh, ga = matched_gt.unbind(dim=1)\n",
        "\n",
        "      # Eq(1) from the paper\n",
        "      dx = ((gx - px) * torch.cos(pa) + (gy - py) * torch.sin(pa)) / pw\n",
        "      dy = ((gy - py) * torch.cos(pa) - (gx - px) * torch.sin(pa)) / ph\n",
        "      dw = torch.log(gw / pw)\n",
        "      dh = torch.log(gh / ph)\n",
        "      dtheta = ((ga - pa) % (2 * torch.pi)) / (2 * torch.pi)\n",
        "      target_offsets = torch.stack([dx, dy, dw, dh, dtheta], dim=1)\n",
        "\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "    return loss_fn(pred_offsets, target_offsets)\n",
        "\n",
        "  def test_results(self):\n",
        "    if self.test_loader is None:\n",
        "        print(\"Test dataset is empty. Skipping evaluation.\")\n",
        "        return [], [], []\n",
        "\n",
        "    self.load_weights()\n",
        "    all_boxes, all_labels, all_scores = [], [], []\n",
        "    with torch.no_grad():\n",
        "      self.eval()\n",
        "      for images, _ in self.test_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        predictions = self(images)\n",
        "        for prediction in predictions:\n",
        "          boxes, labels, scores = prediction['boxes'], prediction['labels'], prediction['scores']\n",
        "          all_boxes.append(boxes.detach().cpu().numpy())\n",
        "          all_labels.append(labels.detach().cpu().numpy())\n",
        "          all_scores.append(scores.detach().cpu().numpy())\n",
        "    return all_boxes, all_labels, all_scores\n",
        "\n",
        "  def save_weights(self):\n",
        "    torch.save({\n",
        "        \"model_state_dict\": self.state_dict(),\n",
        "        \"class_names\": self.class_names\n",
        "    }, self.model_path)\n",
        "\n",
        "  def load_weights(self):\n",
        "    checkpoint = torch.load(self.model_path, map_location=DEVICE)\n",
        "    self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    self.class_names = checkpoint[\"class_names\"]\n",
        "\n",
        "\n",
        "# TODO Replace standard SmoothL1 loss with rotated IoU or 5D regression loss\n",
        "# TODO use nms_rotated() for matching and inference?"
      ],
      "metadata": {
        "id": "Y1hJruRt2yVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset models"
      ],
      "metadata": {
        "id": "20AyC4tw8HuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_MODEL = RotatedFasterRCNNModel(Path(\"hrsc_faster_rcnn_model.pth\"), HRSC_TRAIN_DATASET, HRSC_VAL_DATASET, HRSC_TEST_DATASET, [id for id, _ in sorted(HRSC_CLASSES.items(), key=lambda item: item[1])])\n",
        "DOTA_MODEL = RotatedFasterRCNNModel(Path(\"dota_faster_rcnn_model.pth\"), DOTA_TRAIN_DATASET, DOTA_VAL_DATASET, DOTA_TEST_DATASET, [id for id, _ in sorted(DOTA_CLASSES.items(), key=lambda item: item[1])])\n",
        "NWPU_MODEL = RotatedFasterRCNNModel(Path(\"nwpu_faster_rcnn_model.pth\"), NWPU_TRAIN_DATASET, NWPU_VAL_DATASET, NWPU_TEST_DATASET, [id for id, _ in sorted(NWPU_CLASSES.items(), key=lambda item: item[1])])"
      ],
      "metadata": {
        "id": "tFPaQy7t8MaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train dataset models"
      ],
      "metadata": {
        "id": "eLEecH3487cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if HRSC_MODEL.model_path.exists():\n",
        "  print(\"Loading HRSC weights...\")\n",
        "  HRSC_MODEL.load_weights()\n",
        "  print(\"...HRSC weights loaded.\")\n",
        "else:\n",
        "  print(\"Training HRSC model...\")\n",
        "  HRSC_MODEL.train_model(3)\n",
        "  print(\"...HRSC model trained.\")\n",
        "\n",
        "if DOTA_MODEL.model_path.exists():\n",
        "  print(\"Loading DOTA weights...\")\n",
        "  DOTA_MODEL.load_weights()\n",
        "  print(\"...DOTA weights loaded.\")\n",
        "else:\n",
        "  print(\"Training DOTA model...\")\n",
        "  DOTA_MODEL.train_model(3)\n",
        "  print(\"...DOTA model trained.\")\n",
        "\n",
        "if NWPU_MODEL.model_path.exists():\n",
        "  print(\"Loading NWPU weights...\")\n",
        "  NWPU_MODEL.load_weights()\n",
        "  print(\"...NWPU weights loaded.\")\n",
        "else:\n",
        "  print(\"Training NWPU model...\")\n",
        "  NWPU_MODEL.train(3)\n",
        "  print(\"...NWPU model trained.\")"
      ],
      "metadata": {
        "id": "-_VvrW4p9NVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate dataset models"
      ],
      "metadata": {
        "id": "fo9xloKX89bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_MODEL.roi_heads.score_thresh = 1.0e-6\n",
        "HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES = HRSC_MODEL.test_results()\n",
        "\n",
        "DOTA_MODEL.roi_heads.score_thresh = 0.012\n",
        "DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES = DOTA_MODEL.test_results()\n",
        "\n",
        "NWPU_MODEL.model.roi_heads.score_thresh = 0.001\n",
        "NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES = NWPU_MODEL.test_results()"
      ],
      "metadata": {
        "id": "sthCTKuq9Pmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "FJFkIFmdPr4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class Visualizer:\n",
        "  def __init__(self, test_dataset: TorchDataset, class_names: list, boxes: list, labels: list, scores: list, results_folder: Path):\n",
        "    self.test_dataset = test_dataset\n",
        "    self.class_names = class_names\n",
        "    self.boxes = boxes\n",
        "    self.labels = labels\n",
        "    self.scores = scores\n",
        "    self.results_folder = results_folder\n",
        "    self.results_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  def overlay_rotated_box(self, output, box, wmult, hmult, color, label, score):\n",
        "    center_x, center_y, width, height, theta = box\n",
        "\n",
        "    category = self.class_names[label]\n",
        "    if score is None:\n",
        "      text = f\"{category}\"\n",
        "    else:\n",
        "      text = f\"{category} - score={score:.3g}\"\n",
        "\n",
        "    center_x = float(center_x) * wmult\n",
        "    center_y = float(center_y) * hmult\n",
        "    width = float(width) * wmult\n",
        "    height = float(height) * hmult\n",
        "    # TODO scale theta?\n",
        "    box_points = cv2.boxPoints(((center_x, center_y), (width, height), theta)).astype(np.int32)\n",
        "\n",
        "    cv2.drawContours(output, [box_points], 0, color, 1)\n",
        "    text_pos = tuple(box_points[1])\n",
        "    cv2.putText(output, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "  def visualize(self, index):\n",
        "    image, target = self.test_dataset[index]\n",
        "    wmult, hmult = self.test_dataset.get_image_rescale(index)\n",
        "\n",
        "    # Convert tensor image (C,H,W) → numpy (H,W,C)\n",
        "    if isinstance(image, np.ndarray):\n",
        "      output = image.copy()\n",
        "    else:\n",
        "      output = image.permute(1, 2, 0).cpu().numpy()\n",
        "      output = (output * 255).astype(np.uint8).copy()\n",
        "\n",
        "    # Ground truth\n",
        "    true_boxes = target[\"boxes\"]\n",
        "    true_labels = target[\"labels\"]\n",
        "\n",
        "    # Predictions\n",
        "    predicted_boxes = self.boxes[index]\n",
        "    predicted_labels = self.labels[index]\n",
        "    predicted_scores = self.scores[index]\n",
        "\n",
        "    # Draw boxes\n",
        "    for box, label in zip(true_boxes, true_labels):\n",
        "      self.overlay_rotated_box(output, box, wmult, hmult, (0, 255, 0), label, None)\n",
        "\n",
        "    for box, label, score in zip(predicted_boxes, predicted_labels, predicted_scores):\n",
        "      self.overlay_rotated_box(output, box, 1.0, 1.0, (255, 0, 0), label, score)\n",
        "\n",
        "    # Save output\n",
        "    output_path = self.results_folder / f\"{self.test_dataset.ids[index]}.png\"\n",
        "    cv2.imwrite(output_path.as_posix(), cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n",
        "    print(f\"Saved visualization to {output_path.as_posix()}\")\n",
        "\n",
        "  def visualize_multiple(self, count = 100, start_index = 0, max_workers=4):\n",
        "    end_index = min(start_index + count, len(self.test_dataset)) if count > 0 else len(self.test_dataset)\n",
        "    indices = list(range(start_index, end_index))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "      futures = {executor.submit(self.visualize, i): i for i in indices}\n",
        "      for future in as_completed(futures):\n",
        "        idx = futures[future]\n",
        "        try:\n",
        "          future.result()\n",
        "        except Exception as e:\n",
        "          print(Fore.RED + f\"Visualization failed for index {idx}:\" + Style.RESET_ALL, e)"
      ],
      "metadata": {
        "id": "kVbUuJJfPrhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize results"
      ],
      "metadata": {
        "id": "vG0e5VAvc7lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESULTS_PARENT_FOLDER = Path(\"drive/MyDrive/Colab Notebooks/Results\")\n",
        "HRSC_VISUALIZER = Visualizer(HRSC_TEST_DATASET, HRSC_MODEL.class_names, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES, RESULTS_PARENT_FOLDER / \"HRSC\")\n",
        "DOTA_VISUALIZER = Visualizer(DOTA_TEST_DATASET, DOTA_MODEL.class_names, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES, RESULTS_PARENT_FOLDER / \"DOTA\")\n",
        "NWPU_VISUALIZER = Visualizer(NWPU_TEST_DATASET, NWPU_MODEL.class_names, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES, RESULTS_PARENT_FOLDER / \"unrotated\" / \"NWPU\")\n",
        "\n",
        "HRSC_VISUALIZER.visualize_multiple()\n",
        "DOTA_VISUALIZER.visualize_multiple()\n",
        "NWPU_VISUALIZER.visualize_multiple()"
      ],
      "metadata": {
        "id": "KzLhU-RJc_5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Computation"
      ],
      "metadata": {
        "id": "MrZTUNbzfhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "class Statistician:\n",
        "  def __init__(self, test_dataset: TorchDataset, predicted_boxes: list, predicted_labels: list, predicted_scores: list):\n",
        "    self.metric = MeanAveragePrecision()\n",
        "    self.targets = [target for image, target in test_dataset]\n",
        "    self.predictions = [\n",
        "        {\n",
        "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
        "            \"scores\": torch.as_tensor(scores, dtype=torch.float32)\n",
        "        }\n",
        "        for boxes, labels, scores in zip(predicted_boxes, predicted_labels, predicted_scores)\n",
        "    ]\n",
        "    self.metric.update(self.predictions, self.targets)\n",
        "    self.result = self.metric.compute()\n",
        "\n",
        "  def get_map(self):\n",
        "    return self.result[\"map\"]\n",
        "\n",
        "  def get_map_percent(self):\n",
        "    return self.get_map().detach().cpu().numpy() * 100\n",
        "\n",
        "  def get_map_50(self):\n",
        "    return self.result[\"map_50\"]\n",
        "\n",
        "  def get_map_75(self):\n",
        "    return self.result[\"map_75\"]\n",
        "\n",
        "  def get_map_small(self):\n",
        "    return self.result[\"map_small\"]\n",
        "\n",
        "  def get_map_medium(self):\n",
        "    return self.result[\"map_medium\"]\n",
        "\n",
        "  def get_map_large(self):\n",
        "    return self.result[\"map_large\"]\n",
        "\n",
        "  def get_mar_1(self):\n",
        "    return self.result[\"mar_1\"]\n",
        "\n",
        "  def get_mar_10(self):\n",
        "    return self.result[\"mar_10\"]\n",
        "\n",
        "  def get_mar_100(self):\n",
        "    return self.result[\"mar_100\"]\n",
        "\n",
        "  def get_mar_small(self):\n",
        "    return self.result[\"mar_small\"]\n",
        "\n",
        "  def get_mar_medium(self):\n",
        "    return self.result[\"mar_medium\"]\n",
        "\n",
        "  def get_mar_large(self):\n",
        "    return self.result[\"mar_large\"]\n",
        "\n",
        "  def get_map_per_class(self):\n",
        "    return self.result[\"map_per_class\"]\n",
        "\n",
        "  def get_mar_100_per_class(self):\n",
        "    return self.result[\"mar_100_per_class\"]\n",
        "\n",
        "  def get_classes(self):\n",
        "    return self.result[\"classes\"]"
      ],
      "metadata": {
        "id": "o1_haXv1fmcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print Statistics"
      ],
      "metadata": {
        "id": "i-TRuIeBhtvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HRSC_PRED_STATS = Statistician(HRSC_TEST_DATASET, HRSC_PRED_BOXES, HRSC_PRED_LABELS, HRSC_PRED_SCORES)\n",
        "DOTA_PRED_STATS = Statistician(DOTA_TEST_DATASET, DOTA_PRED_BOXES, DOTA_PRED_LABELS, DOTA_PRED_SCORES)\n",
        "NWPU_PRED_STATS = Statistician(NWPU_TEST_DATASET, NWPU_PRED_BOXES, NWPU_PRED_LABELS, NWPU_PRED_SCORES)\n",
        "\n",
        "print(\"HRSC prediction statistics:\")\n",
        "print(f\"{HRSC_PRED_STATS.get_map_percent()}%\")\n",
        "print(\"DOTA prediction statistics:\")\n",
        "print(f\"{DOTA_PRED_STATS.get_map_percent()}%\")\n",
        "print(\"NWPU prediction statistics:\")\n",
        "print(f\"{NWPU_PRED_STATS.get_map_percent()}%\")"
      ],
      "metadata": {
        "id": "nkicIfHJsf2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}